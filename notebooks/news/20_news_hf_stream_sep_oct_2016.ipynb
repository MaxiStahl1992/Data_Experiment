{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dfb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC-NEWS download started: 2025-12-18T22:33:09.306572\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"CC-NEWS download started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd1249",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb91f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: stanford-oval/ccnews\n",
      "Date range: 2016-09-01 to 2016-10-31 (61 days)\n",
      "Daily limit: 10,000 articles\n",
      "Seed: thesis_sep_oct_2016_v1\n",
      "Hash function: xxhash64\n",
      "Output: data/01_silver/news\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "news_cfg = configs['news']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Output directory\n",
    "silver_news_dir = workspace_root / 'data/01_silver/news'\n",
    "silver_news_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Date range\n",
    "start_date = datetime.strptime(global_cfg['validation_run']['period_start'], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(global_cfg['validation_run']['period_end'], '%Y-%m-%d')\n",
    "total_days = (end_date - start_date).days + 1\n",
    "\n",
    "# Sampling config\n",
    "daily_limit = news_cfg['sampling']['daily_limit_n']\n",
    "seed = global_cfg['validation_run']['seed_string']\n",
    "hash_function = news_cfg['sampling']['hash_function']\n",
    "\n",
    "print(f\"Dataset: {news_cfg['source']['hf_dataset']}\")\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()} ({total_days} days)\")\n",
    "print(f\"Daily limit: {daily_limit:,} articles\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"Hash function: {hash_function}\")\n",
    "print(f\"Output: {silver_news_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f917713",
   "metadata": {},
   "source": [
    "## 2. Define Hash-Based Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a03e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test hash: 12131673047118061681\n",
      "Hash function ready.\n"
     ]
    }
   ],
   "source": [
    "def compute_hash(seed: str, url: str, date: str) -> int:\n",
    "    \"\"\"Compute deterministic hash from seed, url, and date.\"\"\"\n",
    "    combined = f\"{seed}||{url}||{date}\"\n",
    "    return xxhash.xxh64(combined.encode('utf-8')).intdigest()\n",
    "\n",
    "def parse_article_date(article):\n",
    "    \"\"\"Extract date from article, return YYYY-MM-DD string or None.\"\"\"\n",
    "    # CC-NEWS has 'published_date' field (note: not 'date')\n",
    "    date_str = article.get('published_date', '')\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    # Extract just the date part (first 10 chars)\n",
    "    try:\n",
    "        return date_str[:10] if len(date_str) >= 10 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Test the hash function\n",
    "test_hash = compute_hash(seed, \"https://example.com/article\", \"2016-09-01\")\n",
    "print(f\"\\nTest hash: {test_hash}\")\n",
    "print(\"Hash function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6f3ac",
   "metadata": {},
   "source": [
    "## 3. Load HuggingFace Dataset (Streaming Mode)\n",
    "\n",
    "**Note:** This will stream the dataset without downloading everything. We'll filter for our date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20afb544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CC-NEWS dataset...\n",
      "Using DOWNLOAD mode for reliability (streaming has connection issues)\n",
      "This will download the 2016 subset (~1-2 GB), then process locally.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c0960ad0414cd6aa9d61210df04e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d490eb7583b476396f94f339a383838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2016_0000.parquet:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccb4efff50145d29d3e29a47a0479d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2016_0001.parquet:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cae97553f04c37947c5fcaa5ab94e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2016_0002.parquet:   0%|          | 0.00/573M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebbcda0c8f14f8cb5c299de0b455153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded\n",
      "\n",
      "Dataset info:\n",
      "  Name: stanford-oval/ccnews\n",
      "  Year subset: 2016\n",
      "  Total articles: 2,315,442\n",
      "  Mode: Downloaded (cached locally)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CC-NEWS dataset...\")\n",
    "print(\"This will download the 2016 subset (~1-5 GB), then process locally.\\n\")\n",
    "\n",
    "# Load dataset in download mode - more reliable than streaming\n",
    "dataset = load_dataset(\n",
    "    news_cfg['source']['hf_dataset'],\n",
    "    name=\"2016\",  # Only download 2016 articles\n",
    "    split='train'\n",
    "    # No streaming=True - will download to cache\n",
    ")\n",
    "\n",
    "print(\"✓ Dataset loaded\")\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"  Name: {news_cfg['source']['hf_dataset']}\")\n",
    "print(f\"  Year subset: 2016\")\n",
    "print(f\"  Total articles: {len(dataset):,}\")\n",
    "print(f\"  Mode: Downloaded (cached locally)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044c0b9",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5b797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample articles from dataset:\n",
      "\n",
      "Article 1:\n",
      "  Date: 2016-10-18\n",
      "  URL: http://www.coventrytelegraph.net/news/its-not-just-aa-plastic-12041279...\n",
      "  Title: It's not just AA plastic £5 notes that are valuable - have you checked yours? - ...\n",
      "  Text length: 1524 chars\n",
      "  Language: en\n",
      "  Fields: ['requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n",
      "\n",
      "Article 2:\n",
      "  Date: 2016-10-18\n",
      "  URL: http://www.coventrytelegraph.net/news/coventry-news/one-good-guys-tribute-rugby-...\n",
      "  Title: 'One of the good guys' - tribute to rugby-loving former police inspector who cap...\n",
      "  Text length: 2488 chars\n",
      "  Language: en\n",
      "  Fields: ['requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n",
      "\n",
      "Article 3:\n",
      "  Date: 2016-10-18\n",
      "  URL: http://www.coventrytelegraph.net/news/buying-drink-drunk-mate-could-12041327...\n",
      "  Title: Why buying a drink for a drunk mate could land you with a sobering £1,000 fine -...\n",
      "  Text length: 1537 chars\n",
      "  Language: en\n",
      "  Fields: ['requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Peek at first few articles to understand structure\n",
    "print(\"Sample articles from dataset:\\n\")\n",
    "\n",
    "for i in range(min(3, len(dataset))):\n",
    "    article = dataset[i]\n",
    "    print(f\"Article {i+1}:\")\n",
    "    print(f\"  Date: {article.get('published_date', 'N/A')}\")\n",
    "    print(f\"  URL: {article.get('requested_url', 'N/A')[:80]}...\")\n",
    "    print(f\"  Title: {article.get('title', 'N/A')[:80]}...\")\n",
    "    print(f\"  Text length: {len(article.get('plain_text', ''))} chars\")\n",
    "    print(f\"  Language: {article.get('language', 'N/A')}\")\n",
    "    print(f\"  Fields: {list(article.keys())}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ef9de",
   "metadata": {},
   "source": [
    "## 5. Stream, Filter, and Sample Articles\n",
    "\n",
    "**Process:**\n",
    "1. Stream through dataset\n",
    "2. Filter articles in our date range (2016-09-01 to 2016-10-31)\n",
    "3. Compute hash for each article\n",
    "4. Group by date, keep top-10k smallest hashes per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c29a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dates: 61 days from 2016-09-01 to 2016-10-31\n",
      "\n",
      "Processing 2,315,442 articles from downloaded dataset...\n",
      "This should take 5-10 minutes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   4%|▍         | 102615/2315442 [00:05<02:00, 18418.10 articles/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Seen: 100,000 | In range: 98,697 | English: 41,937 | Dates: 49/61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 2315442/2315442 [02:00<00:00, 19183.04 articles/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Total articles seen: 2,315,442\n",
      "Articles in date range: 237,603\n",
      "English articles: 93,737\n",
      "Dates with articles: 61/61\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate target dates\n",
    "target_dates = set()\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    target_dates.add(current.strftime('%Y-%m-%d'))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(f\"Target dates: {len(target_dates)} days from {min(target_dates)} to {max(target_dates)}\")\n",
    "print(f\"\\nProcessing {len(dataset):,} articles from downloaded dataset...\")\n",
    "print(f\"This should take 5-10 minutes.\\n\")\n",
    "\n",
    "# Data structure: {date: [(hash, article), ...]}\n",
    "daily_articles = defaultdict(list)\n",
    "\n",
    "articles_seen = 0\n",
    "articles_in_range = 0\n",
    "articles_english = 0\n",
    "\n",
    "# Process downloaded dataset - much faster than streaming\n",
    "for article in tqdm(dataset, desc=\"Processing articles\", unit=\" articles\"):\n",
    "    articles_seen += 1\n",
    "    \n",
    "    # Parse date\n",
    "    article_date = parse_article_date(article)\n",
    "    \n",
    "    # Skip if not in our date range\n",
    "    if article_date not in target_dates:\n",
    "        continue\n",
    "    \n",
    "    articles_in_range += 1\n",
    "    \n",
    "    # Filter for English only\n",
    "    language = article.get('language', '').lower()\n",
    "    if language != 'en':\n",
    "        continue\n",
    "    \n",
    "    articles_english += 1\n",
    "    \n",
    "    # Get required fields\n",
    "    url = article.get('requested_url', '')\n",
    "    if not url:\n",
    "        continue\n",
    "    \n",
    "    # Compute hash\n",
    "    article_hash = compute_hash(seed, url, article_date)\n",
    "    \n",
    "    # Store with hash\n",
    "    daily_articles[article_date].append((article_hash, article))\n",
    "    \n",
    "    # Progress update every 100k articles\n",
    "    if articles_seen % 100000 == 0:\n",
    "        dates_covered = len(daily_articles)\n",
    "        tqdm.write(f\"  Seen: {articles_seen:,} | In range: {articles_in_range:,} | English: {articles_english:,} | Dates: {dates_covered}/{len(target_dates)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total articles seen: {articles_seen:,}\")\n",
    "print(f\"Articles in date range: {articles_in_range:,}\")\n",
    "print(f\"English articles: {articles_english:,}\")\n",
    "print(f\"Dates with articles: {len(daily_articles)}/{len(target_dates)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f8979",
   "metadata": {},
   "source": [
    "## 6. Select Top-K Articles per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbc9618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting top-k articles per day (smallest hashes)...\n",
      "\n",
      "Daily sampling summary:\n",
      "      date  available  sampled\n",
      "2016-09-01        124      124\n",
      "2016-09-02        104      104\n",
      "2016-09-03         53       53\n",
      "2016-09-04         59       59\n",
      "2016-09-05         58       58\n",
      "2016-09-06        106      106\n",
      "2016-09-07         99       99\n",
      "2016-09-08        141      141\n",
      "2016-09-09        111      111\n",
      "2016-09-10         50       50\n",
      "2016-09-11        354      354\n",
      "2016-09-12        288      288\n",
      "2016-09-13        161      161\n",
      "2016-09-14        108      108\n",
      "2016-09-15        151      151\n",
      "2016-09-16        140      140\n",
      "2016-09-17         67       67\n",
      "2016-09-18         72       72\n",
      "2016-09-19        125      125\n",
      "2016-09-20        147      147\n",
      "2016-09-21        175      175\n",
      "2016-09-22        134      134\n",
      "2016-09-23        118      118\n",
      "2016-09-24         50       50\n",
      "2016-09-25         66       66\n",
      "2016-09-26        124      124\n",
      "2016-09-27        141      141\n",
      "2016-09-28        148      148\n",
      "2016-09-29        132      132\n",
      "2016-09-30        193      193\n",
      "2016-10-01         71       71\n",
      "2016-10-02         77       77\n",
      "2016-10-03        172      172\n",
      "2016-10-04        152      152\n",
      "2016-10-05        191      191\n",
      "2016-10-06        192      192\n",
      "2016-10-07        160      160\n",
      "2016-10-08         81       81\n",
      "2016-10-09         85       85\n",
      "2016-10-10        135      135\n",
      "2016-10-11        342      342\n",
      "2016-10-12        230      230\n",
      "2016-10-13        243      243\n",
      "2016-10-14        225      225\n",
      "2016-10-15         75       75\n",
      "2016-10-16        150      150\n",
      "2016-10-17       3840     3840\n",
      "2016-10-18       7100     7100\n",
      "2016-10-19       7452     7452\n",
      "2016-10-20       7485     7485\n",
      "2016-10-21       6901     6901\n",
      "2016-10-22       3794     3794\n",
      "2016-10-23       3701     3701\n",
      "2016-10-24       6286     6286\n",
      "2016-10-25       6847     6847\n",
      "2016-10-26       7233     7233\n",
      "2016-10-27       7133     7133\n",
      "2016-10-28       6707     6707\n",
      "2016-10-29       3563     3563\n",
      "2016-10-30       3267     3267\n",
      "2016-10-31       6048     6048\n",
      "\n",
      "================================================================================\n",
      "Total articles sampled: 93,737\n",
      "Average per day: 1537\n",
      "Days with full quota (10,000): 0/61\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSelecting top-k articles per day (smallest hashes)...\\n\")\n",
    "\n",
    "daily_stats = []\n",
    "\n",
    "for date in sorted(target_dates):\n",
    "    if date not in daily_articles:\n",
    "        print(f\"⚠ Warning: No articles found for {date}\")\n",
    "        daily_stats.append({\n",
    "            'date': date,\n",
    "            'available': 0,\n",
    "            'sampled': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    articles = daily_articles[date]\n",
    "    \n",
    "    # Sort by hash (ascending) and take top-k\n",
    "    articles.sort(key=lambda x: x[0])\n",
    "    selected = articles[:daily_limit]\n",
    "    \n",
    "    # Update storage with selected articles only\n",
    "    daily_articles[date] = selected\n",
    "    \n",
    "    daily_stats.append({\n",
    "        'date': date,\n",
    "        'available': len(articles),\n",
    "        'sampled': len(selected)\n",
    "    })\n",
    "\n",
    "# Display stats\n",
    "df_stats = pd.DataFrame(daily_stats)\n",
    "print(\"Daily sampling summary:\")\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total articles sampled: {df_stats['sampled'].sum():,}\")\n",
    "print(f\"Average per day: {df_stats['sampled'].mean():.0f}\")\n",
    "print(f\"Days with full quota ({daily_limit:,}): {(df_stats['sampled'] == daily_limit).sum()}/{len(target_dates)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf472922",
   "metadata": {},
   "source": [
    "## 7. Write Daily Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27291823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing daily Parquet files...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing files: 100%|██████████| 61/61 [00:01<00:00, 43.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✓ Files written: 61\n",
      "✓ Output directory: data/01_silver/news\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWriting daily Parquet files...\\n\")\n",
    "\n",
    "files_written = []\n",
    "\n",
    "for date in tqdm(sorted(target_dates), desc=\"Writing files\"):\n",
    "    if date not in daily_articles or not daily_articles[date]:\n",
    "        continue\n",
    "    \n",
    "    # Extract articles (discard hashes)\n",
    "    articles = [article for _, article in daily_articles[date]]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Add date column (ensure it's just YYYY-MM-DD)\n",
    "    df['date'] = date\n",
    "    \n",
    "    # Reorder columns: date first\n",
    "    cols = ['date'] + [col for col in df.columns if col != 'date']\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write to Parquet\n",
    "    output_file = silver_news_dir / f\"{date}.parquet\"\n",
    "    df.to_parquet(output_file, compression='snappy', index=False)\n",
    "    files_written.append(output_file)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Files written: {len(files_written)}\")\n",
    "print(f\"✓ Output directory: {silver_news_dir.relative_to(workspace_root)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6828fc",
   "metadata": {},
   "source": [
    "## 8. Verify Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be7a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: 2016-10-01.parquet\n",
      "================================================================================\n",
      "Shape: (71, 16)\n",
      "Columns: ['date', 'requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n",
      "\n",
      "First 3 rows:\n",
      "      date                                                                                                                                                     title                                                                                                                 requested_url\n",
      "2016-10-01                                                                                                                     10 Propositions for Research-Creation                                                           http://quod.lib.umich.edu/j/jep/3336451.0019.206?view=text;rgn=main\n",
      "2016-10-01                                                                                                                 Judge blocks pro-abortion law in Illinois                                                       https://world.wng.org/2016/12/judge_blocks_pro_abortion_law_in_illinois\n",
      "2016-10-01 CRTN.org - Catholic Radio & Television Network | Events | Catholic Audiovisual Producers from Latin America attended SIPCA TV intensive training workshop http://www.crtn.org/event/catholic-audiovisual-producers-from-latin-america-attended-sipca-tv-intensive-training-workshop/682\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if files_written:\n",
    "    # Check a sample file\n",
    "    sample_file = files_written[len(files_written)//2]\n",
    "    \n",
    "    print(f\"Sample file: {sample_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_sample = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"Shape: {df_sample.shape}\")\n",
    "    print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(df_sample[['date', 'title', 'requested_url']].head(3).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ No files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4acc1",
   "metadata": {},
   "source": [
    "## 9. Save Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6422e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metadata saved: data/01_silver/news/processing_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'source': {\n",
    "        'dataset': news_cfg['source']['hf_dataset'],\n",
    "        'access_method': 'download',  # Changed from streaming\n",
    "        'year_subset': '2016'\n",
    "    },\n",
    "    'period': {\n",
    "        'start': start_date.strftime('%Y-%m-%d'),\n",
    "        'end': end_date.strftime('%Y-%m-%d'),\n",
    "        'days': total_days\n",
    "    },\n",
    "    'filters': {\n",
    "        'language': 'en'\n",
    "    },\n",
    "    'sampling': {\n",
    "        'method': news_cfg['sampling']['method'],\n",
    "        'hash_function': hash_function,\n",
    "        'seed': seed,\n",
    "        'daily_limit': daily_limit\n",
    "    },\n",
    "    'processing': {\n",
    "        'articles_processed': int(articles_seen),\n",
    "        'articles_in_date_range': int(articles_in_range),\n",
    "        'articles_english': int(articles_english),\n",
    "        'total_sampled': int(df_stats['sampled'].sum())\n",
    "    },\n",
    "    'output': {\n",
    "        'directory': str(silver_news_dir.relative_to(workspace_root)),\n",
    "        'files_written': len(files_written),\n",
    "        'dates_covered': sorted([f.stem for f in files_written]),\n",
    "        'missing_dates': sorted(list(target_dates - set([f.stem for f in files_written])))\n",
    "    },\n",
    "    'daily_statistics': df_stats.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_file = silver_news_dir / 'processing_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved: {metadata_file.relative_to(workspace_root)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
