{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dfb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC-NEWS download started: 2025-12-18T20:05:42.749405\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"CC-NEWS download started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd1249",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb91f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: stanford-oval/ccnews\n",
      "Date range: 2016-09-01 to 2016-10-31 (61 days)\n",
      "Daily limit: 10,000 articles\n",
      "Seed: thesis_sep_oct_2016_v1\n",
      "Hash function: xxhash64\n",
      "Output: data/01_silver/news\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "news_cfg = configs['news']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Output directory\n",
    "silver_news_dir = workspace_root / 'data/01_silver/news'\n",
    "silver_news_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Date range\n",
    "start_date = datetime.strptime(global_cfg['validation_run']['period_start'], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(global_cfg['validation_run']['period_end'], '%Y-%m-%d')\n",
    "total_days = (end_date - start_date).days + 1\n",
    "\n",
    "# Sampling config\n",
    "daily_limit = news_cfg['sampling']['daily_limit_n']\n",
    "seed = global_cfg['validation_run']['seed_string']\n",
    "hash_function = news_cfg['sampling']['hash_function']\n",
    "\n",
    "print(f\"Dataset: {news_cfg['source']['hf_dataset']}\")\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()} ({total_days} days)\")\n",
    "print(f\"Daily limit: {daily_limit:,} articles\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"Hash function: {hash_function}\")\n",
    "print(f\"Output: {silver_news_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f917713",
   "metadata": {},
   "source": [
    "## 2. Define Hash-Based Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a03e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test hash: 12131673047118061681\n",
      "Hash function ready.\n"
     ]
    }
   ],
   "source": [
    "def compute_hash(seed: str, url: str, date: str) -> int:\n",
    "    \"\"\"Compute deterministic hash from seed, url, and date.\"\"\"\n",
    "    combined = f\"{seed}||{url}||{date}\"\n",
    "    return xxhash.xxh64(combined.encode('utf-8')).intdigest()\n",
    "\n",
    "def parse_article_date(article):\n",
    "    \"\"\"Extract date from article, return YYYY-MM-DD string or None.\"\"\"\n",
    "    # CC-NEWS has 'date' field in format YYYY-MM-DD or YYYY-MM-DD HH:MM:SS\n",
    "    date_str = article.get('date', '')\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    # Extract just the date part (first 10 chars)\n",
    "    try:\n",
    "        return date_str[:10] if len(date_str) >= 10 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Test the hash function\n",
    "test_hash = compute_hash(seed, \"https://example.com/article\", \"2016-09-01\")\n",
    "print(f\"\\nTest hash: {test_hash}\")\n",
    "print(\"Hash function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6f3ac",
   "metadata": {},
   "source": [
    "## 3. Load HuggingFace Dataset (Streaming Mode)\n",
    "\n",
    "**Note:** This will stream the dataset without downloading everything. We'll filter for our date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20afb544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CC-NEWS dataset in streaming mode...\n",
      "This will take a moment to initialize...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458234c3dc9e48ba81ba3f386bad3a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded in streaming mode\n",
      "\n",
      "Dataset info:\n",
      "  Name: stanford-oval/ccnews\n",
      "  Year subset: 2016\n",
      "  Mode: Streaming (memory efficient)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CC-NEWS dataset in streaming mode...\")\n",
    "print(\"This will take a moment to initialize...\\n\")\n",
    "\n",
    "# Load dataset in streaming mode - use 2016 subset only\n",
    "dataset = load_dataset(\n",
    "    news_cfg['source']['hf_dataset'],\n",
    "    name=\"2016\",  # Only stream 2016 articles\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"✓ Dataset loaded in streaming mode\")\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"  Name: {news_cfg['source']['hf_dataset']}\")\n",
    "print(f\"  Year subset: 2016\")\n",
    "print(f\"  Mode: Streaming (memory efficient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044c0b9",
   "metadata": {},
   "source": [
    "## 4. Inspect Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample articles from dataset:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek at first few articles to understand structure\n",
    "print(\"Sample articles from dataset:\\n\")\n",
    "\n",
    "for i, article in enumerate(dataset[\"train\"].take(3)):\n",
    "    print(f\"Article {i+1}:\")\n",
    "    print(f\"  Date: {article.get('date', 'N/A')}\")\n",
    "    print(f\"  URL: {article.get('url', 'N/A')[:80]}...\")\n",
    "    print(f\"  Title: {article.get('title', 'N/A')[:80]}...\")\n",
    "    print(f\"  Text length: {len(article.get('text', ''))} chars\")\n",
    "    print(f\"  Fields: {list(article.keys())}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ef9de",
   "metadata": {},
   "source": [
    "## 5. Stream, Filter, and Sample Articles\n",
    "\n",
    "**Process:**\n",
    "1. Stream through dataset\n",
    "2. Filter articles in our date range (2016-09-01 to 2016-10-31)\n",
    "3. Compute hash for each article\n",
    "4. Group by date, keep top-10k smallest hashes per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target dates: 61 days from 2016-09-01 to 2016-10-31\n",
      "\n",
      "Streaming and sampling articles...\n",
      "This may take 10-20 minutes depending on dataset size and network speed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 11315334 articles [1:42:49, 4265.96 articles/s]'The read operation timed out' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'The read operation timed out' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'[Errno 54] Connection reset by peer' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'The read operation timed out' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'The read operation timed out' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'[Errno 54] Connection reset by peer' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'The read operation timed out' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'The read operation timed out' thrown while requesting GET https://huggingface.co/datasets/stanford-oval/ccnews/resolve/d733e654c9a506df519e1a166a86c118c7657ce4/2017_0009.parquet\n",
      "Processing articles: 11315442 articles [2:33:36, 1227.77 articles/s]\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "The read operation timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_transports/default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     16\u001b[39m articles_in_range = \u001b[32m0\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Stream through dataset\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Note: We'll process a large chunk to ensure we get articles for all dates\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# CC-NEWS is roughly chronological but not perfectly sorted\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcessing articles\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m articles\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43marticles_seen\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Parse date\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/datasets/iterable_dataset.py:2093\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2090\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2091\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2093\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_typed\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;49;00m\n\u001b[32m   2096\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;49;00m\n\u001b[32m   2097\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_apply_feature_types_on_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_token_per_repo_id\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/datasets/iterable_dataset.py:279\u001b[39m, in \u001b[36mArrowExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    277\u001b[39m shard_example_idx_start = \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mshard_example_idx\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    278\u001b[39m shard_example_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tables_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/datasets/packaged_modules/parquet/parquet.py:93\u001b[39m, in \u001b[36mParquet._generate_tables\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m     91\u001b[39m batch_size = \u001b[38;5;28mself\u001b[39m.config.batch_size \u001b[38;5;129;01mor\u001b[39;00m parquet_fragment.row_groups[\u001b[32m0\u001b[39m].num_rows\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparquet_fragment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mfilter_expr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_readahead\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfragment_readahead\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;49;00m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3806\u001b[39m, in \u001b[36m_iterator\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3424\u001b[39m, in \u001b[36mpyarrow._dataset.TaggedRecordBatchIterator.__next__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/datasets/utils/file_utils.py:826\u001b[39m, in \u001b[36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_retries + \u001b[32m1\u001b[39m):\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m         out = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    827\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    829\u001b[39m         aiohttp.client_exceptions.ClientError,\n\u001b[32m    830\u001b[39m         asyncio.TimeoutError,\n\u001b[32m    831\u001b[39m         requests.exceptions.ConnectionError,\n\u001b[32m    832\u001b[39m         requests.exceptions.Timeout,\n\u001b[32m    833\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py:1104\u001b[39m, in \u001b[36mHfFileSystemFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   1102\u001b[39m         \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/fsspec/spec.py:1941\u001b[39m, in \u001b[36mAbstractBufferedFile.read\u001b[39m\u001b[34m(self, length)\u001b[39m\n\u001b[32m   1938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m length == \u001b[32m0\u001b[39m:\n\u001b[32m   1939\u001b[39m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[32m   1940\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1941\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1943\u001b[39m logger.debug(\n\u001b[32m   1944\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1945\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1948\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache._log_stats(),\n\u001b[32m   1949\u001b[39m )\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mself\u001b[39m.loc += \u001b[38;5;28mlen\u001b[39m(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/fsspec/caching.py:234\u001b[39m, in \u001b[36mReadAheadCache._fetch\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m    232\u001b[39m end = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.size, end + \u001b[38;5;28mself\u001b[39m.blocksize)\n\u001b[32m    233\u001b[39m \u001b[38;5;28mself\u001b[39m.total_requested_bytes += end - start\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.start = start\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.end = \u001b[38;5;28mself\u001b[39m.start + \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py:1065\u001b[39m, in \u001b[36mHfFileSystemFile._fetch_range\u001b[39m\u001b[34m(self, start, end)\u001b[39m\n\u001b[32m   1054\u001b[39m headers = {\n\u001b[32m   1055\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrange\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1056\u001b[39m     **\u001b[38;5;28mself\u001b[39m.fs._api._build_hf_headers(),\n\u001b[32m   1057\u001b[39m }\n\u001b[32m   1058\u001b[39m url = hf_hub_url(\n\u001b[32m   1059\u001b[39m     repo_id=\u001b[38;5;28mself\u001b[39m.resolved_path.repo_id,\n\u001b[32m   1060\u001b[39m     revision=\u001b[38;5;28mself\u001b[39m.resolved_path.revision,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1063\u001b[39m     endpoint=\u001b[38;5;28mself\u001b[39m.fs.endpoint,\n\u001b[32m   1064\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m r = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:506\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_backoff\u001b[39m(\n\u001b[32m    442\u001b[39m     method: HTTP_METHOD_T,\n\u001b[32m    443\u001b[39m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m     **kwargs,\n\u001b[32m    451\u001b[39m ) -> httpx.Response:\n\u001b[32m    452\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around httpx to retry calls on an endpoint, with exponential backoff.\u001b[39;00m\n\u001b[32m    453\u001b[39m \n\u001b[32m    454\u001b[39m \u001b[33;03m    Endpoint call is retried on exceptions (ex: connection timeout, proxy error,...)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m \u001b[33;03m    > issue on [Github](https://github.com/huggingface/huggingface_hub).\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_http_backoff_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:426\u001b[39m, in \u001b[36m_http_backoff_base\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, stream, **kwargs)\u001b[39m\n\u001b[32m    423\u001b[39m         close_session()  \u001b[38;5;66;03m# In case of SSLError it's best to close the shared httpx.Client objects\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nb_tries > max_retries:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ratelimit_reset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    429\u001b[39m     actual_sleep = \u001b[38;5;28mfloat\u001b[39m(ratelimit_reset) + \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# +1s to avoid rounding issues\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:414\u001b[39m, in \u001b[36m_http_backoff_base\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, stream, **kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_retry(response):\n\u001b[32m    416\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_client.py:928\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    927\u001b[39m     response.close()\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_client.py:922\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_models.py:881\u001b[39m, in \u001b[36mResponse.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    877\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[33;03mRead and return the response content.\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_content\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m     \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_transports/default.py:126\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    128\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/01_Studium/11_Thesis/Data_Experiment/venv/lib/python3.13/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: The read operation timed out"
     ]
    }
   ],
   "source": [
    "# Generate target dates\n",
    "target_dates = set()\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    target_dates.add(current.strftime('%Y-%m-%d'))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "print(f\"Target dates: {len(target_dates)} days from {min(target_dates)} to {max(target_dates)}\")\n",
    "print(f\"\\nStreaming and sampling articles...\")\n",
    "print(f\"This may take 10-20 minutes depending on network speed.\\n\")\n",
    "\n",
    "# Data structure: {date: [(hash, article), ...]}\n",
    "daily_articles = defaultdict(list)\n",
    "\n",
    "articles_seen = 0\n",
    "articles_in_range = 0\n",
    "articles_english = 0\n",
    "\n",
    "# Stream through dataset - use dataset[\"train\"] to access the split\n",
    "for article in tqdm(dataset[\"train\"], desc=\"Processing articles\", unit=\" articles\"):\n",
    "    articles_seen += 1\n",
    "    \n",
    "    # Parse date\n",
    "    article_date = parse_article_date(article)\n",
    "    \n",
    "    # Skip if not in our date range\n",
    "    if article_date not in target_dates:\n",
    "        # Stop if we've seen many articles past our end date\n",
    "        if article_date and article_date > max(target_dates):\n",
    "            if articles_seen % 10000 == 0:\n",
    "                # Check if we have enough articles for all dates\n",
    "                dates_with_enough = sum(1 for articles in daily_articles.values() if len(articles) >= daily_limit)\n",
    "                if dates_with_enough == len(target_dates):\n",
    "                    print(f\"\\n✓ All {len(target_dates)} dates have {daily_limit:,}+ articles. Stopping.\")\n",
    "                    break\n",
    "        continue\n",
    "    \n",
    "    articles_in_range += 1\n",
    "    \n",
    "    # Filter for English only\n",
    "    language = article.get('language', '').lower()\n",
    "    if language != 'en':\n",
    "        continue\n",
    "    \n",
    "    articles_english += 1\n",
    "    \n",
    "    # Get required fields\n",
    "    url = article.get('url', '')\n",
    "    if not url:\n",
    "        continue\n",
    "    \n",
    "    # Compute hash\n",
    "    article_hash = compute_hash(seed, url, article_date)\n",
    "    \n",
    "    # Store with hash\n",
    "    daily_articles[article_date].append((article_hash, article))\n",
    "    \n",
    "    # Progress update every 10k articles\n",
    "    if articles_seen % 10000 == 0:\n",
    "        dates_covered = len(daily_articles)\n",
    "        tqdm.write(f\"  Seen: {articles_seen:,} | In range: {articles_in_range:,} | English: {articles_english:,} | Dates: {dates_covered}/{len(target_dates)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total articles seen: {articles_seen:,}\")\n",
    "print(f\"Articles in date range: {articles_in_range:,}\")\n",
    "print(f\"English articles: {articles_english:,}\")\n",
    "print(f\"Dates with articles: {len(daily_articles)}/{len(target_dates)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f8979",
   "metadata": {},
   "source": [
    "## 6. Select Top-K Articles per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bbc9618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting top-k articles per day (smallest hashes)...\n",
      "\n",
      "⚠ Warning: No articles found for 2016-09-01\n",
      "⚠ Warning: No articles found for 2016-09-02\n",
      "⚠ Warning: No articles found for 2016-09-03\n",
      "⚠ Warning: No articles found for 2016-09-04\n",
      "⚠ Warning: No articles found for 2016-09-05\n",
      "⚠ Warning: No articles found for 2016-09-06\n",
      "⚠ Warning: No articles found for 2016-09-07\n",
      "⚠ Warning: No articles found for 2016-09-08\n",
      "⚠ Warning: No articles found for 2016-09-09\n",
      "⚠ Warning: No articles found for 2016-09-10\n",
      "⚠ Warning: No articles found for 2016-09-11\n",
      "⚠ Warning: No articles found for 2016-09-12\n",
      "⚠ Warning: No articles found for 2016-09-13\n",
      "⚠ Warning: No articles found for 2016-09-14\n",
      "⚠ Warning: No articles found for 2016-09-15\n",
      "⚠ Warning: No articles found for 2016-09-16\n",
      "⚠ Warning: No articles found for 2016-09-17\n",
      "⚠ Warning: No articles found for 2016-09-18\n",
      "⚠ Warning: No articles found for 2016-09-19\n",
      "⚠ Warning: No articles found for 2016-09-20\n",
      "⚠ Warning: No articles found for 2016-09-21\n",
      "⚠ Warning: No articles found for 2016-09-22\n",
      "⚠ Warning: No articles found for 2016-09-23\n",
      "⚠ Warning: No articles found for 2016-09-24\n",
      "⚠ Warning: No articles found for 2016-09-25\n",
      "⚠ Warning: No articles found for 2016-09-26\n",
      "⚠ Warning: No articles found for 2016-09-27\n",
      "⚠ Warning: No articles found for 2016-09-28\n",
      "⚠ Warning: No articles found for 2016-09-29\n",
      "⚠ Warning: No articles found for 2016-09-30\n",
      "⚠ Warning: No articles found for 2016-10-01\n",
      "⚠ Warning: No articles found for 2016-10-02\n",
      "⚠ Warning: No articles found for 2016-10-03\n",
      "⚠ Warning: No articles found for 2016-10-04\n",
      "⚠ Warning: No articles found for 2016-10-05\n",
      "⚠ Warning: No articles found for 2016-10-06\n",
      "⚠ Warning: No articles found for 2016-10-07\n",
      "⚠ Warning: No articles found for 2016-10-08\n",
      "⚠ Warning: No articles found for 2016-10-09\n",
      "⚠ Warning: No articles found for 2016-10-10\n",
      "⚠ Warning: No articles found for 2016-10-11\n",
      "⚠ Warning: No articles found for 2016-10-12\n",
      "⚠ Warning: No articles found for 2016-10-13\n",
      "⚠ Warning: No articles found for 2016-10-14\n",
      "⚠ Warning: No articles found for 2016-10-15\n",
      "⚠ Warning: No articles found for 2016-10-16\n",
      "⚠ Warning: No articles found for 2016-10-17\n",
      "⚠ Warning: No articles found for 2016-10-18\n",
      "⚠ Warning: No articles found for 2016-10-19\n",
      "⚠ Warning: No articles found for 2016-10-20\n",
      "⚠ Warning: No articles found for 2016-10-21\n",
      "⚠ Warning: No articles found for 2016-10-22\n",
      "⚠ Warning: No articles found for 2016-10-23\n",
      "⚠ Warning: No articles found for 2016-10-24\n",
      "⚠ Warning: No articles found for 2016-10-25\n",
      "⚠ Warning: No articles found for 2016-10-26\n",
      "⚠ Warning: No articles found for 2016-10-27\n",
      "⚠ Warning: No articles found for 2016-10-28\n",
      "⚠ Warning: No articles found for 2016-10-29\n",
      "⚠ Warning: No articles found for 2016-10-30\n",
      "⚠ Warning: No articles found for 2016-10-31\n",
      "Daily sampling summary:\n",
      "      date  available  sampled\n",
      "2016-09-01          0        0\n",
      "2016-09-02          0        0\n",
      "2016-09-03          0        0\n",
      "2016-09-04          0        0\n",
      "2016-09-05          0        0\n",
      "2016-09-06          0        0\n",
      "2016-09-07          0        0\n",
      "2016-09-08          0        0\n",
      "2016-09-09          0        0\n",
      "2016-09-10          0        0\n",
      "2016-09-11          0        0\n",
      "2016-09-12          0        0\n",
      "2016-09-13          0        0\n",
      "2016-09-14          0        0\n",
      "2016-09-15          0        0\n",
      "2016-09-16          0        0\n",
      "2016-09-17          0        0\n",
      "2016-09-18          0        0\n",
      "2016-09-19          0        0\n",
      "2016-09-20          0        0\n",
      "2016-09-21          0        0\n",
      "2016-09-22          0        0\n",
      "2016-09-23          0        0\n",
      "2016-09-24          0        0\n",
      "2016-09-25          0        0\n",
      "2016-09-26          0        0\n",
      "2016-09-27          0        0\n",
      "2016-09-28          0        0\n",
      "2016-09-29          0        0\n",
      "2016-09-30          0        0\n",
      "2016-10-01          0        0\n",
      "2016-10-02          0        0\n",
      "2016-10-03          0        0\n",
      "2016-10-04          0        0\n",
      "2016-10-05          0        0\n",
      "2016-10-06          0        0\n",
      "2016-10-07          0        0\n",
      "2016-10-08          0        0\n",
      "2016-10-09          0        0\n",
      "2016-10-10          0        0\n",
      "2016-10-11          0        0\n",
      "2016-10-12          0        0\n",
      "2016-10-13          0        0\n",
      "2016-10-14          0        0\n",
      "2016-10-15          0        0\n",
      "2016-10-16          0        0\n",
      "2016-10-17          0        0\n",
      "2016-10-18          0        0\n",
      "2016-10-19          0        0\n",
      "2016-10-20          0        0\n",
      "2016-10-21          0        0\n",
      "2016-10-22          0        0\n",
      "2016-10-23          0        0\n",
      "2016-10-24          0        0\n",
      "2016-10-25          0        0\n",
      "2016-10-26          0        0\n",
      "2016-10-27          0        0\n",
      "2016-10-28          0        0\n",
      "2016-10-29          0        0\n",
      "2016-10-30          0        0\n",
      "2016-10-31          0        0\n",
      "\n",
      "================================================================================\n",
      "Total articles sampled: 0\n",
      "Average per day: 0\n",
      "Days with full quota (10,000): 0/61\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSelecting top-k articles per day (smallest hashes)...\\n\")\n",
    "\n",
    "daily_stats = []\n",
    "\n",
    "for date in sorted(target_dates):\n",
    "    if date not in daily_articles:\n",
    "        print(f\"⚠ Warning: No articles found for {date}\")\n",
    "        daily_stats.append({\n",
    "            'date': date,\n",
    "            'available': 0,\n",
    "            'sampled': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    articles = daily_articles[date]\n",
    "    \n",
    "    # Sort by hash (ascending) and take top-k\n",
    "    articles.sort(key=lambda x: x[0])\n",
    "    selected = articles[:daily_limit]\n",
    "    \n",
    "    # Update storage with selected articles only\n",
    "    daily_articles[date] = selected\n",
    "    \n",
    "    daily_stats.append({\n",
    "        'date': date,\n",
    "        'available': len(articles),\n",
    "        'sampled': len(selected)\n",
    "    })\n",
    "\n",
    "# Display stats\n",
    "df_stats = pd.DataFrame(daily_stats)\n",
    "print(\"Daily sampling summary:\")\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total articles sampled: {df_stats['sampled'].sum():,}\")\n",
    "print(f\"Average per day: {df_stats['sampled'].mean():.0f}\")\n",
    "print(f\"Days with full quota ({daily_limit:,}): {(df_stats['sampled'] == daily_limit).sum()}/{len(target_dates)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf472922",
   "metadata": {},
   "source": [
    "## 7. Write Daily Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27291823",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWriting daily Parquet files...\\n\")\n",
    "\n",
    "files_written = []\n",
    "\n",
    "for date in tqdm(sorted(target_dates), desc=\"Writing files\"):\n",
    "    if date not in daily_articles or not daily_articles[date]:\n",
    "        continue\n",
    "    \n",
    "    # Extract articles (discard hashes)\n",
    "    articles = [article for _, article in daily_articles[date]]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Add date column (ensure it's just YYYY-MM-DD)\n",
    "    df['date'] = date\n",
    "    \n",
    "    # Reorder columns: date first\n",
    "    cols = ['date'] + [col for col in df.columns if col != 'date']\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write to Parquet\n",
    "    output_file = silver_news_dir / f\"{date}.parquet\"\n",
    "    df.to_parquet(output_file, compression='snappy', index=False)\n",
    "    files_written.append(output_file)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Files written: {len(files_written)}\")\n",
    "print(f\"✓ Output directory: {silver_news_dir.relative_to(workspace_root)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6828fc",
   "metadata": {},
   "source": [
    "## 8. Verify Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "if files_written:\n",
    "    # Check a sample file\n",
    "    sample_file = files_written[len(files_written)//2]\n",
    "    \n",
    "    print(f\"Sample file: {sample_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_sample = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"Shape: {df_sample.shape}\")\n",
    "    print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(df_sample[['date', 'title', 'url']].head(3).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ No files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4acc1",
   "metadata": {},
   "source": [
    "## 9. Save Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6422e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'source': {\n",
    "        'dataset': news_cfg['source']['hf_dataset'],\n",
    "        'access_method': 'streaming',\n",
    "        'year_subset': '2016'\n",
    "    },\n",
    "    'period': {\n",
    "        'start': start_date.strftime('%Y-%m-%d'),\n",
    "        'end': end_date.strftime('%Y-%m-%d'),\n",
    "        'days': total_days\n",
    "    },\n",
    "    'filters': {\n",
    "        'language': 'en'\n",
    "    },\n",
    "    'sampling': {\n",
    "        'method': news_cfg['sampling']['method'],\n",
    "        'hash_function': hash_function,\n",
    "        'seed': seed,\n",
    "        'daily_limit': daily_limit\n",
    "    },\n",
    "    'processing': {\n",
    "        'articles_streamed': int(articles_seen),\n",
    "        'articles_in_date_range': int(articles_in_range),\n",
    "        'articles_english': int(articles_english),\n",
    "        'total_sampled': int(df_stats['sampled'].sum())\n",
    "    },\n",
    "    'output': {\n",
    "        'directory': str(silver_news_dir.relative_to(workspace_root)),\n",
    "        'files_written': len(files_written),\n",
    "        'dates_covered': sorted([f.stem for f in files_written]),\n",
    "        'missing_dates': sorted(list(target_dates - set([f.stem for f in files_written])))\n",
    "    },\n",
    "    'daily_statistics': df_stats.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_file = silver_news_dir / 'processing_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved: {metadata_file.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10abc9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**CC-NEWS Sampling Complete!**\n",
    "\n",
    "- ✓ Streamed HuggingFace dataset (memory efficient)\n",
    "- ✓ Applied deterministic hash-based sampling\n",
    "- ✓ Selected top-10k articles per day (smallest hashes)\n",
    "- ✓ Written daily Parquet files\n",
    "- ✓ Saved processing metadata\n",
    "\n",
    "**Next Step:** \n",
    "- Filtering, deduplication, and enrichment (notebook 21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
