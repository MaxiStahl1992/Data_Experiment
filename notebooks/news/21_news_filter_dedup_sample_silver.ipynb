{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca67acff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News filtering started: 2025-12-18T23:13:21.015777\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"News filtering started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691a120",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ecc43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver layer: data/01_silver/news\n",
      "\n",
      "Quality filters to apply:\n",
      "  - Min text length: 200 characters\n",
      "  - Max text length: 100,000 characters\n",
      "  - Must have title\n",
      "  - URL deduplication\n",
      "  - Content deduplication (exact)\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "news_cfg = configs['news']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Data directory\n",
    "silver_dir = workspace_root / 'data/01_silver/news'\n",
    "\n",
    "print(f\"Silver layer: {silver_dir.relative_to(workspace_root)}\")\n",
    "print(f\"\\nQuality filters to apply:\")\n",
    "print(f\"  - Min text length: 200 characters\")\n",
    "print(f\"  - Max text length: 100,000 characters\")\n",
    "print(f\"  - Must have title\")\n",
    "print(f\"  - URL deduplication\")\n",
    "print(f\"  - Content deduplication (exact)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad007d",
   "metadata": {},
   "source": [
    "## 2. Load Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27365a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 daily files\n",
      "Date range: 2016-09-01 to 2016-10-31\n",
      "\n",
      "Loading all files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 61/61 [00:02<00:00, 23.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 93,737 articles\n",
      "  Columns: ['date', 'requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n",
      "  Date range: 2016-09-01 to 2016-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all files\n",
    "silver_files = sorted(silver_dir.glob('2016-*.parquet'))\n",
    "\n",
    "print(f\"Found {len(silver_files)} daily files\")\n",
    "print(f\"Date range: {silver_files[0].stem} to {silver_files[-1].stem}\")\n",
    "\n",
    "# Load all data\n",
    "print(\"\\nLoading all files...\")\n",
    "all_articles = []\n",
    "\n",
    "for file in tqdm(silver_files, desc=\"Reading files\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    all_articles.append(df)\n",
    "\n",
    "df_all = pd.concat(all_articles, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(df_all):,} articles\")\n",
    "print(f\"  Columns: {df_all.columns.tolist()}\")\n",
    "print(f\"  Date range: {df_all['date'].min()} to {df_all['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47145c",
   "metadata": {},
   "source": [
    "## 3. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa002166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality snapshot:\n",
      "\n",
      "Missing values:\n",
      "author       17436\n",
      "sitename        42\n",
      "image_url    13035\n",
      "dtype: int64\n",
      "\n",
      "Text length stats:\n",
      "count    93737.000000\n",
      "mean      2699.781623\n",
      "std       1807.386186\n",
      "min        300.000000\n",
      "25%       1276.000000\n",
      "50%       2344.000000\n",
      "75%       3698.000000\n",
      "max       9997.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Title presence: 93,737 / 93,737 (100.0%)\n",
      "Language distribution:\n",
      "language\n",
      "en    93737\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data quality snapshot:\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "missing = df_all.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Text length distribution\n",
    "df_all['text_length'] = df_all['plain_text'].fillna('').str.len()\n",
    "df_all['title_length'] = df_all['title'].fillna('').str.len()\n",
    "\n",
    "print(f\"\\nText length stats:\")\n",
    "print(df_all['text_length'].describe())\n",
    "\n",
    "print(f\"\\nTitle presence: {df_all['title'].notna().sum():,} / {len(df_all):,} ({100*df_all['title'].notna().sum()/len(df_all):.1f}%)\")\n",
    "print(f\"Language distribution:\")\n",
    "print(df_all['language'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3abc2",
   "metadata": {},
   "source": [
    "## 4. Apply Quality Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b677c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying quality filters...\n",
      "\n",
      "After title filter: 93,737 (100.0%)\n",
      "After text length filter: 93,737 (100.0%)\n",
      "After URL filter: 93,737 (100.0%)\n",
      "After language filter: 93,737 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "Articles remaining: 93,737 / 93,737\n",
      "Removed: 0 (0.0%)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying quality filters...\\n\")\n",
    "\n",
    "initial_count = len(df_all)\n",
    "\n",
    "# Filter 1: Must have title\n",
    "df_filtered = df_all[df_all['title'].notna() & (df_all['title'].str.len() > 0)].copy()\n",
    "print(f\"After title filter: {len(df_filtered):,} ({len(df_filtered)/initial_count*100:.1f}%)\")\n",
    "\n",
    "# Filter 2: Text length (200 to 100,000 chars)\n",
    "df_filtered = df_filtered[(df_filtered['text_length'] >= 200) & (df_filtered['text_length'] <= 100000)]\n",
    "print(f\"After text length filter: {len(df_filtered):,} ({len(df_filtered)/initial_count*100:.1f}%)\")\n",
    "\n",
    "# Filter 3: Must have URL\n",
    "df_filtered = df_filtered[df_filtered['requested_url'].notna()]\n",
    "print(f\"After URL filter: {len(df_filtered):,} ({len(df_filtered)/initial_count*100:.1f}%)\")\n",
    "\n",
    "# Filter 4: Language should be 'en' (should already be, but double-check)\n",
    "df_filtered = df_filtered[df_filtered['language'].str.lower() == 'en']\n",
    "print(f\"After language filter: {len(df_filtered):,} ({len(df_filtered)/initial_count*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Articles remaining: {len(df_filtered):,} / {initial_count:,}\")\n",
    "print(f\"Removed: {initial_count - len(df_filtered):,} ({(initial_count - len(df_filtered))/initial_count*100:.1f}%)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d6866",
   "metadata": {},
   "source": [
    "## 5. URL Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa1dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating by URL...\n",
      "\n",
      "Duplicate URLs found: 0\n",
      "After URL deduplication: 93,737\n",
      "Removed: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Deduplicating by URL...\\n\")\n",
    "\n",
    "before_dedup = len(df_filtered)\n",
    "\n",
    "# Check for duplicate URLs\n",
    "duplicate_urls = df_filtered['requested_url'].duplicated().sum()\n",
    "print(f\"Duplicate URLs found: {duplicate_urls:,}\")\n",
    "\n",
    "# Keep first occurrence (already has smallest hash from sampling)\n",
    "df_filtered = df_filtered.drop_duplicates(subset=['requested_url'], keep='first')\n",
    "\n",
    "print(f\"After URL deduplication: {len(df_filtered):,}\")\n",
    "print(f\"Removed: {before_dedup - len(df_filtered):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a28ea",
   "metadata": {},
   "source": [
    "## 6. Content Deduplication (Exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f497e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating by content hash...\n",
      "\n",
      "Duplicate content found: 68\n",
      "After content deduplication: 93,669\n",
      "Removed: 68\n"
     ]
    }
   ],
   "source": [
    "print(\"Deduplicating by content hash...\\n\")\n",
    "\n",
    "before_content_dedup = len(df_filtered)\n",
    "\n",
    "# Compute content hash (using first 1000 chars to be fast)\n",
    "df_filtered['content_hash'] = df_filtered['plain_text'].fillna('').str[:1000].apply(\n",
    "    lambda x: xxhash.xxh64(x.encode('utf-8')).hexdigest()\n",
    ")\n",
    "\n",
    "# Check duplicates\n",
    "duplicate_content = df_filtered['content_hash'].duplicated().sum()\n",
    "print(f\"Duplicate content found: {duplicate_content:,}\")\n",
    "\n",
    "# Keep first occurrence\n",
    "df_filtered = df_filtered.drop_duplicates(subset=['content_hash'], keep='first')\n",
    "\n",
    "# Drop the temporary hash column\n",
    "df_filtered = df_filtered.drop(columns=['content_hash'])\n",
    "\n",
    "print(f\"After content deduplication: {len(df_filtered):,}\")\n",
    "print(f\"Removed: {before_content_dedup - len(df_filtered):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f1e88",
   "metadata": {},
   "source": [
    "## 7. Final Statistics by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50efab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles per day after filtering and deduplication:\n",
      "\n",
      "      date  count\n",
      "2016-09-01    124\n",
      "2016-09-02    104\n",
      "2016-09-03     53\n",
      "2016-09-04     59\n",
      "2016-09-05     58\n",
      "2016-09-06    106\n",
      "2016-09-07     99\n",
      "2016-09-08    141\n",
      "2016-09-09    111\n",
      "2016-09-10     50\n",
      "2016-09-11    354\n",
      "2016-09-12    288\n",
      "2016-09-13    161\n",
      "2016-09-14    108\n",
      "2016-09-15    151\n",
      "2016-09-16    140\n",
      "2016-09-17     67\n",
      "2016-09-18     72\n",
      "2016-09-19    125\n",
      "2016-09-20    147\n",
      "2016-09-21    175\n",
      "2016-09-22    134\n",
      "2016-09-23    118\n",
      "2016-09-24     50\n",
      "2016-09-25     66\n",
      "2016-09-26    124\n",
      "2016-09-27    141\n",
      "2016-09-28    148\n",
      "2016-09-29    132\n",
      "2016-09-30    193\n",
      "2016-10-01     71\n",
      "2016-10-02     77\n",
      "2016-10-03    171\n",
      "2016-10-04    152\n",
      "2016-10-05    190\n",
      "2016-10-06    192\n",
      "2016-10-07    160\n",
      "2016-10-08     81\n",
      "2016-10-09     85\n",
      "2016-10-10    135\n",
      "2016-10-11    342\n",
      "2016-10-12    230\n",
      "2016-10-13    243\n",
      "2016-10-14    225\n",
      "2016-10-15     75\n",
      "2016-10-16    150\n",
      "2016-10-17   3835\n",
      "2016-10-18   7098\n",
      "2016-10-19   7449\n",
      "2016-10-20   7481\n",
      "2016-10-21   6898\n",
      "2016-10-22   3789\n",
      "2016-10-23   3697\n",
      "2016-10-24   6281\n",
      "2016-10-25   6841\n",
      "2016-10-26   7225\n",
      "2016-10-27   7122\n",
      "2016-10-28   6702\n",
      "2016-10-29   3561\n",
      "2016-10-30   3266\n",
      "2016-10-31   6046\n",
      "\n",
      "================================================================================\n",
      "Total articles: 93,669\n",
      "Average per day: 1536\n",
      "Min per day: 50\n",
      "Max per day: 7481\n",
      "Days with data: 61\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Count articles per day\n",
    "daily_counts = df_filtered.groupby('date').size().reset_index(name='count')\n",
    "daily_counts = daily_counts.sort_values('date')\n",
    "\n",
    "print(\"Articles per day after filtering and deduplication:\\n\")\n",
    "print(daily_counts.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total articles: {len(df_filtered):,}\")\n",
    "print(f\"Average per day: {daily_counts['count'].mean():.0f}\")\n",
    "print(f\"Min per day: {daily_counts['count'].min()}\")\n",
    "print(f\"Max per day: {daily_counts['count'].max()}\")\n",
    "print(f\"Days with data: {len(daily_counts)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f30328",
   "metadata": {},
   "source": [
    "## 8. Write Cleaned Data Back to Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d135a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cleaned data back to silver layer...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing files: 100%|██████████| 61/61 [00:01<00:00, 57.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Files written: 61\n",
      "✓ Output directory: data/01_silver/news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing cleaned data back to silver layer...\\n\")\n",
    "\n",
    "# Drop temporary columns\n",
    "if 'text_length' in df_filtered.columns:\n",
    "    df_filtered = df_filtered.drop(columns=['text_length'])\n",
    "if 'title_length' in df_filtered.columns:\n",
    "    df_filtered = df_filtered.drop(columns=['title_length'])\n",
    "\n",
    "files_written = []\n",
    "\n",
    "for date, group in tqdm(df_filtered.groupby('date'), desc=\"Writing files\"):\n",
    "    output_file = silver_dir / f\"{date}.parquet\"\n",
    "    group.to_parquet(output_file, compression='snappy', index=False)\n",
    "    files_written.append(output_file)\n",
    "\n",
    "print(f\"\\n✓ Files written: {len(files_written)}\")\n",
    "print(f\"✓ Output directory: {silver_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a31b5d",
   "metadata": {},
   "source": [
    "## 9. Verify Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94008d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: 2016-10-01.parquet\n",
      "================================================================================\n",
      "Shape: (71, 16)\n",
      "Columns: ['date', 'requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n",
      "\n",
      "First 2 articles:\n",
      "\n",
      "  Date: 2016-10-01\n",
      "  Title: 10 Propositions for Research-Creation...\n",
      "  URL: http://quod.lib.umich.edu/j/jep/3336451.0019.206?view=text;rgn=main...\n",
      "  Text length: 696 chars\n",
      "\n",
      "  Date: 2016-10-01\n",
      "  Title: Judge blocks pro-abortion law in Illinois...\n",
      "  URL: https://world.wng.org/2016/12/judge_blocks_pro_abortion_law_in_illinois...\n",
      "  Text length: 3672 chars\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if files_written:\n",
    "    sample_file = files_written[len(files_written)//2]\n",
    "    \n",
    "    print(f\"Sample file: {sample_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_sample = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"Shape: {df_sample.shape}\")\n",
    "    print(f\"Columns: {df_sample.columns.tolist()}\")\n",
    "    print(f\"\\nFirst 2 articles:\")\n",
    "    for idx, row in df_sample.head(2).iterrows():\n",
    "        print(f\"\\n  Date: {row['date']}\")\n",
    "        print(f\"  Title: {row['title'][:80]}...\")\n",
    "        print(f\"  URL: {row['requested_url'][:80]}...\")\n",
    "        print(f\"  Text length: {len(row['plain_text'])} chars\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ No files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39cd27",
   "metadata": {},
   "source": [
    "## 10. Save Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defe0972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metadata saved: data/01_silver/news/filtering_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Update metadata\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'processing_stage': 'filtering_and_deduplication',\n",
    "    'input': {\n",
    "        'articles_initial': int(initial_count)\n",
    "    },\n",
    "    'filters_applied': {\n",
    "        'must_have_title': True,\n",
    "        'min_text_length': 200,\n",
    "        'max_text_length': 100000,\n",
    "        'must_have_url': True,\n",
    "        'language': 'en'\n",
    "    },\n",
    "    'deduplication': {\n",
    "        'url_duplicates_removed': int(before_dedup - len(df_filtered)),\n",
    "        'content_duplicates_removed': int(duplicate_content),\n",
    "        'method': 'exact_match'\n",
    "    },\n",
    "    'output': {\n",
    "        'articles_final': int(len(df_filtered)),\n",
    "        'articles_removed': int(initial_count - len(df_filtered)),\n",
    "        'removal_rate': round((initial_count - len(df_filtered)) / initial_count * 100, 2),\n",
    "        'files_written': len(files_written),\n",
    "        'avg_per_day': round(daily_counts['count'].mean(), 1)\n",
    "    },\n",
    "    'daily_counts': daily_counts.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_file = silver_dir / 'filtering_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Metadata saved: {metadata_file.relative_to(workspace_root)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
