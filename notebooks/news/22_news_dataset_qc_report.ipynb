{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1676469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News QC report generation started: 2025-12-19T09:22:28.704475\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"News QC report generation started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f80980",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471ae9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver layer: data/01_silver/news\n",
      "Report output: reports/data_validation/2016-09_2016-10/news\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "news_cfg = configs['news']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Paths\n",
    "silver_dir = workspace_root / 'data/01_silver/news'\n",
    "report_dir = workspace_root / 'reports/data_validation/2016-09_2016-10/news'\n",
    "figures_dir = report_dir / 'figures'\n",
    "tables_dir = report_dir / 'tables'\n",
    "\n",
    "# Create directories\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "tables_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Silver layer: {silver_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Report output: {report_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ea7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 daily files\n",
      "Loading all data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:01<00:00, 55.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Loaded 93,669 articles\n",
      "  Date range: 2016-09-01 to 2016-10-31\n",
      "  Columns: ['date', 'requested_url', 'plain_text', 'published_date', 'title', 'tags', 'categories', 'author', 'sitename', 'image_url', 'language', 'language_score', 'responded_url', 'publisher', 'warc_path', 'crawl_date']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all parquet files\n",
    "silver_files = sorted(silver_dir.glob('2016-*.parquet'))\n",
    "print(f\"Found {len(silver_files)} daily files\")\n",
    "\n",
    "print(\"Loading all data...\")\n",
    "all_articles = []\n",
    "\n",
    "for file in tqdm(silver_files, desc=\"Reading files\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    all_articles.append(df)\n",
    "\n",
    "df = pd.concat(all_articles, ignore_index=True)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(df):,} articles\")\n",
    "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"  Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99868058",
   "metadata": {},
   "source": [
    "## 2. Temporal Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7199fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Coverage:\n",
      "  Expected: 61 days (2016-09-01 to 2016-10-31)\n",
      "  Actual: 61 days\n",
      "  Coverage: 100.0%\n",
      "\n",
      "âœ“ Complete coverage (no missing dates)\n"
     ]
    }
   ],
   "source": [
    "# Expected date range\n",
    "start_date = '2016-09-01'\n",
    "end_date = '2016-10-31'\n",
    "expected_dates = pd.date_range(start_date, end_date, freq='D')\n",
    "expected_dates_str = [d.strftime('%Y-%m-%d') for d in expected_dates]\n",
    "\n",
    "# Actual dates\n",
    "actual_dates = sorted(df['date'].unique())\n",
    "\n",
    "# Coverage\n",
    "coverage_pct = len(actual_dates) / len(expected_dates_str) * 100\n",
    "missing_dates = set(expected_dates_str) - set(actual_dates)\n",
    "\n",
    "print(f\"Temporal Coverage:\")\n",
    "print(f\"  Expected: {len(expected_dates_str)} days ({start_date} to {end_date})\")\n",
    "print(f\"  Actual: {len(actual_dates)} days\")\n",
    "print(f\"  Coverage: {coverage_pct:.1f}%\")\n",
    "\n",
    "if missing_dates:\n",
    "    print(f\"\\nâš  Missing dates ({len(missing_dates)}): {sorted(missing_dates)}\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Complete coverage (no missing dates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b07694",
   "metadata": {},
   "source": [
    "## 3. Daily Article Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da07d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily article statistics:\n",
      "  Total articles: 93,669\n",
      "  Mean per day: 1535.6\n",
      "  Median per day: 150\n",
      "  Min per day: 50\n",
      "  Max per day: 7481\n",
      "  Std dev: 2593.0\n",
      "\n",
      "âœ“ Saved: reports/data_validation/2016-09_2016-10/news/tables/daily_article_counts.csv\n"
     ]
    }
   ],
   "source": [
    "# Count by date\n",
    "daily_counts = df.groupby('date').size().reset_index(name='count')\n",
    "daily_counts = daily_counts.sort_values('date')\n",
    "\n",
    "print(\"Daily article statistics:\")\n",
    "print(f\"  Total articles: {len(df):,}\")\n",
    "print(f\"  Mean per day: {daily_counts['count'].mean():.1f}\")\n",
    "print(f\"  Median per day: {daily_counts['count'].median():.0f}\")\n",
    "print(f\"  Min per day: {daily_counts['count'].min()}\")\n",
    "print(f\"  Max per day: {daily_counts['count'].max()}\")\n",
    "print(f\"  Std dev: {daily_counts['count'].std():.1f}\")\n",
    "\n",
    "# Save table\n",
    "daily_counts.to_csv(tables_dir / 'daily_article_counts.csv', index=False)\n",
    "print(f\"\\nâœ“ Saved: {(tables_dir / 'daily_article_counts.csv').relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59397488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved: reports/data_validation/2016-09_2016-10/news/figures/daily_article_counts.png\n"
     ]
    }
   ],
   "source": [
    "# Plot daily counts\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(daily_counts['date'], daily_counts['count'], marker='o', linewidth=1.5, markersize=3)\n",
    "ax.axhline(y=daily_counts['count'].mean(), color='r', linestyle='--', label=f'Mean: {daily_counts[\"count\"].mean():.0f}')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Article Count')\n",
    "ax.set_title('Daily News Article Counts: Sep-Oct 2016')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(figures_dir / 'daily_article_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {(figures_dir / 'daily_article_counts.png').relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965629d4",
   "metadata": {},
   "source": [
    "## 4. Text Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5074c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length statistics:\n",
      "\n",
      "Article text:\n",
      "count    93669.000000\n",
      "mean      2698.933382\n",
      "std       1806.848335\n",
      "min        300.000000\n",
      "25%       1275.000000\n",
      "50%       2343.000000\n",
      "75%       3698.000000\n",
      "max       9997.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Article title:\n",
      "count    93669.000000\n",
      "mean        57.756995\n",
      "std         21.970231\n",
      "min          3.000000\n",
      "25%         44.000000\n",
      "50%         57.000000\n",
      "75%         69.000000\n",
      "max        264.000000\n",
      "Name: title_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute text lengths\n",
    "df['text_length'] = df['plain_text'].str.len()\n",
    "df['title_length'] = df['title'].str.len()\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(\"\\nArticle text:\")\n",
    "print(df['text_length'].describe())\n",
    "print(\"\\nArticle title:\")\n",
    "print(df['title_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d5e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved: reports/data_validation/2016-09_2016-10/news/figures/text_length_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# Plot text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Article text length\n",
    "axes[0].hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['text_length'].median(), color='r', linestyle='--', label=f'Median: {df[\"text_length\"].median():.0f}')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Article Text Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Title length\n",
    "axes[1].hist(df['title_length'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].axvline(df['title_length'].median(), color='r', linestyle='--', label=f'Median: {df[\"title_length\"].median():.0f}')\n",
    "axes[1].set_xlabel('Title Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Article Title Length Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(figures_dir / 'text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {(figures_dir / 'text_length_distribution.png').relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e24d74",
   "metadata": {},
   "source": [
    "## 5. Domain/Source Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d189049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique domains: 1,957\n",
      "\n",
      "Top 30 domains:\n",
      "domain\n",
      "thehindu.com                   3735\n",
      "theguardian.com                2825\n",
      "modernghana.com                2269\n",
      "irishtimes.com                 2263\n",
      "seattletimes.com               2067\n",
      "dailyrecord.co.uk              2005\n",
      "firmenpresse.de                1710\n",
      "theglobeandmail.com            1610\n",
      "metronews.ca                   1566\n",
      "finanznachrichten.de           1269\n",
      "manchestereveningnews.co.uk    1251\n",
      "cnn.com                        1159\n",
      "reuters.com                    1143\n",
      "scotsman.com                   1061\n",
      "nation.co.ke                    985\n",
      "arkansasonline.com              941\n",
      "reviewjournal.com               927\n",
      "taipeitimes.com                 875\n",
      "yorkshirepost.co.uk             832\n",
      "timesofmalta.com                828\n",
      "mercedsunstar.com               814\n",
      "miamiherald.com                 805\n",
      "sunherald.com                   794\n",
      "ledger-enquirer.com             681\n",
      "grandforksherald.com            678\n",
      "charlotteobserver.com           657\n",
      "bnd.com                         618\n",
      "tribuneonlineng.com             607\n",
      "manilatimes.net                 594\n",
      "thenewstribune.com              591\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ“ Saved: reports/data_validation/2016-09_2016-10/news/tables/top_30_domains.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract domains from URLs\n",
    "def extract_domain(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        # Remove www. prefix\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['domain'] = df['requested_url'].apply(extract_domain)\n",
    "\n",
    "# Top domains\n",
    "top_domains = df['domain'].value_counts().head(30)\n",
    "\n",
    "print(f\"Unique domains: {df['domain'].nunique():,}\")\n",
    "print(f\"\\nTop 30 domains:\")\n",
    "print(top_domains)\n",
    "\n",
    "# Save table\n",
    "top_domains.to_csv(tables_dir / 'top_30_domains.csv', header=['count'])\n",
    "print(f\"\\nâœ“ Saved: {(tables_dir / 'top_30_domains.csv').relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72debe2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved: reports/data_validation/2016-09_2016-10/news/figures/top_domains.png\n"
     ]
    }
   ],
   "source": [
    "# Plot top 20 domains\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_20 = df['domain'].value_counts().head(20)\n",
    "ax.barh(range(len(top_20)), top_20.values, color='steelblue')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20.index)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Article Count')\n",
    "ax.set_title('Top 20 News Domains')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(figures_dir / 'top_domains.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Saved: {(figures_dir / 'top_domains.png').relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb72e4",
   "metadata": {},
   "source": [
    "## 6. Language Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da50a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language distribution:\n",
      "language\n",
      "en    93669\n",
      "Name: count, dtype: int64\n",
      "\n",
      "English articles: 93,669 (100.00%)\n",
      "\n",
      "âœ“ All articles are in English\n"
     ]
    }
   ],
   "source": [
    "# Language distribution\n",
    "language_counts = df['language'].value_counts()\n",
    "\n",
    "print(\"Language distribution:\")\n",
    "print(language_counts)\n",
    "\n",
    "english_pct = (df['language'].str.lower() == 'en').sum() / len(df) * 100\n",
    "print(f\"\\nEnglish articles: {(df['language'].str.lower() == 'en').sum():,} ({english_pct:.2f}%)\")\n",
    "\n",
    "if english_pct < 99.9:\n",
    "    print(f\"\\nâš  Non-English articles detected:\")\n",
    "    non_english = df[df['language'].str.lower() != 'en']\n",
    "    print(non_english[['date', 'title', 'language']].head(10))\n",
    "else:\n",
    "    print(f\"\\nâœ“ All articles are in English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae57c20",
   "metadata": {},
   "source": [
    "## 7. Data Completeness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4554ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data completeness:\n",
      "\n",
      "Missing values per column:\n",
      "           missing_count  missing_pct\n",
      "author             17422        18.60\n",
      "sitename              42         0.04\n",
      "image_url          13034        13.91\n",
      "\n",
      "Required fields check:\n",
      "  âœ“ date: complete\n",
      "  âœ“ title: complete\n",
      "  âœ“ plain_text: complete\n",
      "  âœ“ requested_url: complete\n"
     ]
    }
   ],
   "source": [
    "# Check for missing/null values\n",
    "print(\"Data completeness:\")\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "completeness_df = pd.DataFrame({\n",
    "    'missing_count': missing,\n",
    "    'missing_pct': missing_pct\n",
    "})\n",
    "print(completeness_df[completeness_df['missing_count'] > 0])\n",
    "\n",
    "# Required fields\n",
    "required_fields = ['date', 'title', 'plain_text', 'requested_url']\n",
    "print(f\"\\nRequired fields check:\")\n",
    "for field in required_fields:\n",
    "    null_count = df[field].isnull().sum()\n",
    "    if null_count == 0:\n",
    "        print(f\"  âœ“ {field}: complete\")\n",
    "    else:\n",
    "        print(f\"  âœ— {field}: {null_count} missing ({null_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0214d1d",
   "metadata": {},
   "source": [
    "## 8. Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b0ef350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample articles:\n",
      "\n",
      "================================================================================\n",
      "Date: 2016-10-18\n",
      "Title: Mattapoisett Bike Path\n",
      "Domain: wanderer.com\n",
      "URL: http://www.wanderer.com/happenings/mattapoisett-bike-path-5/...\n",
      "Text length: 845 chars\n",
      "Text preview: The Mattapoisett Bike Path project has reached and passed an important milestone in the project process. The environmental permitting process began in early September with a submission to MEPA, the Ma...\n",
      "\n",
      "================================================================================\n",
      "Date: 2016-09-12\n",
      "Title: Noahide Lecture: The purpose behind the test\n",
      "Domain: israelnationalnews.com\n",
      "URL: http://www.israelnationalnews.com/News/News.aspx/221500...\n",
      "Text length: 1065 chars\n",
      "Text preview: Noahide Lecture: The purpose behind the test What we can learn from Jacob's travel and his tests in Haran? Contact Editor Rod Bryant, 09/12/16 14:20 Rod BryantBy PR Discover the beauty of lifeâ€™s test:...\n",
      "\n",
      "================================================================================\n",
      "Date: 2016-10-28\n",
      "Title: Air Canada flight bound for Montreal lands in London following medical emergency\n",
      "Domain: theglobeandmail.com\n",
      "URL: http://www.theglobeandmail.com/news/national/air-canada-flight-bound-for-montreal-lands-in-london-fo...\n",
      "Text length: 540 chars\n",
      "Text preview: A Montreal-bound Air Canada flight was briefly diverted to London this morning due to an onboard medical emergency with a passenger. A spokeswoman for the airline says the plane left Rome but was forc...\n",
      "\n",
      "================================================================================\n",
      "Date: 2016-10-21\n",
      "Title: Terrence Malickâ€™s Voyage of Time a lyrical account of Earthâ€™s evolution\n",
      "Domain: theglobeandmail.com\n",
      "URL: http://www.theglobeandmail.com/arts/film/film-reviews/terrence-malicks-voyage-of-time-a-lyrical-acco...\n",
      "Text length: 1185 chars\n",
      "Text preview: As Brad Pittâ€™s pleasant voice lulls you into an appreciation of a deep meaning you canâ€™t quite grasp, vast images of the exploding cosmos, the primordial oceans and the original plains fill the giant ...\n",
      "\n",
      "================================================================================\n",
      "Date: 2016-10-28\n",
      "Title: Mackay's support for Adani could sway federal politicians\n",
      "Domain: dailymercury.com.au\n",
      "URL: http://www.dailymercury.com.au/news/mackays-support-for-adani-could-sway-federal-polit/3105303/...\n",
      "Text length: 3421 chars\n",
      "Text preview: THE right to challenge government approvals in court could be taken away from environmental groups before the end of the year.  After connections between a US Presidential advisor and Australian group...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show random sample\n",
    "sample = df.sample(n=min(5, len(df)), random_state=42)\n",
    "\n",
    "print(\"Sample articles:\\n\")\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Date: {row['date']}\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Domain: {row['domain']}\")\n",
    "    print(f\"URL: {row['requested_url'][:100]}...\")\n",
    "    print(f\"Text length: {row['text_length']} chars\")\n",
    "    print(f\"Text preview: {row['plain_text'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcac2e8",
   "metadata": {},
   "source": [
    "## 9. Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a1cc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved: reports/data_validation/2016-09_2016-10/news/summary_statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Compile summary statistics\n",
    "summary_stats = {\n",
    "    'temporal_coverage': {\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'expected_days': len(expected_dates_str),\n",
    "        'actual_days': len(actual_dates),\n",
    "        'coverage_pct': round(coverage_pct, 2),\n",
    "        'missing_dates': sorted(list(missing_dates))\n",
    "    },\n",
    "    'dataset_size': {\n",
    "        'total_articles': int(len(df)),\n",
    "        'articles_per_day': {\n",
    "            'mean': round(daily_counts['count'].mean(), 1),\n",
    "            'median': int(daily_counts['count'].median()),\n",
    "            'min': int(daily_counts['count'].min()),\n",
    "            'max': int(daily_counts['count'].max()),\n",
    "            'std': round(daily_counts['count'].std(), 1)\n",
    "        }\n",
    "    },\n",
    "    'text_statistics': {\n",
    "        'article_text_length': {\n",
    "            'mean': round(df['text_length'].mean(), 1),\n",
    "            'median': int(df['text_length'].median()),\n",
    "            'min': int(df['text_length'].min()),\n",
    "            'max': int(df['text_length'].max())\n",
    "        },\n",
    "        'title_length': {\n",
    "            'mean': round(df['title_length'].mean(), 1),\n",
    "            'median': int(df['title_length'].median()),\n",
    "            'min': int(df['title_length'].min()),\n",
    "            'max': int(df['title_length'].max())\n",
    "        }\n",
    "    },\n",
    "    'sources': {\n",
    "        'unique_domains': int(df['domain'].nunique()),\n",
    "        'top_10_domains': top_domains.head(10).to_dict()\n",
    "    },\n",
    "    'language': {\n",
    "        'distribution': language_counts.to_dict(),\n",
    "        'english_pct': round(english_pct, 2)\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'completeness': completeness_df.to_dict()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "stats_file = report_dir / 'summary_statistics.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved: {stats_file.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d65331",
   "metadata": {},
   "source": [
    "## 10. Generate Markdown Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f4b3b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ“ QC Report generated: reports/data_validation/2016-09_2016-10/news/qc_summary.md\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive markdown report\n",
    "report_lines = [\n",
    "    \"# News Dataset Quality Control Report\",\n",
    "    \"\",\n",
    "    f\"**Dataset Period:** September - October 2016\",\n",
    "    f\"**Report Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"**Source:** CC-NEWS via HuggingFace\",\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 1. Executive Summary\",\n",
    "    \"\",\n",
    "    f\"- **Total Articles:** {len(df):,}\",\n",
    "    f\"- **Date Range:** {df['date'].min()} to {df['date'].max()}\",\n",
    "    f\"- **Temporal Coverage:** {len(actual_dates)}/{len(expected_dates_str)} days ({coverage_pct:.1f}%)\",\n",
    "    f\"- **Unique Domains:** {df['domain'].nunique():,}\",\n",
    "    f\"- **Language:** {english_pct:.2f}% English\",\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 2. Temporal Coverage\",\n",
    "    \"\",\n",
    "    f\"- **Expected Days:** {len(expected_dates_str)} (2016-09-01 to 2016-10-31)\",\n",
    "    f\"- **Actual Days:** {len(actual_dates)}\",\n",
    "    f\"- **Coverage:** {coverage_pct:.1f}%\",\n",
    "]\n",
    "\n",
    "if missing_dates:\n",
    "    report_lines.append(f\"- **Missing Dates ({len(missing_dates)}):** {', '.join(sorted(missing_dates))}\")\n",
    "else:\n",
    "    report_lines.append(\"- **Status:** âœ“ Complete coverage (no missing dates)\")\n",
    "\n",
    "report_lines.extend([\n",
    "    \"\",\n",
    "    \"### Daily Article Counts\",\n",
    "    \"\",\n",
    "    f\"- **Mean:** {daily_counts['count'].mean():.1f} articles/day\",\n",
    "    f\"- **Median:** {daily_counts['count'].median():.0f} articles/day\",\n",
    "    f\"- **Range:** {daily_counts['count'].min()} - {daily_counts['count'].max()} articles/day\",\n",
    "    f\"- **Std Dev:** {daily_counts['count'].std():.1f}\",\n",
    "    \"\",\n",
    "    \"![Daily Article Counts](figures/daily_article_counts.png)\",\n",
    "    \"\",\n",
    "    \"ðŸ“Š **See detailed table:** [daily_article_counts.csv](tables/daily_article_counts.csv)\",\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 3. Text Length Analysis\",\n",
    "    \"\",\n",
    "    \"### Article Text\",\n",
    "    \"\",\n",
    "    f\"- **Mean Length:** {df['text_length'].mean():.0f} characters\",\n",
    "    f\"- **Median Length:** {df['text_length'].median():.0f} characters\",\n",
    "    f\"- **Range:** {df['text_length'].min()} - {df['text_length'].max():,} characters\",\n",
    "    \"\",\n",
    "    \"### Article Titles\",\n",
    "    \"\",\n",
    "    f\"- **Mean Length:** {df['title_length'].mean():.0f} characters\",\n",
    "    f\"- **Median Length:** {df['title_length'].median():.0f} characters\",\n",
    "    f\"- **Range:** {df['title_length'].min()} - {df['title_length'].max()} characters\",\n",
    "    \"\",\n",
    "    \"![Text Length Distribution](figures/text_length_distribution.png)\",\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 4. Domain/Source Analysis\",\n",
    "    \"\",\n",
    "    f\"- **Unique Domains:** {df['domain'].nunique():,}\",\n",
    "    f\"- **Top Domain:** {top_domains.index[0]} ({top_domains.values[0]:,} articles)\",\n",
    "    \"\",\n",
    "    \"### Top 10 Domains\",\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "for i, (domain, count) in enumerate(top_domains.head(10).items(), 1):\n",
    "    report_lines.append(f\"{i}. **{domain}**: {count:,} articles\")\n",
    "\n",
    "report_lines.extend([\n",
    "    \"\",\n",
    "    \"![Top Domains](figures/top_domains.png)\",\n",
    "    \"\",\n",
    "    \"ðŸ“Š **See full list:** [top_30_domains.csv](tables/top_30_domains.csv)\",\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 5. Language Distribution\",\n",
    "    \"\",\n",
    "    f\"- **English:** {(df['language'].str.lower() == 'en').sum():,} articles ({english_pct:.2f}%)\",\n",
    "])\n",
    "\n",
    "if english_pct < 99.9:\n",
    "    for lang, count in language_counts.items():\n",
    "        if lang.lower() != 'en':\n",
    "            report_lines.append(f\"- **{lang}:** {count:,} articles ({count/len(df)*100:.2f}%)\")\n",
    "else:\n",
    "    report_lines.append(\"- **Status:** âœ“ All articles are in English\")\n",
    "\n",
    "report_lines.extend([\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 6. Data Completeness\",\n",
    "    \"\",\n",
    "    \"### Required Fields\",\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "for field in required_fields:\n",
    "    null_count = df[field].isnull().sum()\n",
    "    if null_count == 0:\n",
    "        report_lines.append(f\"- âœ“ **{field}**: Complete (0 missing)\")\n",
    "    else:\n",
    "        report_lines.append(f\"- âœ— **{field}**: {null_count:,} missing ({null_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "report_lines.extend([\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    \"## 7. Summary\",\n",
    "    \"\",\n",
    "    \"### Dataset Quality Assessment\",\n",
    "    \"\",\n",
    "    f\"âœ“ **Temporal Coverage:** {coverage_pct:.1f}% ({len(actual_dates)}/{len(expected_dates_str)} days)\",\n",
    "    f\"âœ“ **Dataset Size:** {len(df):,} articles\",\n",
    "    f\"âœ“ **Language Quality:** {english_pct:.2f}% English\",\n",
    "    f\"âœ“ **Source Diversity:** {df['domain'].nunique():,} unique domains\",\n",
    "    f\"âœ“ **Data Completeness:** All required fields present\",\n",
    "    \"\",\n",
    "    \"### Files Generated\",\n",
    "    \"\",\n",
    "    \"- `summary_statistics.json` - Complete statistics in JSON format\",\n",
    "    \"- `figures/daily_article_counts.png` - Daily counts visualization\",\n",
    "    \"- `figures/text_length_distribution.png` - Text length analysis\",\n",
    "    \"- `figures/top_domains.png` - Top domains visualization\",\n",
    "    \"- `tables/daily_article_counts.csv` - Daily counts data\",\n",
    "    \"- `tables/top_30_domains.csv` - Top 30 domains data\",\n",
    "    \"\",\n",
    "    \"---\",\n",
    "    \"\",\n",
    "    f\"**Report completed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "])\n",
    "\n",
    "# Write report\n",
    "report_file = report_dir / 'qc_summary.md'\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ“ QC Report generated: {report_file.relative_to(workspace_root)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
