{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c141dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n",
      "Python version: 3.12.0 (v3.12.0:0fb18b02c8, Oct  2 2023, 09:45:56) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "print(f\"Project root: {workspace_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"✓ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa88123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Thesis pipeline utilities\n",
    "from thesis_pipeline.io.paths import get_data_path\n",
    "from thesis_pipeline.io.parquet import read_parquet, write_parquet\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacfd7c4",
   "metadata": {},
   "source": [
    "## 1. Load Thread Pseudo-Documents\n",
    "\n",
    "Load the corpus prepared in notebook 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8befb04",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_data_path() got multiple values for argument 'platform'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Paths\u001b[39;00m\n\u001b[32m      2\u001b[39m corpus_path = get_data_path(\u001b[33m'\u001b[39m\u001b[33mqa\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mreddit\u001b[39m\u001b[33m'\u001b[39m) / \u001b[33m'\u001b[39m\u001b[33mthread_pseudodocs.parquet\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m output_path = \u001b[43mget_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcustom\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m02_topics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreddit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCorpus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorpus_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: get_data_path() got multiple values for argument 'platform'"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "corpus_path = get_data_path('qa', 'reddit') / 'thread_pseudodocs.parquet'\n",
    "output_path = get_data_path('custom', '02_topics', platform='reddit', create=True)\n",
    "\n",
    "print(f\"Corpus: {corpus_path}\")\n",
    "print(f\"Output: {output_path}\")\n",
    "print(f\"Corpus exists: {corpus_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107776e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pseudo-documents\n",
    "print(\"Loading thread pseudo-documents...\")\n",
    "df = read_parquet(corpus_path)\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} thread pseudo-documents\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nDate range:\")\n",
    "dates = pd.to_datetime(df['created_utc'], unit='s')\n",
    "print(f\"  {dates.min().date()} to {dates.max().date()}\")\n",
    "print(f\"\\nComment statistics:\")\n",
    "print(f\"  Mean comments/thread: {df['n_comments'].mean():.1f}\")\n",
    "print(f\"  Median comments/thread: {df['n_comments'].median():.0f}\")\n",
    "print(f\"  Total comments: {df['n_comments'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed57e5",
   "metadata": {},
   "source": [
    "## 2. Define Target Topics\n",
    "\n",
    "Define 5 political topics with multiple text variations to capture different phrasings and aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8daeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define topics with multiple query variations\n",
    "# These will be embedded and averaged to create robust topic representations\n",
    "\n",
    "topic_definitions = {\n",
    "    'climate_change': {\n",
    "        'label': 'Climate Change',\n",
    "        'description': 'Climate change, global warming, environmental policy, carbon emissions, renewable energy',\n",
    "        'queries': [\n",
    "            'climate change and global warming',\n",
    "            'environmental protection and carbon emissions',\n",
    "            'renewable energy and fossil fuels',\n",
    "            'climate science and temperature rise',\n",
    "            'greenhouse gases and climate policy'\n",
    "        ]\n",
    "    },\n",
    "    'donald_trump': {\n",
    "        'label': 'Donald Trump',\n",
    "        'description': 'Donald Trump, his presidency, policies, and actions',\n",
    "        'queries': [\n",
    "            'Donald Trump and his presidency',\n",
    "            'Trump administration policies',\n",
    "            'President Trump political actions',\n",
    "            'Trump executive orders and decisions',\n",
    "            'Donald Trump campaign and election'\n",
    "        ]\n",
    "    },\n",
    "    'gun_control': {\n",
    "        'label': 'Gun Control',\n",
    "        'description': 'Gun control, Second Amendment, firearm regulations, mass shootings',\n",
    "        'queries': [\n",
    "            'gun control and firearm regulations',\n",
    "            'Second Amendment rights and gun ownership',\n",
    "            'mass shootings and gun violence',\n",
    "            'background checks and gun laws',\n",
    "            'assault weapons and gun restrictions'\n",
    "        ]\n",
    "    },\n",
    "    'immigration': {\n",
    "        'label': 'Immigration',\n",
    "        'description': 'Immigration policy, border control, refugees, DACA, travel ban',\n",
    "        'queries': [\n",
    "            'immigration policy and border security',\n",
    "            'illegal immigration and border wall',\n",
    "            'refugees and asylum seekers',\n",
    "            'DACA and dreamers immigration',\n",
    "            'immigration reform and deportation'\n",
    "        ]\n",
    "    },\n",
    "    'vaccination': {\n",
    "        'label': 'Vaccination',\n",
    "        'description': 'Vaccination policy, vaccine mandates, anti-vax movement, public health',\n",
    "        'queries': [\n",
    "            'vaccination and vaccine policy',\n",
    "            'mandatory vaccines and immunization',\n",
    "            'anti-vaccination movement and vaccine hesitancy',\n",
    "            'vaccine safety and public health',\n",
    "            'vaccine mandates and exemptions'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Defined topics:\")\n",
    "for topic_id, topic_info in topic_definitions.items():\n",
    "    print(f\"  {topic_id}: {topic_info['label']}\")\n",
    "    print(f\"    {topic_info['description']}\")\n",
    "    print(f\"    {len(topic_info['queries'])} query variations\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12a938",
   "metadata": {},
   "source": [
    "## 3. Load Sentence Embedding Model\n",
    "\n",
    "Using **all-MiniLM-L6-v2** for efficient semantic similarity:\n",
    "- Fast inference (~14k sentences/second on CPU)\n",
    "- Good semantic understanding\n",
    "- Widely used in research (Reimers & Gurevych, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentence embedding model\n",
    "print(\"Loading sentence embedding model...\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"✓ Loaded model: {model_name}\")\n",
    "print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Max sequence length: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae51d5b",
   "metadata": {},
   "source": [
    "## 4. Create Topic Embeddings\n",
    "\n",
    "Embed all query variations for each topic and average them to create robust topic representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topic embeddings by averaging query embeddings\n",
    "print(\"Creating topic embeddings...\\n\")\n",
    "\n",
    "topic_embeddings = {}\n",
    "\n",
    "for topic_id, topic_info in topic_definitions.items():\n",
    "    print(f\"Embedding topic: {topic_info['label']}\")\n",
    "    \n",
    "    # Embed all query variations\n",
    "    query_embeddings = model.encode(\n",
    "        topic_info['queries'],\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Average query embeddings to get topic representation\n",
    "    topic_embedding = query_embeddings.mean(axis=0)\n",
    "    topic_embeddings[topic_id] = topic_embedding\n",
    "    \n",
    "    print(f\"  Embedding shape: {topic_embedding.shape}\")\n",
    "    print(f\"  L2 norm: {np.linalg.norm(topic_embedding):.3f}\\n\")\n",
    "\n",
    "print(f\"✓ Created embeddings for {len(topic_embeddings)} topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise similarities between topics\n",
    "print(\"Topic-to-topic similarities:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "topic_ids = list(topic_embeddings.keys())\n",
    "topic_matrix = np.stack([topic_embeddings[tid] for tid in topic_ids])\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(topic_matrix)\n",
    "\n",
    "# Display as DataFrame\n",
    "topic_labels = [topic_definitions[tid]['label'] for tid in topic_ids]\n",
    "sim_df = pd.DataFrame(similarity_matrix, index=topic_labels, columns=topic_labels)\n",
    "\n",
    "print(sim_df.round(3))\n",
    "print(\"\\nNote: Topics should have relatively low similarity to each other\")\n",
    "print(\"      (< 0.5 is good, confirms they are distinct topics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ae75b",
   "metadata": {},
   "source": [
    "## 5. Sample-Based Topic Assignment\n",
    "\n",
    "For efficiency, we'll:\n",
    "1. Sample 50,000 threads randomly\n",
    "2. Compute similarity scores\n",
    "3. Analyze distributions to determine optimal threshold\n",
    "4. Estimate full corpus statistics based on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample threads for exploration\n",
    "SAMPLE_SIZE = 50_000\n",
    "\n",
    "print(f\"Sampling {SAMPLE_SIZE:,} threads for analysis...\")\n",
    "\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df_sample = df.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
    "    print(f\"  Sampled {len(df_sample):,} threads ({len(df_sample)/len(df)*100:.1f}% of corpus)\")\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "    print(f\"  Using full corpus ({len(df_sample):,} threads)\")\n",
    "\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(f\"  Total comments in sample: {df_sample['n_comments'].sum():,}\")\n",
    "print(f\"  Mean comments/thread: {df_sample['n_comments'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45603ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed sample pseudo-documents\n",
    "print(\"Embedding sample pseudo-documents...\")\n",
    "print(\"This will take a few minutes...\\n\")\n",
    "\n",
    "# Use title + first 500 chars of pseudodoc for efficiency\n",
    "# (Full pseudodocs can be very long due to many comments)\n",
    "texts_to_embed = (\n",
    "    df_sample['title'] + ' ' + \n",
    "    df_sample['pseudodoc_text'].str[:500]\n",
    ").tolist()\n",
    "\n",
    "# Embed with progress bar\n",
    "doc_embeddings = model.encode(\n",
    "    texts_to_embed,\n",
    "    batch_size=256,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Created embeddings for {len(doc_embeddings):,} documents\")\n",
    "print(f\"  Embedding shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity scores for each topic\n",
    "print(\"Computing similarity scores for each topic...\\n\")\n",
    "\n",
    "for topic_id, topic_embedding in topic_embeddings.items():\n",
    "    topic_label = topic_definitions[topic_id]['label']\n",
    "    \n",
    "    # Compute cosine similarity between all documents and this topic\n",
    "    similarities = cosine_similarity(doc_embeddings, topic_embedding.reshape(1, -1)).flatten()\n",
    "    \n",
    "    # Add to dataframe\n",
    "    df_sample[f'sim_{topic_id}'] = similarities\n",
    "    \n",
    "    print(f\"{topic_label}:\")\n",
    "    print(f\"  Mean similarity: {similarities.mean():.3f}\")\n",
    "    print(f\"  Median similarity: {np.median(similarities):.3f}\")\n",
    "    print(f\"  Max similarity: {similarities.max():.3f}\")\n",
    "    print(f\"  > 0.5: {(similarities > 0.5).sum():,} ({(similarities > 0.5).sum()/len(similarities)*100:.1f}%)\")\n",
    "    print(f\"  > 0.6: {(similarities > 0.6).sum():,} ({(similarities > 0.6).sum()/len(similarities)*100:.1f}%)\")\n",
    "    print(f\"  > 0.7: {(similarities > 0.7).sum():,} ({(similarities > 0.7).sum()/len(similarities)*100:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40023b39",
   "metadata": {},
   "source": [
    "## 6. Visualize Similarity Distributions\n",
    "\n",
    "Analyze the distribution of similarity scores to determine optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69bdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity distributions\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (topic_id, topic_info) in enumerate(topic_definitions.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    similarities = df_sample[f'sim_{topic_id}']\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(similarities, bins=50, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Add threshold lines\n",
    "    ax.axvline(0.5, color='orange', linestyle='--', label='0.5 threshold', linewidth=2)\n",
    "    ax.axvline(0.6, color='red', linestyle='--', label='0.6 threshold', linewidth=2)\n",
    "    ax.axvline(0.7, color='darkred', linestyle='--', label='0.7 threshold', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f\"{topic_info['label']} - Similarity Distribution\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_ylabel('Number of Threads')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'topic_similarity_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved plot: topic_similarity_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table: threads above various thresholds\n",
    "thresholds = [0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "print(\"Threads matching each topic at different thresholds:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for topic_id, topic_info in topic_definitions.items():\n",
    "    similarities = df_sample[f'sim_{topic_id}']\n",
    "    \n",
    "    row = {'Topic': topic_info['label']}\n",
    "    for thresh in thresholds:\n",
    "        count = (similarities > thresh).sum()\n",
    "        pct = count / len(similarities) * 100\n",
    "        row[f'>= {thresh}'] = f\"{count:,} ({pct:.1f}%)\"\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nNote: These are sample estimates. Full corpus will have ~2x these counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21090c21",
   "metadata": {},
   "source": [
    "## 7. Assign Best Topic to Each Thread\n",
    "\n",
    "For each thread, assign it to the topic with the highest similarity score (if above threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best matching topic for each thread\n",
    "THRESHOLD = 0.6  # Starting threshold based on literature\n",
    "\n",
    "print(f\"Assigning threads to topics (threshold = {THRESHOLD})...\\n\")\n",
    "\n",
    "# Get all similarity columns\n",
    "sim_cols = [f'sim_{topic_id}' for topic_id in topic_definitions.keys()]\n",
    "\n",
    "# Find max similarity and corresponding topic\n",
    "df_sample['max_similarity'] = df_sample[sim_cols].max(axis=1)\n",
    "df_sample['best_topic_idx'] = df_sample[sim_cols].idxmax(axis=1)\n",
    "\n",
    "# Map back to topic ID\n",
    "topic_id_list = list(topic_definitions.keys())\n",
    "df_sample['best_topic'] = df_sample['best_topic_idx'].apply(\n",
    "    lambda x: topic_id_list[sim_cols.index(x)] if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "# Apply threshold filter\n",
    "df_sample['assigned_topic'] = df_sample.apply(\n",
    "    lambda row: row['best_topic'] if row['max_similarity'] >= THRESHOLD else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Statistics\n",
    "print(f\"Assignment results:\")\n",
    "print(f\"  Threads with topic (>= {THRESHOLD}): {df_sample['assigned_topic'].notna().sum():,} ({df_sample['assigned_topic'].notna().sum()/len(df_sample)*100:.1f}%)\")\n",
    "print(f\"  Threads without topic (< {THRESHOLD}): {df_sample['assigned_topic'].isna().sum():,} ({df_sample['assigned_topic'].isna().sum()/len(df_sample)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTopic distribution:\")\n",
    "topic_counts = df_sample['assigned_topic'].value_counts()\n",
    "for topic_id, count in topic_counts.items():\n",
    "    topic_label = topic_definitions[topic_id]['label']\n",
    "    pct = count / df_sample['assigned_topic'].notna().sum() * 100\n",
    "    print(f\"  {topic_label}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic assignments\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart: assigned vs unassigned\n",
    "assigned = df_sample['assigned_topic'].notna().sum()\n",
    "unassigned = df_sample['assigned_topic'].isna().sum()\n",
    "\n",
    "ax1.pie(\n",
    "    [assigned, unassigned],\n",
    "    labels=[f'Assigned\\n({assigned:,})', f'Unassigned\\n({unassigned:,})'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['#2ecc71', '#e74c3c'],\n",
    "    startangle=90\n",
    ")\n",
    "ax1.set_title(f'Topic Assignment Coverage (threshold = {THRESHOLD})', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Bar chart: distribution across topics\n",
    "topic_counts = df_sample['assigned_topic'].value_counts()\n",
    "topic_labels_ordered = [topic_definitions[tid]['label'] for tid in topic_counts.index]\n",
    "\n",
    "ax2.barh(topic_labels_ordered, topic_counts.values, color='steelblue')\n",
    "ax2.set_xlabel('Number of Threads')\n",
    "ax2.set_title('Threads per Topic', fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add counts on bars\n",
    "for i, (label, count) in enumerate(zip(topic_labels_ordered, topic_counts.values)):\n",
    "    ax2.text(count + 100, i, f'{count:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'topic_assignment_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved plot: topic_assignment_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb709142",
   "metadata": {},
   "source": [
    "## 8. Sample Review: Manual Validation\n",
    "\n",
    "Review random samples from each topic to validate assignment quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 threads from each topic for manual review\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE REVIEW - Manual Validation\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nReview these samples to assess assignment quality.\\n\")\n",
    "\n",
    "for topic_id, topic_info in topic_definitions.items():\n",
    "    topic_threads = df_sample[df_sample['assigned_topic'] == topic_id]\n",
    "    \n",
    "    if len(topic_threads) == 0:\n",
    "        print(f\"\\n{topic_info['label']}: No threads assigned\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{topic_info['label'].upper()} - Sample threads\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    samples = topic_threads.nlargest(5, f'sim_{topic_id}')\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        print(f\"\\n[{idx}] Similarity: {row[f'sim_{topic_id}']:.3f}\")\n",
    "        print(f\"    Subreddit: r/{row['subreddit']}\")\n",
    "        print(f\"    Comments: {row['n_comments']}\")\n",
    "        print(f\"    Title: {row['title']}\")\n",
    "        if len(row['selftext']) > 0:\n",
    "            print(f\"    Body: {row['selftext'][:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac6237",
   "metadata": {},
   "source": [
    "## 9. Estimate Full Corpus Statistics\n",
    "\n",
    "Extrapolate sample results to estimate full corpus topic coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate to full corpus\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL CORPUS ESTIMATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_threads = len(df)\n",
    "sample_size = len(df_sample)\n",
    "scaling_factor = total_threads / sample_size\n",
    "\n",
    "print(f\"\\nSample size: {sample_size:,}\")\n",
    "print(f\"Full corpus: {total_threads:,}\")\n",
    "print(f\"Scaling factor: {scaling_factor:.2f}x\\n\")\n",
    "\n",
    "# Estimate topic coverage\n",
    "assigned_sample = df_sample['assigned_topic'].notna().sum()\n",
    "assigned_estimate = int(assigned_sample * scaling_factor)\n",
    "\n",
    "print(f\"Estimated threads with topics (>= {THRESHOLD}): {assigned_estimate:,} ({assigned_estimate/total_threads*100:.1f}%)\")\n",
    "print(f\"Estimated threads without topics: {total_threads - assigned_estimate:,} ({(total_threads - assigned_estimate)/total_threads*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nEstimated topic distribution:\")\n",
    "for topic_id, topic_info in topic_definitions.items():\n",
    "    sample_count = (df_sample['assigned_topic'] == topic_id).sum()\n",
    "    estimated_count = int(sample_count * scaling_factor)\n",
    "    \n",
    "    # Estimate total comments for this topic\n",
    "    topic_threads = df_sample[df_sample['assigned_topic'] == topic_id]\n",
    "    if len(topic_threads) > 0:\n",
    "        avg_comments = topic_threads['n_comments'].mean()\n",
    "        estimated_comments = int(estimated_count * avg_comments)\n",
    "    else:\n",
    "        estimated_comments = 0\n",
    "    \n",
    "    print(f\"  {topic_info['label']}:\")\n",
    "    print(f\"    Threads: {estimated_count:,}\")\n",
    "    print(f\"    Comments: {estimated_comments:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357edf7e",
   "metadata": {},
   "source": [
    "## 10. Threshold Sensitivity Analysis\n",
    "\n",
    "Test different thresholds to understand trade-offs between coverage and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple thresholds\n",
    "test_thresholds = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "print(\"Threshold sensitivity analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for thresh in test_thresholds:\n",
    "    # Apply threshold\n",
    "    assigned = (df_sample['max_similarity'] >= thresh).sum()\n",
    "    pct = assigned / len(df_sample) * 100\n",
    "    \n",
    "    # Estimate for full corpus\n",
    "    estimated_full = int(assigned * scaling_factor)\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold': thresh,\n",
    "        'Sample Assigned': f\"{assigned:,}\",\n",
    "        'Sample %': f\"{pct:.1f}%\",\n",
    "        'Estimated Full Corpus': f\"{estimated_full:,}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nRecommendation: Start with threshold = 0.6\")\n",
    "print(\"  - Balances precision (relevant threads) with coverage\")\n",
    "print(\"  - Based on Thakur et al. (2021) BEIR benchmark\")\n",
    "print(\"  - Can be adjusted after manual validation in next notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe69792",
   "metadata": {},
   "source": [
    "## 11. Save Topic Definitions and Embeddings\n",
    "\n",
    "Save topic metadata and embeddings for use in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topic definitions\n",
    "definitions_file = output_path / 'topic_definitions.json'\n",
    "\n",
    "with open(definitions_file, 'w') as f:\n",
    "    json.dump(topic_definitions, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved topic definitions: {definitions_file}\")\n",
    "\n",
    "# Save topic embeddings\n",
    "embeddings_file = output_path / 'topic_embeddings.npy'\n",
    "topic_ids_file = output_path / 'topic_ids.json'\n",
    "\n",
    "# Stack embeddings in same order as topic_ids\n",
    "topic_ids_list = list(topic_embeddings.keys())\n",
    "embeddings_matrix = np.stack([topic_embeddings[tid] for tid in topic_ids_list])\n",
    "\n",
    "np.save(embeddings_file, embeddings_matrix)\n",
    "with open(topic_ids_file, 'w') as f:\n",
    "    json.dump(topic_ids_list, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved topic embeddings: {embeddings_file}\")\n",
    "print(f\"  Shape: {embeddings_matrix.shape}\")\n",
    "print(f\"✓ Saved topic IDs: {topic_ids_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a88d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample results for reference\n",
    "sample_results_file = output_path / 'topic_assignment_sample.parquet'\n",
    "\n",
    "# Select relevant columns\n",
    "cols_to_save = [\n",
    "    'submission_id', 'title', 'subreddit', 'n_comments', 'created_utc',\n",
    "    'assigned_topic', 'max_similarity'\n",
    "] + sim_cols\n",
    "\n",
    "df_sample[cols_to_save].to_parquet(sample_results_file, index=False)\n",
    "\n",
    "print(f\"✓ Saved sample results: {sample_results_file}\")\n",
    "print(f\"  {len(df_sample):,} threads with similarity scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72282e3",
   "metadata": {},
   "source": [
    "## 12. Summary & Recommendations\n",
    "\n",
    "Generate final summary and recommendations for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary metadata\n",
    "summary = {\n",
    "    'notebook': '15a_topic_selection_exploration',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': model_name,\n",
    "    'embedding_dimension': model.get_sentence_embedding_dimension(),\n",
    "    'topics': {\n",
    "        tid: {\n",
    "            'label': info['label'],\n",
    "            'description': info['description'],\n",
    "            'n_queries': len(info['queries'])\n",
    "        }\n",
    "        for tid, info in topic_definitions.items()\n",
    "    },\n",
    "    'sample_analysis': {\n",
    "        'corpus_size': total_threads,\n",
    "        'sample_size': sample_size,\n",
    "        'recommended_threshold': THRESHOLD,\n",
    "        'sample_coverage': {\n",
    "            'assigned': int(df_sample['assigned_topic'].notna().sum()),\n",
    "            'assigned_pct': float(df_sample['assigned_topic'].notna().sum() / len(df_sample) * 100),\n",
    "            'unassigned': int(df_sample['assigned_topic'].isna().sum())\n",
    "        },\n",
    "        'estimated_full_corpus': {\n",
    "            'assigned': assigned_estimate,\n",
    "            'assigned_pct': float(assigned_estimate / total_threads * 100)\n",
    "        },\n",
    "        'topic_distribution': {\n",
    "            topic_definitions[tid]['label']: int((df_sample['assigned_topic'] == tid).sum())\n",
    "            for tid in topic_definitions.keys()\n",
    "        }\n",
    "    },\n",
    "    'outputs': {\n",
    "        'topic_definitions': str(definitions_file),\n",
    "        'topic_embeddings': str(embeddings_file),\n",
    "        'sample_results': str(sample_results_file)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_file = output_path / '15a_run_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Defined {len(topic_definitions)} political topics\")\n",
    "print(f\"✓ Analyzed {sample_size:,} thread sample\")\n",
    "print(f\"✓ Recommended threshold: {THRESHOLD}\")\n",
    "print(f\"✓ Estimated coverage: {assigned_estimate:,} threads ({assigned_estimate/total_threads*100:.1f}%)\")\n",
    "print(f\"\\n✓ Summary saved: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ab074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final recommendations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS FOR NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. TOPIC SELECTION: ✓ Validated\")\n",
    "print(\"   All 5 topics show good coverage and distinct semantic profiles\")\n",
    "\n",
    "print(\"\\n2. THRESHOLD: Start with 0.6\")\n",
    "print(\"   - Provides good balance of precision and coverage\")\n",
    "print(\"   - Can be adjusted per topic if needed\")\n",
    "print(\"   - Review sample assignments above to validate quality\")\n",
    "\n",
    "print(\"\\n3. EXPECTED DATA REDUCTION:\")\n",
    "reduction_pct = (1 - assigned_estimate/total_threads) * 100\n",
    "print(f\"   From {total_threads:,} threads → {assigned_estimate:,} threads\")\n",
    "print(f\"   Reduction: {reduction_pct:.1f}% removed (off-topic threads)\")\n",
    "\n",
    "# Estimate comments reduction\n",
    "assigned_threads = df_sample[df_sample['assigned_topic'].notna()]\n",
    "unassigned_threads = df_sample[df_sample['assigned_topic'].isna()]\n",
    "\n",
    "assigned_comments = int(assigned_threads['n_comments'].sum() * scaling_factor)\n",
    "total_comments = int(df_sample['n_comments'].sum() * scaling_factor)\n",
    "comments_reduction_pct = (1 - assigned_comments/total_comments) * 100\n",
    "\n",
    "print(f\"   From {total_comments:,} comments → {assigned_comments:,} comments\")\n",
    "print(f\"   Reduction: {comments_reduction_pct:.1f}% removed\")\n",
    "\n",
    "print(\"\\n4. NEXT NOTEBOOK: 15b_reddit_topic_assignment.ipynb\")\n",
    "print(\"   - Process full corpus with chosen threshold\")\n",
    "print(\"   - Assign topics to all threads and comments\")\n",
    "print(\"   - Filter corpus to topic-relevant content\")\n",
    "print(\"   - Save filtered dataset for stance detection\")\n",
    "\n",
    "print(\"\\n5. PROCESSING TIME ESTIMATE:\")\n",
    "if sample_size > 0:\n",
    "    # Rough estimate based on embedding time\n",
    "    texts_per_sec = 100  # Conservative estimate for all-MiniLM-L6-v2\n",
    "    full_corpus_time = total_threads / texts_per_sec / 60\n",
    "    print(f\"   Full corpus embedding: ~{full_corpus_time:.0f} minutes\")\n",
    "    print(\"   (Can be optimized with batch processing and GPU)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
