{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dce8164",
   "metadata": {},
   "source": [
    "## 1. Load Politosphere Subreddit Filter\n",
    "\n",
    "Load the list of political subreddits from politosphere dataset to filter Pushshift submissions.\n",
    "\n",
    "**Output**: Silver layer submissions in `data/01_corpus/01_silver/reddit/submissions/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9a444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission extraction started: 2025-12-19T16:11:24.346671\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.paths import get_data_path\n",
    "from thesis_pipeline.io.parquet import write_parquet\n",
    "\n",
    "print(f\"Submission extraction started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subreddit filter from: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/00_raw/reddit/subreddits.txt\n",
      "\n",
      "✓ Loaded 605 political subreddits\n",
      "Sample subreddits: ['2012elections', '2016_elections', '2016elections', '2aliberals', 'abetterworldnews', 'abortiondebate', 'accidentallycommunist', 'acteuropa', 'activemeasures', 'actualconspiracies']\n"
     ]
    }
   ],
   "source": [
    "# Load subreddit filter list\n",
    "subreddits_file = get_data_path('raw') / 'reddit' / 'subreddits.txt'\n",
    "\n",
    "print(f\"Loading subreddit filter from: {subreddits_file}\")\n",
    "\n",
    "with open(subreddits_file, 'r') as f:\n",
    "    political_subreddits = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(political_subreddits)} political subreddits\")\n",
    "print(f\"Sample subreddits: {sorted(list(political_subreddits))[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01235ad",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Check Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af449f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "  ✓ RS_2016-09.zst: 2011.1 MB\n",
      "  ✓ RS_2016-10.zst: 2173.9 MB\n",
      "\n",
      "Output directory: data/01_corpus/00_raw/reddit/submissions\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "raw_dir = workspace_root / 'data/01_corpus/00_raw/reddit/politosphere_2016-09_2016-10'\n",
    "output_dir = workspace_root / 'data/01_corpus/01_silver/reddit/submissions'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Submission archive files\n",
    "submission_files = [\n",
    "    raw_dir / 'RS_2016-09.zst',\n",
    "    raw_dir / 'RS_2016-10.zst'\n",
    "]\n",
    "\n",
    "print(\"Input files:\")\n",
    "for f in submission_files:\n",
    "    if f.exists():\n",
    "        size_mb = f.stat().st_size / (1024**2)\n",
    "        print(f\"  ✓ {f.name}: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"  ✗ {f.name}: NOT FOUND\")\n",
    "\n",
    "print(f\"\\nOutput directory: {output_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ea134",
   "metadata": {},
   "source": [
    "## 3. Inspect Submission File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0cf1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample submissions from RS_2016-09.zst:\n",
      "================================================================================\n",
      "\n",
      "Submission 1:\n",
      "  ID: 50kc60\n",
      "  Title: Server Owners/Operators, how have your servers been doing in terms of traffic si...\n",
      "  Subreddit: tf2\n",
      "  Author: Herpsties\n",
      "  Created UTC: 1472688000\n",
      "  Selftext: It's been awhile since MyM hit and a lot of people were discussing the impact the update may have on...\n",
      "\n",
      "  All fields: ['title', 'stickied', 'created_utc', 'over_18', 'selftext', 'contest_mode', 'subreddit_id', 'media_embed', 'distinguished', 'gilded', 'author_flair_text', 'thumbnail', 'media', 'url', 'secure_media', 'is_self', 'archived', 'downs', 'ups', 'permalink', 'name', 'locked', 'link_flair_css_class', 'retrieved_on', 'subreddit', 'hide_score', 'secure_media_embed', 'domain', 'num_comments', 'saved', 'author_flair_css_class', 'link_flair_text', 'id', 'score', 'author', 'edited', 'quarantine']\n",
      "\n",
      "Submission 2:\n",
      "  ID: 50kc61\n",
      "  Title: Crashed Presto site leaves customers, staff scrambling | Metro News...\n",
      "  Subreddit: ottawa\n",
      "  Author: neoCanuck\n",
      "  Created UTC: 1472688000\n",
      "  Selftext: ...\n",
      "\n",
      "Submission 3:\n",
      "  ID: 50kc62\n",
      "  Title: [META]Map of the World 1/9/13...\n",
      "  Subreddit: WastelandPowers\n",
      "  Author: Edudogel\n",
      "  Created UTC: 1472688001\n",
      "  Selftext: [MAP](http://i.imgur.com/KENCQVf.png)...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Peek at first few submissions to understand structure\n",
    "print(\"Sample submissions from RS_2016-09.zst:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with open(submission_files[0], 'rb') as fh:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(fh) as reader:\n",
    "        text_stream = reader.read(1024 * 1024).decode('utf-8')  # Read first 1MB\n",
    "        lines = text_stream.split('\\n')\n",
    "        \n",
    "        for i, line in enumerate(lines[:3]):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                sub = json.loads(line)\n",
    "                print(f\"\\nSubmission {i+1}:\")\n",
    "                print(f\"  ID: {sub.get('id', 'N/A')}\")\n",
    "                print(f\"  Title: {sub.get('title', '')[:80]}...\")\n",
    "                print(f\"  Subreddit: {sub.get('subreddit', 'N/A')}\")\n",
    "                print(f\"  Author: {sub.get('author', 'N/A')}\")\n",
    "                print(f\"  Created UTC: {sub.get('created_utc', 'N/A')}\")\n",
    "                print(f\"  Selftext: {sub.get('selftext', '')[:100]}...\")\n",
    "                \n",
    "                if i == 0:\n",
    "                    print(f\"\\n  All fields: {list(sub.keys())}\")\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c892d54",
   "metadata": {},
   "source": [
    "## 4. Extract and Filter Submissions\n",
    "\n",
    "Process the Pushshift archives and extract submissions from political subreddits for Sep-Oct 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0494d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_submission_file(file_path, political_subreddits, period_start, period_end):\n",
    "    \"\"\"\n",
    "    Extract submissions from a Pushshift .zst file.\n",
    "    Returns dict mapping date -> list of submission records.\n",
    "    \"\"\"\n",
    "    submissions_by_date = defaultdict(list)\n",
    "    found_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    print(f\"\\nProcessing: {file_path.name}\")\n",
    "    print(f\"Filtering for {len(political_subreddits)} political subreddits\")\n",
    "    print(f\"Date range: {period_start.date()} to {period_end.date()}\")\n",
    "    \n",
    "    with open(file_path, 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            text_reader = reader.read()\n",
    "            \n",
    "            # Process line by line\n",
    "            for line in tqdm(text_reader.decode('utf-8', errors='ignore').split('\\n'), \n",
    "                           desc=f\"Scanning {file_path.name}\"):\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                    \n",
    "                total_count += 1\n",
    "                \n",
    "                try:\n",
    "                    sub = json.loads(line)\n",
    "                    \n",
    "                    # Check subreddit filter\n",
    "                    subreddit = sub.get('subreddit', '').lower()\n",
    "                    if subreddit not in political_subreddits:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check date range\n",
    "                    created_utc = sub.get('created_utc', 0)\n",
    "                    created_dt = datetime.fromtimestamp(created_utc)\n",
    "                    if not (period_start <= created_dt <= period_end):\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract relevant fields\n",
    "                    record = {\n",
    "                        'thread_id': sub.get('id'),\n",
    "                        'title': sub.get('title', ''),\n",
    "                        'selftext': sub.get('selftext', ''),\n",
    "                        'author': sub.get('author', ''),\n",
    "                        'subreddit': sub.get('subreddit', ''),\n",
    "                        'subreddit_id': sub.get('subreddit_id', ''),\n",
    "                        'created_utc': created_utc,\n",
    "                        'score': sub.get('score', 0),\n",
    "                        'num_comments': sub.get('num_comments', 0),\n",
    "                        'url': sub.get('url', ''),\n",
    "                        'domain': sub.get('domain', ''),\n",
    "                    }\n",
    "                    \n",
    "                    # Group by date\n",
    "                    date = created_dt.strftime('%Y-%m-%d')\n",
    "                    submissions_by_date[date].append(record)\n",
    "                    found_count += 1\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    # Skip malformed records\n",
    "                    continue\n",
    "    \n",
    "    print(f\"\\n✓ Processed {total_count:,} total submissions\")\n",
    "    print(f\"✓ Found {found_count:,} matching submissions ({found_count/total_count*100:.2f}%)\")\n",
    "    print(f\"✓ Covering {len(submissions_by_date)} dates\")\n",
    "    \n",
    "    return submissions_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d364e7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting submissions from Pushshift archives...\n",
      "================================================================================\n",
      "\n",
      "Processing: RS_2016-09.zst\n",
      "Filtering for 605 political subreddits\n",
      "Date range: 2016-09-01 to 2016-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning RS_2016-09.zst: 100%|██████████| 7437863/7437863 [00:58<00:00, 127585.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 7,437,862 total submissions\n",
      "✓ Found 387,373 matching submissions (5.21%)\n",
      "✓ Covering 31 dates\n",
      "\n",
      "Processing: RS_2016-10.zst\n",
      "Filtering for 605 political subreddits\n",
      "Date range: 2016-09-01 to 2016-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning RS_2016-10.zst: 100%|██████████| 8286760/8286760 [01:33<00:00, 89029.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 8,286,759 total submissions\n",
      "✓ Found 536,058 matching submissions (6.47%)\n",
      "✓ Covering 31 dates\n",
      "\n",
      "================================================================================\n",
      "Total unique dates with submissions: 61\n",
      "Total submissions extracted: 923,431\n",
      "Coverage: 100.0% of 61 days\n"
     ]
    }
   ],
   "source": [
    "# Define date range for Sep-Oct 2016\n",
    "period_start = datetime(2016, 9, 1)\n",
    "period_end = datetime(2016, 10, 31, 23, 59, 59)\n",
    "\n",
    "# Process both submission files\n",
    "print(\"Extracting submissions from Pushshift archives...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_submissions_by_date = defaultdict(list)\n",
    "\n",
    "for sub_file in submission_files:\n",
    "    if not sub_file.exists():\n",
    "        print(f\"⚠️  Skipping {sub_file.name} - file not found\")\n",
    "        continue\n",
    "        \n",
    "    file_submissions = process_submission_file(sub_file, political_subreddits, period_start, period_end)\n",
    "    \n",
    "    # Merge into main dict\n",
    "    for date, subs in file_submissions.items():\n",
    "        all_submissions_by_date[date].extend(subs)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total unique dates with submissions: {len(all_submissions_by_date)}\")\n",
    "print(f\"Total submissions extracted: {sum(len(v) for v in all_submissions_by_date.values()):,}\")\n",
    "print(f\"Coverage: {100*len([k for k in all_submissions_by_date if k])/61:.1f}% of 61 days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faeaa2f",
   "metadata": {},
   "source": [
    "## 5. Save Daily Submission Files\n",
    "\n",
    "Save as parquet files, one per day, matching the comment data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84be1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving daily submission files...\n",
      "================================================================================\n",
      "✓ Wrote 11,525 rows to 2016-09-01.parquet (1.7 MB)\n",
      "  2016-09-01: 11,525 submissions -> 2016-09-01.parquet\n",
      "✓ Wrote 11,323 rows to 2016-09-02.parquet (1.6 MB)\n",
      "  2016-09-02: 11,323 submissions -> 2016-09-02.parquet\n",
      "✓ Wrote 8,746 rows to 2016-09-03.parquet (1.3 MB)\n",
      "  2016-09-03: 8,746 submissions -> 2016-09-03.parquet\n",
      "✓ Wrote 7,939 rows to 2016-09-04.parquet (1.2 MB)\n",
      "  2016-09-04: 7,939 submissions -> 2016-09-04.parquet\n",
      "✓ Wrote 9,248 rows to 2016-09-05.parquet (1.3 MB)\n",
      "  2016-09-05: 9,248 submissions -> 2016-09-05.parquet\n",
      "✓ Wrote 12,184 rows to 2016-09-06.parquet (1.7 MB)\n",
      "  2016-09-06: 12,184 submissions -> 2016-09-06.parquet\n",
      "✓ Wrote 12,363 rows to 2016-09-07.parquet (1.8 MB)\n",
      "  2016-09-07: 12,363 submissions -> 2016-09-07.parquet\n",
      "✓ Wrote 12,783 rows to 2016-09-08.parquet (1.8 MB)\n",
      "  2016-09-08: 12,783 submissions -> 2016-09-08.parquet\n",
      "✓ Wrote 12,062 rows to 2016-09-09.parquet (1.7 MB)\n",
      "  2016-09-09: 12,062 submissions -> 2016-09-09.parquet\n",
      "✓ Wrote 9,838 rows to 2016-09-10.parquet (1.4 MB)\n",
      "  2016-09-10: 9,838 submissions -> 2016-09-10.parquet\n",
      "✓ Wrote 12,395 rows to 2016-09-11.parquet (1.7 MB)\n",
      "  2016-09-11: 12,395 submissions -> 2016-09-11.parquet\n",
      "✓ Wrote 14,480 rows to 2016-09-12.parquet (2.1 MB)\n",
      "  2016-09-12: 14,480 submissions -> 2016-09-12.parquet\n",
      "✓ Wrote 15,138 rows to 2016-09-13.parquet (2.1 MB)\n",
      "  2016-09-13: 15,138 submissions -> 2016-09-13.parquet\n",
      "✓ Wrote 14,683 rows to 2016-09-14.parquet (2.1 MB)\n",
      "  2016-09-14: 14,683 submissions -> 2016-09-14.parquet\n",
      "✓ Wrote 13,390 rows to 2016-09-15.parquet (1.9 MB)\n",
      "  2016-09-15: 13,390 submissions -> 2016-09-15.parquet\n",
      "✓ Wrote 13,307 rows to 2016-09-16.parquet (1.9 MB)\n",
      "  2016-09-16: 13,307 submissions -> 2016-09-16.parquet\n",
      "✓ Wrote 11,485 rows to 2016-09-17.parquet (1.7 MB)\n",
      "  2016-09-17: 11,485 submissions -> 2016-09-17.parquet\n",
      "✓ Wrote 10,737 rows to 2016-09-18.parquet (1.6 MB)\n",
      "  2016-09-18: 10,737 submissions -> 2016-09-18.parquet\n",
      "✓ Wrote 13,885 rows to 2016-09-19.parquet (2.0 MB)\n",
      "  2016-09-19: 13,885 submissions -> 2016-09-19.parquet\n",
      "✓ Wrote 14,865 rows to 2016-09-20.parquet (2.1 MB)\n",
      "  2016-09-20: 14,865 submissions -> 2016-09-20.parquet\n",
      "✓ Wrote 14,438 rows to 2016-09-21.parquet (2.1 MB)\n",
      "  2016-09-21: 14,438 submissions -> 2016-09-21.parquet\n",
      "✓ Wrote 14,325 rows to 2016-09-22.parquet (2.0 MB)\n",
      "  2016-09-22: 14,325 submissions -> 2016-09-22.parquet\n",
      "✓ Wrote 14,230 rows to 2016-09-23.parquet (2.0 MB)\n",
      "  2016-09-23: 14,230 submissions -> 2016-09-23.parquet\n",
      "✓ Wrote 11,169 rows to 2016-09-24.parquet (1.6 MB)\n",
      "  2016-09-24: 11,169 submissions -> 2016-09-24.parquet\n",
      "✓ Wrote 10,113 rows to 2016-09-25.parquet (1.5 MB)\n",
      "  2016-09-25: 10,113 submissions -> 2016-09-25.parquet\n",
      "✓ Wrote 14,283 rows to 2016-09-26.parquet (2.1 MB)\n",
      "  2016-09-26: 14,283 submissions -> 2016-09-26.parquet\n",
      "✓ Wrote 22,046 rows to 2016-09-27.parquet (3.2 MB)\n",
      "  2016-09-27: 22,046 submissions -> 2016-09-27.parquet\n",
      "✓ Wrote 15,218 rows to 2016-09-28.parquet (2.2 MB)\n",
      "  2016-09-28: 15,218 submissions -> 2016-09-28.parquet\n",
      "✓ Wrote 14,331 rows to 2016-09-29.parquet (2.1 MB)\n",
      "  2016-09-29: 14,331 submissions -> 2016-09-29.parquet\n",
      "✓ Wrote 13,685 rows to 2016-09-30.parquet (2.0 MB)\n",
      "  2016-09-30: 13,685 submissions -> 2016-09-30.parquet\n",
      "✓ Wrote 11,051 rows to 2016-10-01.parquet (1.6 MB)\n",
      "  2016-10-01: 11,051 submissions -> 2016-10-01.parquet\n",
      "✓ Wrote 10,692 rows to 2016-10-02.parquet (1.6 MB)\n",
      "  2016-10-02: 10,692 submissions -> 2016-10-02.parquet\n",
      "✓ Wrote 13,314 rows to 2016-10-03.parquet (1.9 MB)\n",
      "  2016-10-03: 13,314 submissions -> 2016-10-03.parquet\n",
      "✓ Wrote 15,337 rows to 2016-10-04.parquet (2.2 MB)\n",
      "  2016-10-04: 15,337 submissions -> 2016-10-04.parquet\n",
      "✓ Wrote 16,941 rows to 2016-10-05.parquet (2.3 MB)\n",
      "  2016-10-05: 16,941 submissions -> 2016-10-05.parquet\n",
      "✓ Wrote 13,312 rows to 2016-10-06.parquet (1.9 MB)\n",
      "  2016-10-06: 13,312 submissions -> 2016-10-06.parquet\n",
      "✓ Wrote 13,840 rows to 2016-10-07.parquet (1.9 MB)\n",
      "  2016-10-07: 13,840 submissions -> 2016-10-07.parquet\n",
      "✓ Wrote 16,725 rows to 2016-10-08.parquet (2.6 MB)\n",
      "  2016-10-08: 16,725 submissions -> 2016-10-08.parquet\n",
      "✓ Wrote 14,659 rows to 2016-10-09.parquet (2.3 MB)\n",
      "  2016-10-09: 14,659 submissions -> 2016-10-09.parquet\n",
      "✓ Wrote 25,502 rows to 2016-10-10.parquet (3.4 MB)\n",
      "  2016-10-10: 25,502 submissions -> 2016-10-10.parquet\n",
      "✓ Wrote 18,222 rows to 2016-10-11.parquet (2.7 MB)\n",
      "  2016-10-11: 18,222 submissions -> 2016-10-11.parquet\n",
      "✓ Wrote 18,121 rows to 2016-10-12.parquet (2.7 MB)\n",
      "  2016-10-12: 18,121 submissions -> 2016-10-12.parquet\n",
      "✓ Wrote 19,291 rows to 2016-10-13.parquet (2.9 MB)\n",
      "  2016-10-13: 19,291 submissions -> 2016-10-13.parquet\n",
      "✓ Wrote 17,979 rows to 2016-10-14.parquet (2.9 MB)\n",
      "  2016-10-14: 17,979 submissions -> 2016-10-14.parquet\n",
      "✓ Wrote 14,531 rows to 2016-10-15.parquet (2.3 MB)\n",
      "  2016-10-15: 14,531 submissions -> 2016-10-15.parquet\n",
      "✓ Wrote 13,388 rows to 2016-10-16.parquet (2.1 MB)\n",
      "  2016-10-16: 13,388 submissions -> 2016-10-16.parquet\n",
      "✓ Wrote 18,262 rows to 2016-10-17.parquet (2.8 MB)\n",
      "  2016-10-17: 18,262 submissions -> 2016-10-17.parquet\n",
      "✓ Wrote 20,968 rows to 2016-10-18.parquet (3.2 MB)\n",
      "  2016-10-18: 20,968 submissions -> 2016-10-18.parquet\n",
      "✓ Wrote 20,030 rows to 2016-10-19.parquet (3.1 MB)\n",
      "  2016-10-19: 20,030 submissions -> 2016-10-19.parquet\n",
      "✓ Wrote 25,284 rows to 2016-10-20.parquet (3.6 MB)\n",
      "  2016-10-20: 25,284 submissions -> 2016-10-20.parquet\n",
      "✓ Wrote 17,094 rows to 2016-10-21.parquet (2.8 MB)\n",
      "  2016-10-21: 17,094 submissions -> 2016-10-21.parquet\n",
      "✓ Wrote 13,506 rows to 2016-10-22.parquet (2.1 MB)\n",
      "  2016-10-22: 13,506 submissions -> 2016-10-22.parquet\n",
      "✓ Wrote 12,382 rows to 2016-10-23.parquet (2.1 MB)\n",
      "  2016-10-23: 12,382 submissions -> 2016-10-23.parquet\n",
      "✓ Wrote 16,718 rows to 2016-10-24.parquet (2.6 MB)\n",
      "  2016-10-24: 16,718 submissions -> 2016-10-24.parquet\n",
      "✓ Wrote 16,983 rows to 2016-10-25.parquet (2.7 MB)\n",
      "  2016-10-25: 16,983 submissions -> 2016-10-25.parquet\n",
      "✓ Wrote 18,334 rows to 2016-10-26.parquet (2.7 MB)\n",
      "  2016-10-26: 18,334 submissions -> 2016-10-26.parquet\n",
      "✓ Wrote 17,150 rows to 2016-10-27.parquet (2.6 MB)\n",
      "  2016-10-27: 17,150 submissions -> 2016-10-27.parquet\n",
      "✓ Wrote 23,395 rows to 2016-10-28.parquet (3.3 MB)\n",
      "  2016-10-28: 23,395 submissions -> 2016-10-28.parquet\n",
      "✓ Wrote 18,739 rows to 2016-10-29.parquet (2.7 MB)\n",
      "  2016-10-29: 18,739 submissions -> 2016-10-29.parquet\n",
      "✓ Wrote 18,625 rows to 2016-10-30.parquet (2.7 MB)\n",
      "  2016-10-30: 18,625 submissions -> 2016-10-30.parquet\n",
      "✓ Wrote 26,842 rows to 2016-10-31.parquet (3.8 MB)\n",
      "  2016-10-31: 26,842 submissions -> 2016-10-31.parquet\n",
      "\n",
      "================================================================================\n",
      "✓ Saved 61 daily files\n",
      "✓ Total submissions: 923,431\n",
      "\n",
      "Output location: data/01_corpus/00_raw/reddit/submissions\n"
     ]
    }
   ],
   "source": [
    "# Save each day's submissions\n",
    "print(\"Saving daily submission files...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "saved_files = []\n",
    "total_submissions = 0\n",
    "\n",
    "for date in sorted(all_submissions_by_date.keys()):\n",
    "    subs = all_submissions_by_date[date]\n",
    "    \n",
    "    if not subs:\n",
    "        continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(subs)\n",
    "    \n",
    "    # Add date column for consistency\n",
    "    df['date'] = date\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = ['date', 'created_utc', 'thread_id', 'title', 'selftext', \n",
    "                    'author', 'subreddit', 'subreddit_id', 'score', 'num_comments',\n",
    "                    'url', 'domain']\n",
    "    df = df[column_order]\n",
    "    \n",
    "    # Save\n",
    "    output_file = output_dir / f'{date}.parquet'\n",
    "    write_parquet(df, output_file)\n",
    "    \n",
    "    saved_files.append(output_file)\n",
    "    total_submissions += len(df)\n",
    "    \n",
    "    print(f\"  {date}: {len(df):,} submissions -> {output_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ Saved {len(saved_files)} daily files\")\n",
    "print(f\"✓ Total submissions: {total_submissions:,}\")\n",
    "print(f\"\\nOutput location: {output_dir.relative_to(workspace_root)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
