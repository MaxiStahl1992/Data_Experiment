{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6357c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit extraction started: 2025-12-18T14:52:18.325052\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import bz2\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"Reddit extraction started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0c41c",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a54fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: data/00_raw/reddit/politosphere_2016-09_2016-10\n",
      "Output: data/01_silver/reddit\n",
      "Period: 2016-09-01 to 2016-10-31\n",
      "Time index field: created_utc\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "reddit_cfg = configs['reddit']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Input/output directories\n",
    "raw_reddit_dir = workspace_root / 'data/00_raw/reddit/politosphere_2016-09_2016-10'\n",
    "silver_reddit_dir = workspace_root / 'data/01_silver/reddit'\n",
    "silver_reddit_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Period\n",
    "period_start = global_cfg['validation_run']['period_start']\n",
    "period_end = global_cfg['validation_run']['period_end']\n",
    "\n",
    "print(f\"Input: {raw_reddit_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Output: {silver_reddit_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Period: {period_start} to {period_end}\")\n",
    "print(f\"Time index field: {reddit_cfg['processing']['time_index_field']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1879b2b",
   "metadata": {},
   "source": [
    "## 2. Inspect Comment File Structure\n",
    "\n",
    "First, let's examine a few comments to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfdc13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample comments from 2016-09:\n",
      "================================================================================\n",
      "\n",
      "Comment 1:\n",
      "  Author: krb7H\n",
      "  Subreddit: politics\n",
      "  Created UTC: 1472688001\n",
      "  Body preview: trump seems to be gaining supporters at an increasing rate. I remember when he had single-digit chan...\n",
      "\n",
      "  Available fields: ['author', 'body', 'body_cleaned', 'controversiality', 'created_utc', 'distinguished', 'edited', 'gilded', 'id', 'language', 'link_id', 'parent_id', 'retrieved_on', 'score', 'subreddit', 'subreddit_id']\n",
      "\n",
      "Comment 2:\n",
      "  Author: OQcjv\n",
      "  Subreddit: politics\n",
      "  Created UTC: 1472688001\n",
      "  Body preview: Hi `alictrmods`. Thank you for participating in /r/Politics. However, [your submission](https://www....\n",
      "\n",
      "Comment 3:\n",
      "  Author: mQu7y\n",
      "  Subreddit: The_Donald\n",
      "  Created UTC: 1472688002\n",
      "  Body preview: The Mistakes of the Obama......\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Look at first 3 comments from September file\n",
    "sample_file = raw_reddit_dir / 'comments_2016-09.bz2'\n",
    "\n",
    "print(\"Sample comments from 2016-09:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with bz2.open(sample_file, 'rt', encoding='utf-8') as f:\n",
    "    for i in range(3):\n",
    "        line = f.readline()\n",
    "        comment = json.loads(line)\n",
    "        \n",
    "        print(f\"\\nComment {i+1}:\")\n",
    "        print(f\"  Author: {comment.get('author', 'N/A')}\")\n",
    "        print(f\"  Subreddit: {comment.get('subreddit', 'N/A')}\")\n",
    "        print(f\"  Created UTC: {comment.get('created_utc', 'N/A')}\")\n",
    "        print(f\"  Body preview: {comment.get('body', '')[:100]}...\")\n",
    "        \n",
    "        # Show all available fields\n",
    "        if i == 0:\n",
    "            print(f\"\\n  Available fields: {list(comment.keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05202c71",
   "metadata": {},
   "source": [
    "## 3. Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6275a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing functions defined\n"
     ]
    }
   ],
   "source": [
    "def parse_comment(line: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a JSON line from the bz2 file and extract relevant fields.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with selected fields, or None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        comment = json.loads(line)\n",
    "        \n",
    "        # Extract timestamp (comment creation time ONLY)\n",
    "        created_utc = comment.get('created_utc')\n",
    "        if not created_utc:\n",
    "            return None\n",
    "        \n",
    "        # Convert to datetime for date bucketing\n",
    "        timestamp = datetime.fromtimestamp(created_utc, tz=timezone.utc)\n",
    "        date_str = timestamp.strftime('%Y-%m-%d')\n",
    "        \n",
    "        return {\n",
    "            'comment_id': comment.get('id'),\n",
    "            'author': comment.get('author'),\n",
    "            'subreddit': comment.get('subreddit'),\n",
    "            'body': comment.get('body'),\n",
    "            'created_utc': created_utc,\n",
    "            'date': date_str,\n",
    "            'score': comment.get('score'),\n",
    "            'link_id': comment.get('link_id'),  # Thread reference (for context, not indexing)\n",
    "            'parent_id': comment.get('parent_id')\n",
    "        }\n",
    "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def should_keep_comment(comment: dict, config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Apply filters based on configuration.\n",
    "    \n",
    "    Returns:\n",
    "        True if comment should be kept\n",
    "    \"\"\"\n",
    "    # Check for deleted/removed\n",
    "    if config['filters']['remove_deleted']:\n",
    "        body = comment.get('body', '')\n",
    "        if body in ['[deleted]', '[removed]']:\n",
    "            return False\n",
    "    \n",
    "    # Check minimum length\n",
    "    min_length = config['filters']['min_comment_length']\n",
    "    if len(comment.get('body', '')) < min_length:\n",
    "        return False\n",
    "    \n",
    "    # Check for bot comments (simple heuristic: author ends with 'bot')\n",
    "    if config['filters']['remove_bot_comments']:\n",
    "        author = comment.get('author', '').lower()\n",
    "        if author.endswith('bot'):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "print(\"✓ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c5461",
   "metadata": {},
   "source": [
    "## 4. Process Comment Files\n",
    "\n",
    "Read both monthly files and organize comments by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87230b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comment files...\n",
      "================================================================================\n",
      "\n",
      "Processing: comments_2016-09.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Reading comments_2016-09.bz2: 4229159it [01:10, 60318.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: comments_2016-10.bz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Reading comments_2016-10.bz2: 5566298it [01:29, 62095.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Total lines read: 9,795,457\n",
      "Successfully parsed: 9,795,457 (100.0%)\n",
      "After filtering: 8,787,694 (89.7%)\n",
      "Unique dates: 61\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect comments by date\n",
    "comments_by_date = defaultdict(list)\n",
    "\n",
    "# Process both month files\n",
    "comment_files = [\n",
    "    raw_reddit_dir / 'comments_2016-09.bz2',\n",
    "    raw_reddit_dir / 'comments_2016-10.bz2'\n",
    "]\n",
    "\n",
    "total_lines = 0\n",
    "total_parsed = 0\n",
    "total_kept = 0\n",
    "\n",
    "print(\"Processing comment files...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for comment_file in comment_files:\n",
    "    if not comment_file.exists():\n",
    "        print(f\"⚠ Skipping missing file: {comment_file.name}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing: {comment_file.name}\")\n",
    "    \n",
    "    with bz2.open(comment_file, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"  Reading {comment_file.name}\"):\n",
    "            total_lines += 1\n",
    "            \n",
    "            # Parse comment\n",
    "            comment = parse_comment(line)\n",
    "            if comment is None:\n",
    "                continue\n",
    "            total_parsed += 1\n",
    "            \n",
    "            # Apply filters\n",
    "            if not should_keep_comment(comment, reddit_cfg):\n",
    "                continue\n",
    "            total_kept += 1\n",
    "            \n",
    "            # Group by date\n",
    "            date_str = comment['date']\n",
    "            comments_by_date[date_str].append(comment)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total lines read: {total_lines:,}\")\n",
    "print(f\"Successfully parsed: {total_parsed:,} ({100*total_parsed/total_lines:.1f}%)\")\n",
    "print(f\"After filtering: {total_kept:,} ({100*total_kept/total_parsed:.1f}%)\")\n",
    "print(f\"Unique dates: {len(comments_by_date)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc0ff5",
   "metadata": {},
   "source": [
    "## 5. Deduplicate and Write Daily Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73e1e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deduplicating and writing daily Parquet files...\n",
      "================================================================================\n",
      "✓ 2016-09-01: 109,184 comments (20014.5 KB)\n",
      "✓ 2016-09-02: 98,775 comments (17807.7 KB)\n",
      "✓ 2016-09-03: 74,731 comments (13677.4 KB)\n",
      "✓ 2016-09-04: 80,990 comments (14837.5 KB)\n",
      "✓ 2016-09-05: 85,066 comments (15418.9 KB)\n",
      "✓ 2016-09-06: 110,391 comments (19796.8 KB)\n",
      "✓ 2016-09-07: 105,031 comments (19094.4 KB)\n",
      "✓ 2016-09-08: 123,756 comments (21187.7 KB)\n",
      "✓ 2016-09-09: 98,605 comments (18197.1 KB)\n",
      "✓ 2016-09-10: 87,648 comments (15302.4 KB)\n",
      "✓ 2016-09-11: 138,072 comments (21245.0 KB)\n",
      "✓ 2016-09-12: 153,446 comments (25045.1 KB)\n",
      "✓ 2016-09-13: 142,864 comments (23227.4 KB)\n",
      "✓ 2016-09-14: 137,999 comments (23159.3 KB)\n",
      "✓ 2016-09-15: 132,073 comments (22930.3 KB)\n",
      "✓ 2016-09-16: 143,633 comments (24518.1 KB)\n",
      "✓ 2016-09-17: 111,225 comments (18485.4 KB)\n",
      "✓ 2016-09-18: 110,791 comments (18184.4 KB)\n",
      "✓ 2016-09-19: 148,111 comments (24955.9 KB)\n",
      "✓ 2016-09-20: 154,930 comments (26133.1 KB)\n",
      "✓ 2016-09-21: 138,415 comments (24035.1 KB)\n",
      "✓ 2016-09-22: 144,055 comments (24741.8 KB)\n",
      "✓ 2016-09-23: 134,682 comments (22730.8 KB)\n",
      "✓ 2016-09-24: 105,681 comments (17464.3 KB)\n",
      "✓ 2016-09-25: 102,135 comments (17451.9 KB)\n",
      "✓ 2016-09-26: 141,567 comments (23843.1 KB)\n",
      "✓ 2016-09-27: 284,161 comments (36633.3 KB)\n",
      "✓ 2016-09-28: 147,935 comments (25159.4 KB)\n",
      "✓ 2016-09-29: 135,857 comments (23671.3 KB)\n",
      "✓ 2016-09-30: 130,836 comments (22270.6 KB)\n",
      "✓ 2016-10-01: 102,991 comments (17274.8 KB)\n",
      "✓ 2016-10-02: 107,017 comments (18166.6 KB)\n",
      "✓ 2016-10-03: 131,381 comments (22466.9 KB)\n",
      "✓ 2016-10-04: 142,385 comments (23419.7 KB)\n",
      "✓ 2016-10-05: 169,042 comments (24593.9 KB)\n",
      "✓ 2016-10-06: 119,611 comments (20493.7 KB)\n",
      "✓ 2016-10-07: 130,476 comments (22050.2 KB)\n",
      "✓ 2016-10-08: 169,904 comments (25675.4 KB)\n",
      "✓ 2016-10-09: 143,190 comments (22940.7 KB)\n",
      "✓ 2016-10-10: 300,025 comments (37686.6 KB)\n",
      "✓ 2016-10-11: 161,582 comments (26648.1 KB)\n",
      "✓ 2016-10-12: 168,756 comments (27445.3 KB)\n",
      "✓ 2016-10-13: 182,091 comments (29112.5 KB)\n",
      "✓ 2016-10-14: 164,325 comments (27509.1 KB)\n",
      "✓ 2016-10-15: 135,963 comments (22738.3 KB)\n",
      "✓ 2016-10-16: 138,284 comments (22961.0 KB)\n",
      "✓ 2016-10-17: 179,637 comments (28086.4 KB)\n",
      "✓ 2016-10-18: 188,674 comments (29775.3 KB)\n",
      "✓ 2016-10-19: 175,878 comments (28485.4 KB)\n",
      "✓ 2016-10-20: 282,475 comments (36469.1 KB)\n",
      "✓ 2016-10-21: 160,980 comments (25837.0 KB)\n",
      "✓ 2016-10-22: 119,790 comments (19683.7 KB)\n",
      "✓ 2016-10-23: 119,293 comments (20065.6 KB)\n",
      "✓ 2016-10-24: 146,802 comments (25227.2 KB)\n",
      "✓ 2016-10-25: 140,856 comments (24164.8 KB)\n",
      "✓ 2016-10-26: 155,386 comments (25636.0 KB)\n",
      "✓ 2016-10-27: 153,547 comments (25621.3 KB)\n",
      "✓ 2016-10-28: 196,605 comments (29440.9 KB)\n",
      "✓ 2016-10-29: 144,141 comments (22094.5 KB)\n",
      "✓ 2016-10-30: 143,028 comments (22351.3 KB)\n",
      "✓ 2016-10-31: 199,035 comments (31241.8 KB)\n",
      "\n",
      "================================================================================\n",
      "Before deduplication: 8,787,694 comments\n",
      "After deduplication: 8,785,795 comments\n",
      "Duplicates removed: 1,899\n",
      "Files written: 61\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Deduplication and write\n",
    "print(\"\\nDeduplicating and writing daily Parquet files...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_before_dedup = 0\n",
    "total_after_dedup = 0\n",
    "files_written = []\n",
    "\n",
    "for date_str in sorted(comments_by_date.keys()):\n",
    "    comments = comments_by_date[date_str]\n",
    "    total_before_dedup += len(comments)\n",
    "    \n",
    "    # Deduplicate: exact match on (author, body, created_utc)\n",
    "    seen = set()\n",
    "    unique_comments = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        key = (comment['author'], comment['body'], comment['created_utc'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique_comments.append(comment)\n",
    "    \n",
    "    total_after_dedup += len(unique_comments)\n",
    "    \n",
    "    # Write to Parquet\n",
    "    if unique_comments:\n",
    "        df = pd.DataFrame(unique_comments)\n",
    "        \n",
    "        # Write with snappy compression\n",
    "        output_file = silver_reddit_dir / f\"{date_str}.parquet\"\n",
    "        df.to_parquet(output_file, compression='snappy', index=False)\n",
    "        files_written.append(output_file)\n",
    "        \n",
    "        print(f\"✓ {date_str}: {len(unique_comments):,} comments ({output_file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Before deduplication: {total_before_dedup:,} comments\")\n",
    "print(f\"After deduplication: {total_after_dedup:,} comments\")\n",
    "print(f\"Duplicates removed: {total_before_dedup - total_after_dedup:,}\")\n",
    "print(f\"Files written: {len(files_written)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577053e",
   "metadata": {},
   "source": [
    "## 6. Verify Date Range Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e828623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date coverage:\n",
      "Expected days: 61\n",
      "Files written: 61\n",
      "Coverage: 100.0%\n",
      "\n",
      "✓ Full coverage: all dates have data\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Check coverage\n",
    "start_date = datetime.strptime(period_start, '%Y-%m-%d')\n",
    "end_date = datetime.strptime(period_end, '%Y-%m-%d')\n",
    "expected_days = (end_date - start_date).days + 1\n",
    "\n",
    "# Find missing dates\n",
    "expected_dates = set()\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    expected_dates.add(current.strftime('%Y-%m-%d'))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "written_dates = set(f.stem for f in files_written)\n",
    "missing_dates = expected_dates - written_dates\n",
    "\n",
    "print(\"Date coverage:\")\n",
    "print(f\"Expected days: {expected_days}\")\n",
    "print(f\"Files written: {len(files_written)}\")\n",
    "print(f\"Coverage: {100*len(files_written)/expected_days:.1f}%\")\n",
    "\n",
    "if missing_dates:\n",
    "    print(f\"\\n⚠ Missing {len(missing_dates)} dates (no comments found):\")\n",
    "    for date in sorted(missing_dates)[:10]:  # Show first 10\n",
    "        print(f\"  - {date}\")\n",
    "    if len(missing_dates) > 10:\n",
    "        print(f\"  ... and {len(missing_dates) - 10} more\")\n",
    "else:\n",
    "    print(\"\\n✓ Full coverage: all dates have data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf6078",
   "metadata": {},
   "source": [
    "## 7. Sample Verification\n",
    "\n",
    "Spot-check a few daily files to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f35c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample verification: 2016-10-01.parquet\n",
      "================================================================================\n",
      "Rows: 102,991\n",
      "Columns: ['comment_id', 'author', 'subreddit', 'body', 'created_utc', 'date', 'score', 'link_id', 'parent_id']\n",
      "\n",
      "Data types:\n",
      "comment_id     object\n",
      "author         object\n",
      "subreddit      object\n",
      "body           object\n",
      "created_utc     int64\n",
      "date           object\n",
      "score           int64\n",
      "link_id        object\n",
      "parent_id      object\n",
      "dtype: object\n",
      "\n",
      "First 3 comments:\n",
      "\n",
      "  1. iQHgh in r/The_Donald\n",
      "     Date: 2016-10-01\n",
      "     Body: *Someone* gets it ..\n",
      "\n",
      "...\n",
      "\n",
      "  2. 5KBRe in r/politics\n",
      "     Date: 2016-10-01\n",
      "     Body: &gt;Dollar coins finally catch on thanks to triumphant RenFaire nerds. \n",
      "\n",
      "Alright, let's try to be a ...\n",
      "\n",
      "  3. [deleted] in r/The_Donald\n",
      "     Date: 2016-10-01\n",
      "     Body: Something something socialism only works until......\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Read and verify a sample file\n",
    "if files_written:\n",
    "    sample_file = files_written[len(files_written)//2]  # Pick middle file\n",
    "    \n",
    "    print(f\"Sample verification: {sample_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_sample = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"Rows: {len(df_sample):,}\")\n",
    "    print(f\"Columns: {list(df_sample.columns)}\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df_sample.dtypes)\n",
    "    \n",
    "    print(f\"\\nFirst 3 comments:\")\n",
    "    for idx, row in df_sample.head(3).iterrows():\n",
    "        print(f\"\\n  {idx+1}. {row['author']} in r/{row['subreddit']}\")\n",
    "        print(f\"     Date: {row['date']}\")\n",
    "        print(f\"     Body: {row['body'][:100]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ No files to verify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264632c",
   "metadata": {},
   "source": [
    "## 8. Save Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e40184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processing metadata saved: data/01_silver/reddit/processing_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save processing summary\n",
    "processing_metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'period': {\n",
    "        'start': period_start,\n",
    "        'end': period_end,\n",
    "        'days': expected_days\n",
    "    },\n",
    "    'input_files': [f.name for f in comment_files if f.exists()],\n",
    "    'processing': {\n",
    "        'total_lines_read': total_lines,\n",
    "        'successfully_parsed': total_parsed,\n",
    "        'after_filtering': total_kept,\n",
    "        'before_deduplication': total_before_dedup,\n",
    "        'after_deduplication': total_after_dedup,\n",
    "        'duplicates_removed': total_before_dedup - total_after_dedup\n",
    "    },\n",
    "    'output': {\n",
    "        'directory': str(silver_reddit_dir.relative_to(workspace_root)),\n",
    "        'files_written': len(files_written),\n",
    "        'dates_covered': sorted([f.stem for f in files_written]),\n",
    "        'missing_dates': sorted(list(missing_dates)) if missing_dates else []\n",
    "    },\n",
    "    'config': {\n",
    "        'time_index_field': reddit_cfg['processing']['time_index_field'],\n",
    "        'filters': reddit_cfg['filters'],\n",
    "        'deduplication': reddit_cfg['deduplication']\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = silver_reddit_dir / 'processing_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(processing_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Processing metadata saved: {metadata_file.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4446698",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Processing Complete!**\n",
    "\n",
    "Silver layer created with:\n",
    "- Daily Parquet files (one per date)\n",
    "- Timestamped by `created_utc` (comment creation time)\n",
    "- Filtered for quality (length, deleted, bots)\n",
    "- Deduplicated by (author, body, created_utc)\n",
    "\n",
    "**Next Step:** Proceed to notebook 12 to create thread-context pseudodocuments (gold layer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
