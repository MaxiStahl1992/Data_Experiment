{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f0a9a3",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add60ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n",
      "Device available: MPS\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ML and NLP\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Local utilities\n",
    "import sys\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.paths import get_data_path\n",
    "from thesis_pipeline.io.parquet import read_parquet, write_parquet\n",
    "from thesis_pipeline.stance.model import topic_to_claim\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"Workspace: {workspace_root}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'MPS' if torch.backends.mps.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f262b61",
   "metadata": {},
   "source": [
    "## 2. Load Topic-Enhanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ff3121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading topic-enhanced datasets...\n",
      "From: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/02_topics/reddit/embeddings\n",
      "\n",
      "‚úì Comments loaded: 11,710,114 rows\n",
      "  Unique comments: 8,624,040\n",
      "  Columns: ['comment_id', 'submission_id', 'created_utc', 'text', 'topic_id', 'topic_label', 'topic_description']\n",
      "\n",
      "‚úì Submissions loaded: 549,962 rows\n",
      "  Unique submissions: 433,973\n",
      "  Columns: ['submission_id', 'created_utc', 'text', 'topic_id', 'topic_label', 'topic_description']\n",
      "\n",
      "Topic distribution (comments):\n",
      "topic_label\n",
      "Media & Free Speech              3126604\n",
      "Presidential Politics            2657794\n",
      "Elections & Voting                959110\n",
      "Social Issues                     802861\n",
      "Immigration & Borders             715903\n",
      "Civil Rights & Discrimination     545749\n",
      "Technology & Privacy              387474\n",
      "Defense & Military                361601\n",
      "Budget & Taxation                 297167\n",
      "Healthcare Policy                 284613\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Topic distribution (submissions):\n",
      "topic_label\n",
      "Media & Free Speech              157939\n",
      "Presidential Politics            102906\n",
      "Immigration & Borders             40739\n",
      "Social Issues                     36213\n",
      "Elections & Voting                30290\n",
      "Civil Rights & Discrimination     28707\n",
      "Defense & Military                22365\n",
      "Technology & Privacy              20335\n",
      "Foreign Policy & Diplomacy        13177\n",
      "Healthcare Policy                 12691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load topic-enhanced datasets\n",
    "topics_path = workspace_root / 'data' / '02_topics' / 'reddit' / 'embeddings'\n",
    "\n",
    "print(\"Loading topic-enhanced datasets...\")\n",
    "print(f\"From: {topics_path}\\n\")\n",
    "\n",
    "# Load comments\n",
    "comments_df = read_parquet(\n",
    "    topics_path / 'comments_expanded_with_topics.parquet',\n",
    ")\n",
    "print(f\"‚úì Comments loaded: {len(comments_df):,} rows\")\n",
    "print(f\"  Unique comments: {comments_df['comment_id'].nunique():,}\")\n",
    "print(f\"  Columns: {list(comments_df.columns)}\")\n",
    "\n",
    "# Load submissions\n",
    "submissions_df = read_parquet(\n",
    "    topics_path / 'submissions_expanded_with_topics.parquet',\n",
    ")\n",
    "print(f\"\\n‚úì Submissions loaded: {len(submissions_df):,} rows\")\n",
    "print(f\"  Unique submissions: {submissions_df['submission_id'].nunique():,}\")\n",
    "print(f\"  Columns: {list(submissions_df.columns)}\")\n",
    "\n",
    "# Check topic distribution\n",
    "print(f\"\\nTopic distribution (comments):\")\n",
    "print(comments_df['topic_label'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nTopic distribution (submissions):\")\n",
    "print(submissions_df['topic_label'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817ec10",
   "metadata": {},
   "source": [
    "## 3. Create Test Set for Model Comparison\n",
    "\n",
    "Sample stratified by topic and content type for fair model evaluation.\n",
    "\n",
    "### Annotation Strategy\n",
    "\n",
    "**Key insight:** You can't take a stance toward \"Infrastructure\" (a topic category), but you CAN take a stance toward \"Government should invest in infrastructure\" (a claim).\n",
    "\n",
    "**Solution:** Topics are automatically converted to claims:\n",
    "- \"Healthcare Policy\" ‚Üí \"Healthcare Policy is beneficial\"\n",
    "- \"Infrastructure\" ‚Üí \"Government should invest in infrastructure\"  \n",
    "- \"Immigration Policy\" ‚Üí \"Immigration Policy is beneficial\"\n",
    "\n",
    "**When annotating:**\n",
    "- **FAVOUR** = Text supports/agrees with the claim\n",
    "- **AGAINST** = Text opposes/disagrees with the claim\n",
    "- **NEUTRAL** = No clear position on the claim, or balanced/descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3112ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Test set created: 160 samples\n",
      "  Topics: 20\n",
      "  Samples per topic: count    20.0\n",
      "mean      8.0\n",
      "std       0.0\n",
      "min       8.0\n",
      "25%       8.0\n",
      "50%       8.0\n",
      "75%       8.0\n",
      "max       8.0\n",
      "dtype: float64\n",
      "\n",
      "================================================================================\n",
      "TOPIC ‚Üí CLAIM CONVERSION (for stance detection)\n",
      "================================================================================\n",
      "\n",
      "Models will test stance toward CLAIMS, not broad topics:\n",
      "This makes annotation clearer: you're judging if text supports/opposes the claim\n",
      "\n",
      "  Presidential Politics          ‚Üí Presidential Politics is beneficial\n",
      "  Congress & Legislation         ‚Üí Congress & Legislation is beneficial\n",
      "  Immigration & Borders          ‚Üí Immigration & Borders is beneficial\n",
      "  Foreign Policy & Diplomacy     ‚Üí Foreign Policy & Diplomacy is beneficial\n",
      "  Media & Free Speech            ‚Üí Media & Free Speech is beneficial\n",
      "\n",
      "(Models will automatically convert all topics to claims)\n",
      "\n",
      "Sample test cases:\n",
      "\n",
      "1. Topic: Presidential Politics\n",
      "   Claim: Presidential Politics is beneficial\n",
      "   Text: He's still got his base, who will be with him no matter what. Everybody else will flee away from him...\n",
      "   ‚Üí Annotate: Does text FAVOUR/AGAINST/NEUTRAL toward the claim?\n",
      "\n",
      "2. Topic: Presidential Politics\n",
      "   Claim: Presidential Politics is beneficial\n",
      "   Text: ISIS is too weak to capture anywhere with a real army. (they can't even beat PKK with less manpower ...\n",
      "   ‚Üí Annotate: Does text FAVOUR/AGAINST/NEUTRAL toward the claim?\n",
      "\n",
      "3. Topic: Presidential Politics\n",
      "   Claim: Presidential Politics is beneficial\n",
      "   Text: Which she will abandon once elected. ...\n",
      "   ‚Üí Annotate: Does text FAVOUR/AGAINST/NEUTRAL toward the claim?\n"
     ]
    }
   ],
   "source": [
    "# Create test set: 100 samples stratified by topic\n",
    "# Mix of comments and submissions\n",
    "np.random.seed(73)\n",
    "\n",
    "def create_test_set(df, n_samples_per_topic=10, text_col='text'):\n",
    "    \"\"\"Sample texts stratified by topic.\"\"\"\n",
    "    samples = []\n",
    "    topics = df['topic_label'].unique()\n",
    "    \n",
    "    for topic in topics:\n",
    "        topic_df = df[df['topic_label'] == topic]\n",
    "        if len(topic_df) >= n_samples_per_topic:\n",
    "            sample = topic_df.sample(n=n_samples_per_topic, random_state=42)\n",
    "        else:\n",
    "            sample = topic_df\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return pd.concat(samples, ignore_index=True)\n",
    "\n",
    "# Sample from comments (5 per topic)\n",
    "test_comments = create_test_set(comments_df, n_samples_per_topic=5, text_col='text')\n",
    "\n",
    "# Sample from submissions (3 per topic, less data available)\n",
    "test_submissions = create_test_set(submissions_df, n_samples_per_topic=3, text_col='text')\n",
    "\n",
    "# Combine - use 'body' for comments, 'selftext' for submissions\n",
    "test_set = pd.concat([\n",
    "    test_comments[['text', 'topic_label']],\n",
    "    test_submissions[['text', 'topic_label']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Add IDs\n",
    "test_set['test_id'] = range(len(test_set))\n",
    "\n",
    "print(f\"‚úì Test set created: {len(test_set)} samples\")\n",
    "print(f\"  Topics: {test_set['topic_label'].nunique()}\")\n",
    "print(f\"  Samples per topic: {test_set.groupby('topic_label').size().describe()}\")\n",
    "\n",
    "# Convert topics to stance-able claims\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOPIC ‚Üí CLAIM CONVERSION (for stance detection)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nModels will test stance toward CLAIMS, not broad topics:\")\n",
    "print(\"This makes annotation clearer: you're judging if text supports/opposes the claim\\n\")\n",
    "\n",
    "for topic in test_set['topic_label'].unique()[:5]:\n",
    "    claim = topic_to_claim(topic)\n",
    "    print(f\"  {topic:30s} ‚Üí {claim}\")\n",
    "\n",
    "print(\"\\n(Models will automatically convert all topics to claims)\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample test cases:\")\n",
    "for i in range(3):\n",
    "    row = test_set.iloc[i]\n",
    "    claim = topic_to_claim(row['topic_label'])\n",
    "    print(f\"\\n{i+1}. Topic: {row['topic_label']}\")\n",
    "    print(f\"   Claim: {claim}\")\n",
    "    print(f\"   Text: {row['text'][:100]}...\")\n",
    "    print(f\"   ‚Üí Annotate: Does text FAVOUR/AGAINST/NEUTRAL toward the claim?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc988fdf",
   "metadata": {},
   "source": [
    "## 4. Define Stance Models to Compare\n",
    "\n",
    "**Models to test:**\n",
    "1. `roberta-large-mnli` - Baseline NLI\n",
    "2. `microsoft/deberta-v3-base-mnli-fever-anli-ling-wanli` - Stronger multi-task NLI\n",
    "3. `cross-encoder/nli-deberta-v3-base` - Cross-encoder NLI\n",
    "4. `facebook/bart-large-mnli` - Zero-shot classification baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658457c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to compare:\n",
      "1. RoBERTa-MNLI: Baseline NLI model (stable)\n",
      "   Model: roberta-large-mnli\n",
      "2. DeBERTa-v3-Multi: Multi-task NLI (stronger)\n",
      "   Model: microsoft/deberta-v3-base-mnli-fever-anli-ling-wanli\n",
      "3. DeBERTa-v3-CrossEncoder: Cross-encoder architecture\n",
      "   Model: cross-encoder/nli-deberta-v3-base\n",
      "4. BART-MNLI: BART zero-shot baseline\n",
      "   Model: facebook/bart-large-mnli\n"
     ]
    }
   ],
   "source": [
    "# Import stance utilities\n",
    "from thesis_pipeline.stance.model import ImprovedNLIStanceModel\n",
    "from thesis_pipeline.stance.comparison import StanceModelComparison, evaluate_stance_model\n",
    "\n",
    "# Define models to compare\n",
    "MODELS_TO_COMPARE = [\n",
    "    {\n",
    "        'name': 'RoBERTa-MNLI',\n",
    "        'model_name': 'roberta-large-mnli',\n",
    "        'description': 'Baseline NLI model (stable)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'DeBERTa-v3-Multi',\n",
    "        'model_name': 'MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli',\n",
    "        'description': 'Multi-task NLI (stronger)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'DeBERTa-v3-CrossEncoder',\n",
    "        'model_name': 'cross-encoder/nli-deberta-v3-base',\n",
    "        'description': 'Cross-encoder architecture'\n",
    "    },\n",
    "    {\n",
    "        'name': 'BART-MNLI',\n",
    "        'model_name': 'facebook/bart-large-mnli',\n",
    "        'description': 'BART zero-shot baseline'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Models to compare:\")\n",
    "for i, model in enumerate(MODELS_TO_COMPARE, 1):\n",
    "    print(f\"{i}. {model['name']}: {model['description']}\")\n",
    "    print(f\"   Model: {model['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa1f8d",
   "metadata": {},
   "source": [
    "## 5. Manual Annotation Interface\n",
    "\n",
    "Annotate the test set to get ground truth labels.\n",
    "\n",
    "### Quick Annotation Guide\n",
    "\n",
    "**What you're judging:** Does the text support/oppose the **CLAIM** (not just the topic)?\n",
    "\n",
    "| Stance | Definition | Example (Claim: \"Healthcare Policy is beneficial\") |\n",
    "|--------|------------|---------------------------------------------------|\n",
    "| **FAVOUR** | Text supports/agrees with claim | \"ACA has improved access to healthcare\" |\n",
    "| **AGAINST** | Text opposes/disagrees with claim | \"ACA is terrible, increased costs\" |\n",
    "| **NEUTRAL** | No clear position, balanced, or descriptive | \"Healthcare policy affects many people\" |\n",
    "\n",
    "**Tips:**\n",
    "- Focus on the **claim** shown in the interactive widget\n",
    "- If text doesn't mention the topic at all ‚Üí likely NEUTRAL\n",
    "- If text describes topic without taking position ‚Üí NEUTRAL\n",
    "- Only FAVOUR/AGAINST if text clearly supports/opposes the claim\n",
    "\n",
    "**Target:** 50-100 annotated samples for reliable model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad4042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Annotations found, loading...\n",
      "  Annotated: 66 / 160\n",
      "\n",
      "Annotation progress: 66/160 (41.2%)\n",
      "\n",
      "Ground truth distribution:\n",
      "ground_truth_stance\n",
      "NEUTRAL    38\n",
      "FAVOUR     16\n",
      "AGAINST    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "This distribution looks reasonable for political text:\n",
      "  - Some clear positions (FAVOUR/AGAINST)\n",
      "  - Many neutral/descriptive statements\n",
      "  - Balanced across stances indicates good annotation quality\n"
     ]
    }
   ],
   "source": [
    "# Check if annotations already exist\n",
    "annotations_path = workspace_root / 'data' / '03_stance' / 'reddit' / 'test_set_annotations.parquet'\n",
    "annotations_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if annotations_path.exists():\n",
    "    print(\"‚úì Annotations found, loading...\")\n",
    "    test_set = read_parquet(annotations_path)\n",
    "    print(f\"  Annotated: {test_set['ground_truth_stance'].notna().sum()} / {len(test_set)}\")\n",
    "else:\n",
    "    print(\"‚ö† No annotations found. Please annotate the test set.\")\n",
    "    print(\"\\nAnnotation instructions:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"IMPORTANT: You're annotating stance toward CLAIMS, not topics!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nFor each text, determine if it:\")\n",
    "    print(\"  - FAVOUR: Supports/agrees with the claim (positive toward it)\")\n",
    "    print(\"  - AGAINST: Opposes/disagrees with the claim (negative toward it)\")\n",
    "    print(\"  - NEUTRAL: No clear stance, balanced, or just descriptive\")\n",
    "    print(\"\\nExamples:\")\n",
    "    print(\"  Claim: 'Government should invest in infrastructure'\")\n",
    "    print(\"  Text: 'We need better roads and bridges' ‚Üí FAVOUR\")\n",
    "    print(\"  Text: 'Infrastructure spending wastes money' ‚Üí AGAINST\")\n",
    "    print(\"  Text: 'Infrastructure is an important topic' ‚Üí NEUTRAL\")\n",
    "    print(\"\\n2. Add 'ground_truth_stance' column to test_set\")\n",
    "    print(\"3. Save to:\", annotations_path)\n",
    "    \n",
    "# Show annotation progress\n",
    "if 'ground_truth_stance' in test_set.columns:\n",
    "    annotated = test_set['ground_truth_stance'].notna().sum()\n",
    "    print(f\"\\nAnnotation progress: {annotated}/{len(test_set)} ({annotated/len(test_set):.1%})\")\n",
    "    if annotated > 0:\n",
    "        print(\"\\nGround truth distribution:\")\n",
    "        print(test_set['ground_truth_stance'].value_counts())\n",
    "        print(\"\\nThis distribution looks reasonable for political text:\")\n",
    "        print(\"  - Some clear positions (FAVOUR/AGAINST)\")\n",
    "        print(\"  - Many neutral/descriptive statements\")\n",
    "        print(\"  - Balanced across stances indicates good annotation quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22e620",
   "metadata": {},
   "source": [
    "### Manual Annotation Helper\n",
    "\n",
    "Run this cell to annotate samples interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134f3e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1da17d4bec460e86a244d22e2b9067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33afbcf348d2489696457bf029562e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='‚úì FAVOUR', layout=Layout(width='120px'), style=Butt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b62d09ab49a472191400b0cf6819cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='‚Üê Previous', layout=Layout(width='120px'), style=ButtonStyle()), Button(but‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0503771da6493396be9493693ec08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Progress: 55/160')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive annotation (optional - can also annotate in CSV/Excel)\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from thesis_pipeline.stance.model import topic_to_claim\n",
    "\n",
    "def annotate_interactive(test_set_df, start_idx=0):\n",
    "    \"\"\"Interactive annotation widget.\"\"\"\n",
    "    if 'ground_truth_stance' not in test_set_df.columns:\n",
    "        test_set_df['ground_truth_stance'] = None\n",
    "    \n",
    "    current_idx = start_idx\n",
    "    out = widgets.Output()\n",
    "    \n",
    "    def show_sample(idx):\n",
    "        row = test_set_df.iloc[idx]\n",
    "        claim = topic_to_claim(row['topic_label'])\n",
    "        out.clear_output()\n",
    "\n",
    "        with out:\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Sample {idx + 1} / {len(test_set_df)}\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"\\nTopic: {row['topic_label']}\")\n",
    "            print(f\"Claim: {claim}\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"\\nText:\\n{row['text']}\")\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"\\n‚ùì Does this text FAVOUR, OPPOSE (AGAINST), or is NEUTRAL toward the claim?\")\n",
    "            \n",
    "            if row['ground_truth_stance'] is not None:\n",
    "                print(f\"\\n‚úì Current annotation: {row['ground_truth_stance']}\")\n",
    "            else:\n",
    "                print(\"\\n‚ö† Not yet annotated\")\n",
    "    \n",
    "    def on_button_click(stance):\n",
    "        nonlocal current_idx\n",
    "        test_set_df.loc[test_set_df.index[current_idx], 'ground_truth_stance'] = stance\n",
    "        current_idx = min(current_idx + 1, len(test_set_df) - 1)\n",
    "        show_sample(current_idx)\n",
    "    \n",
    "    # Create buttons\n",
    "    btn_favour = widgets.Button(description='‚úì FAVOUR', button_style='success', layout=widgets.Layout(width='120px'))\n",
    "    btn_against = widgets.Button(description='‚úó AGAINST', button_style='danger', layout=widgets.Layout(width='120px'))\n",
    "    btn_neutral = widgets.Button(description='‚óã NEUTRAL', button_style='info', layout=widgets.Layout(width='120px'))\n",
    "    btn_skip = widgets.Button(description='‚Üí Skip', button_style='warning', layout=widgets.Layout(width='120px'))\n",
    "    btn_prev = widgets.Button(description='‚Üê Previous', button_style='', layout=widgets.Layout(width='120px'))\n",
    "    btn_save = widgets.Button(description='üíæ Save', button_style='primary', layout=widgets.Layout(width='120px'))\n",
    "    \n",
    "    btn_favour.on_click(lambda b: on_button_click('FAVOUR'))\n",
    "    btn_against.on_click(lambda b: on_button_click('AGAINST'))\n",
    "    btn_neutral.on_click(lambda b: on_button_click('NEUTRAL'))\n",
    "    \n",
    "    def skip_sample(b):\n",
    "        nonlocal current_idx\n",
    "        current_idx = min(current_idx + 1, len(test_set_df) - 1)\n",
    "        show_sample(current_idx)\n",
    "    \n",
    "    def prev_sample(b):\n",
    "        nonlocal current_idx\n",
    "        current_idx = max(current_idx - 1, 0)\n",
    "        show_sample(current_idx)\n",
    "    \n",
    "    btn_skip.on_click(skip_sample)\n",
    "    btn_prev.on_click(prev_sample)\n",
    "    \n",
    "    def save_progress(b):\n",
    "        write_parquet(test_set_df, annotations_path)\n",
    "        annotated = test_set_df['ground_truth_stance'].notna().sum()\n",
    "        with out:\n",
    "            print(f\"\\n‚úì Saved {annotated}/{len(test_set_df)} annotations to:\")\n",
    "            print(f\"  {annotations_path}\")\n",
    "    \n",
    "    btn_save.on_click(save_progress)\n",
    "    \n",
    "    show_sample(current_idx)\n",
    "    display(out)\n",
    "    display(widgets.HBox([btn_favour, btn_against, btn_neutral]))\n",
    "    display(widgets.HBox([btn_prev, btn_skip, btn_save]))\n",
    "    \n",
    "    # Show progress\n",
    "    progress_label = widgets.Label(value=f\"Progress: {test_set_df['ground_truth_stance'].notna().sum()}/{len(test_set_df)}\")\n",
    "    display(progress_label)\n",
    "\n",
    "# Uncomment to start interactive annotation\n",
    "annotate_interactive(test_set, start_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43094ca9",
   "metadata": {},
   "source": [
    "## 6. Run Model Comparison Experiments\n",
    "\n",
    "Test each model on the annotated test set and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a6b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments on 66 annotated samples...\n",
      "\n",
      "================================================================================\n",
      "Testing: RoBERTa-MNLI\n",
      "Model: roberta-large-mnli\n",
      "================================================================================\n",
      "Loading NLI stance model: roberta-large-mnli\n",
      "Device: mps\n",
      "Claim formulation: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded\n",
      "Evaluating RoBERTa-MNLI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stance prediction:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úó RoBERTa-MNLI failed: 'ImprovedNLIStanceModel' object has no attribute 'use_zero_shot'\n",
      "\n",
      "================================================================================\n",
      "Testing: DeBERTa-v3-Multi\n",
      "Model: microsoft/deberta-v3-base-mnli-fever-anli-ling-wanli\n",
      "================================================================================\n",
      "Loading NLI stance model: microsoft/deberta-v3-base-mnli-fever-anli-ling-wanli\n",
      "Device: mps\n",
      "Claim formulation: True\n",
      "\n",
      "‚úó DeBERTa-v3-Multi failed: microsoft/deberta-v3-base-mnli-fever-anli-ling-wanli is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "\n",
      "================================================================================\n",
      "Testing: DeBERTa-v3-CrossEncoder\n",
      "Model: cross-encoder/nli-deberta-v3-base\n",
      "================================================================================\n",
      "Loading NLI stance model: cross-encoder/nli-deberta-v3-base\n",
      "Device: mps\n",
      "Claim formulation: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded\n",
      "Evaluating DeBERTa-v3-CrossEncoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stance prediction:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úó DeBERTa-v3-CrossEncoder failed: 'ImprovedNLIStanceModel' object has no attribute 'use_zero_shot'\n",
      "\n",
      "================================================================================\n",
      "Testing: BART-MNLI\n",
      "Model: facebook/bart-large-mnli\n",
      "================================================================================\n",
      "Loading NLI stance model: facebook/bart-large-mnli\n",
      "Device: mps\n",
      "Claim formulation: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using zero-shot classification pipeline for BART\n",
      "‚úì Model loaded\n",
      "Evaluating BART-MNLI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stance prediction:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úó BART-MNLI failed: 'ImprovedNLIStanceModel' object has no attribute 'use_zero_shot'\n",
      "\n",
      "\n",
      "================================================================================\n",
      "All model experiments complete!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have ground truth\n",
    "if 'ground_truth_stance' not in test_set.columns or test_set['ground_truth_stance'].isna().all():\n",
    "    print(\"‚ö† ERROR: No ground truth annotations found.\")\n",
    "    print(\"Please annotate the test set first (see cell above).\")\n",
    "else:\n",
    "    # Filter to annotated samples only\n",
    "    test_set_annotated = test_set[test_set['ground_truth_stance'].notna()].copy()\n",
    "    print(f\"Running experiments on {len(test_set_annotated)} annotated samples...\\n\")\n",
    "    \n",
    "    # Initialize comparison framework\n",
    "    comparison = StanceModelComparison(test_set_annotated)\n",
    "    \n",
    "    # Test each model\n",
    "    for model_config in MODELS_TO_COMPARE:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Testing: {model_config['name']}\")\n",
    "        print(f\"Model: {model_config['model_name']}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        try:\n",
    "            # Initialize model\n",
    "            model = ImprovedNLIStanceModel(\n",
    "                model_name=model_config['model_name'],\n",
    "                confidence_threshold=0.04\n",
    "            )\n",
    "            \n",
    "            # Run predictions\n",
    "            results_df = comparison.evaluate_model(\n",
    "                model=model,\n",
    "                model_name=model_config['name'],\n",
    "                batch_size=16,\n",
    "                show_progress=True\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚úì {model_config['name']} complete\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚úó {model_config['name']} failed: {str(e)}\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All model experiments complete!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58739af",
   "metadata": {},
   "source": [
    "## 7. Compare Model Performance\n",
    "\n",
    "Calculate metrics and visualize comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comparison metrics\n",
    "metrics_df = comparison.get_metrics_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - SUMMARY METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig = comparison.plot_comparison(figsize=(14, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save comparison results\n",
    "output_path = workspace_root / 'data' / '03_stance' / 'reddit'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_df.to_csv(output_path / 'model_comparison_metrics.csv', index=False)\n",
    "fig.savefig(output_path / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\n‚úì Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69c859",
   "metadata": {},
   "source": [
    "## 8. Select Best Model\n",
    "\n",
    "Choose model based on metrics (prioritize Macro-F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d49223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on Macro-F1\n",
    "best_model_row = metrics_df.loc[metrics_df['macro_f1'].idxmax()]\n",
    "best_model_name = best_model_row['model_name']\n",
    "best_model_config = next(m for m in MODELS_TO_COMPARE if m['name'] == best_model_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Selected Model: {best_model_name}\")\n",
    "print(f\"Model ID: {best_model_config['model_name']}\")\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Macro-F1: {best_model_row['macro_f1']:.3f}\")\n",
    "print(f\"  Accuracy: {best_model_row['accuracy']:.3f}\")\n",
    "print(f\"  FAVOUR F1: {best_model_row['favour_f1']:.3f}\")\n",
    "print(f\"  AGAINST F1: {best_model_row['against_f1']:.3f}\")\n",
    "print(f\"  NEUTRAL F1: {best_model_row['neutral_f1']:.3f}\")\n",
    "\n",
    "# Save selection\n",
    "model_selection = {\n",
    "    'selected_model': best_model_name,\n",
    "    'model_id': best_model_config['model_name'],\n",
    "    'selection_date': datetime.now().isoformat(),\n",
    "    'metrics': best_model_row.to_dict(),\n",
    "    'selection_criteria': 'Macro-F1 score',\n",
    "    'test_set_size': len(test_set_annotated)\n",
    "}\n",
    "\n",
    "with open(output_path / 'model_selection.json', 'w') as f:\n",
    "    json.dump(model_selection, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Model selection saved to {output_path / 'model_selection.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1610d",
   "metadata": {},
   "source": [
    "## 9. Full Dataset Stance Detection - Comments\n",
    "\n",
    "Apply selected model to all comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best model\n",
    "print(f\"Initializing {best_model_name} for full dataset processing...\\n\")\n",
    "final_model = ImprovedNLIStanceModel(\n",
    "    model_name=best_model_config['model_name'],\n",
    "    confidence_threshold=0.04\n",
    ")\n",
    "\n",
    "# Process comments in batches\n",
    "print(\"=\"*80)\n",
    "print(\"PROCESSING COMMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total rows: {len(comments_df):,}\")\n",
    "print(f\"Unique comments: {comments_df['comment_id'].nunique():,}\")\n",
    "print(f\"Topics: {comments_df['topic_label'].nunique()}\\n\")\n",
    "\n",
    "# Run stance detection\n",
    "texts = comments_df['body'].tolist()\n",
    "targets = comments_df['topic_label'].tolist()\n",
    "\n",
    "print(\"Running stance detection...\")\n",
    "stance_results = final_model.batch_predict(\n",
    "    texts=texts,\n",
    "    targets=targets,\n",
    "    batch_size=32,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Add results to dataframe\n",
    "comments_with_stance = comments_df.copy()\n",
    "comments_with_stance['stance'] = stance_results['predicted_label']\n",
    "comments_with_stance['stance_confidence'] = stance_results['confidence']\n",
    "comments_with_stance['stance_prob_favour'] = stance_results['prob_favour']\n",
    "comments_with_stance['stance_prob_against'] = stance_results['prob_against']\n",
    "comments_with_stance['stance_prob_neutral'] = stance_results['prob_neutral']\n",
    "\n",
    "print(\"\\n‚úì Comments stance detection complete!\")\n",
    "print(f\"\\nStance distribution:\")\n",
    "print(comments_with_stance['stance'].value_counts())\n",
    "print(f\"\\nConfidence stats:\")\n",
    "print(comments_with_stance['stance_confidence'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4040fe6",
   "metadata": {},
   "source": [
    "## 10. Full Dataset Stance Detection - Submissions\n",
    "\n",
    "Apply selected model to all submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a99737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process submissions\n",
    "print(\"=\"*80)\n",
    "print(\"PROCESSING SUBMISSIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total rows: {len(submissions_df):,}\")\n",
    "print(f\"Unique submissions: {submissions_df['submission_id_text'].nunique():,}\")\n",
    "print(f\"Topics: {submissions_df['topic_label'].nunique()}\\n\")\n",
    "\n",
    "# Run stance detection\n",
    "texts = submissions_df['selftext'].tolist()\n",
    "targets = submissions_df['topic_label'].tolist()\n",
    "\n",
    "print(\"Running stance detection...\")\n",
    "stance_results = final_model.batch_predict(\n",
    "    texts=texts,\n",
    "    targets=targets,\n",
    "    batch_size=32,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Add results to dataframe\n",
    "submissions_with_stance = submissions_df.copy()\n",
    "submissions_with_stance['stance'] = stance_results['predicted_label']\n",
    "submissions_with_stance['stance_confidence'] = stance_results['confidence']\n",
    "submissions_with_stance['stance_prob_favour'] = stance_results['prob_favour']\n",
    "submissions_with_stance['stance_prob_against'] = stance_results['prob_against']\n",
    "submissions_with_stance['stance_prob_neutral'] = stance_results['prob_neutral']\n",
    "\n",
    "print(\"\\n‚úì Submissions stance detection complete!\")\n",
    "print(f\"\\nStance distribution:\")\n",
    "print(submissions_with_stance['stance'].value_counts())\n",
    "print(f\"\\nConfidence stats:\")\n",
    "print(submissions_with_stance['stance_confidence'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5b5ef",
   "metadata": {},
   "source": [
    "## 11. Save Results\n",
    "\n",
    "Save stance-enhanced datasets to `data/03_stance/reddit/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5800678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_path = workspace_root / 'data' / '03_stance' / 'reddit'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving stance-enhanced datasets...\\n\")\n",
    "\n",
    "# Save comments\n",
    "comments_output = output_path / 'comments_with_stance.parquet'\n",
    "write_parquet_safe(comments_with_stance, comments_output, show_progress=True)\n",
    "print(f\"‚úì Comments saved: {comments_output}\")\n",
    "print(f\"  Rows: {len(comments_with_stance):,}\")\n",
    "print(f\"  Columns: {len(comments_with_stance.columns)}\")\n",
    "\n",
    "# Save submissions\n",
    "submissions_output = output_path / 'submissions_with_stance.parquet'\n",
    "write_parquet_safe(submissions_with_stance, submissions_output, show_progress=True)\n",
    "print(f\"\\n‚úì Submissions saved: {submissions_output}\")\n",
    "print(f\"  Rows: {len(submissions_with_stance):,}\")\n",
    "print(f\"  Columns: {len(submissions_with_stance.columns)}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'model': {\n",
    "        'name': best_model_name,\n",
    "        'model_id': best_model_config['model_name'],\n",
    "        'confidence_threshold': 0.04\n",
    "    },\n",
    "    'performance': model_selection['metrics'],\n",
    "    'data': {\n",
    "        'comments': {\n",
    "            'total_rows': len(comments_with_stance),\n",
    "            'unique_comments': int(comments_with_stance['comment_id'].nunique()),\n",
    "            'stance_distribution': comments_with_stance['stance'].value_counts().to_dict()\n",
    "        },\n",
    "        'submissions': {\n",
    "            'total_rows': len(submissions_with_stance),\n",
    "            'unique_submissions': int(submissions_with_stance['submission_id_text'].nunique()),\n",
    "            'stance_distribution': submissions_with_stance['stance'].value_counts().to_dict()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_path / 'stance_detection_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Metadata saved: {output_path / 'stance_detection_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505b5c8",
   "metadata": {},
   "source": [
    "## 12. Quality Analysis\n",
    "\n",
    "Analyze stance detection quality and create visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined analysis\n",
    "all_data = pd.concat([\n",
    "    comments_with_stance[['stance', 'stance_confidence', 'topic_label']].assign(content_type='comment'),\n",
    "    submissions_with_stance[['stance', 'stance_confidence', 'topic_label']].assign(content_type='submission')\n",
    "])\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Overall stance distribution\n",
    "stance_counts = all_data['stance'].value_counts()\n",
    "axes[0, 0].bar(stance_counts.index, stance_counts.values, color=['green', 'red', 'gray'])\n",
    "axes[0, 0].set_title('Overall Stance Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "for i, v in enumerate(stance_counts.values):\n",
    "    axes[0, 0].text(i, v, f'{v:,}\\n({v/len(all_data):.1%})', ha='center', va='bottom')\n",
    "\n",
    "# 2. Stance by content type\n",
    "stance_by_type = pd.crosstab(all_data['content_type'], all_data['stance'], normalize='index') * 100\n",
    "stance_by_type.plot(kind='bar', ax=axes[0, 1], color=['green', 'red', 'gray'])\n",
    "axes[0, 1].set_title('Stance by Content Type', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Percentage')\n",
    "axes[0, 1].set_xlabel('')\n",
    "axes[0, 1].legend(title='Stance')\n",
    "axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=0)\n",
    "\n",
    "# 3. Confidence distribution by stance\n",
    "for stance in ['FAVOUR', 'AGAINST', 'NEUTRAL']:\n",
    "    data = all_data[all_data['stance'] == stance]['stance_confidence']\n",
    "    axes[1, 0].hist(data, alpha=0.5, label=stance, bins=30)\n",
    "axes[1, 0].set_title('Confidence Distribution by Stance', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Confidence')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Top topics by stance\n",
    "topic_stance = pd.crosstab(all_data['topic_label'], all_data['stance'])\n",
    "topic_stance_pct = topic_stance.div(topic_stance.sum(axis=1), axis=0) * 100\n",
    "top_topics = topic_stance.sum(axis=1).nlargest(10).index\n",
    "topic_stance_pct.loc[top_topics].plot(kind='barh', stacked=True, ax=axes[1, 1], \n",
    "                                       color=['green', 'red', 'gray'])\n",
    "axes[1, 1].set_title('Stance Distribution - Top 10 Topics', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Percentage')\n",
    "axes[1, 1].set_ylabel('')\n",
    "axes[1, 1].legend(title='Stance', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(output_path / 'stance_quality_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Quality analysis saved: {output_path / 'stance_quality_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b8728",
   "metadata": {},
   "source": [
    "## 13. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00108d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STANCE DETECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Data Processed:\")\n",
    "print(f\"  Comments: {len(comments_with_stance):,} rows ({comments_with_stance['comment_id'].nunique():,} unique)\")\n",
    "print(f\"  Submissions: {len(submissions_with_stance):,} rows ({submissions_with_stance['submission_id_text'].nunique():,} unique)\")\n",
    "print(f\"  Total: {len(all_data):,} text-topic pairs\")\n",
    "\n",
    "print(f\"\\nü§ñ Model Used:\")\n",
    "print(f\"  Name: {best_model_name}\")\n",
    "print(f\"  Model ID: {best_model_config['model_name']}\")\n",
    "print(f\"  Macro-F1: {best_model_row['macro_f1']:.3f}\")\n",
    "print(f\"  Accuracy: {best_model_row['accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìà Overall Results:\")\n",
    "print(f\"  FAVOUR: {(all_data['stance'] == 'FAVOUR').sum():,} ({(all_data['stance'] == 'FAVOUR').mean():.1%})\")\n",
    "print(f\"  AGAINST: {(all_data['stance'] == 'AGAINST').sum():,} ({(all_data['stance'] == 'AGAINST').mean():.1%})\")\n",
    "print(f\"  NEUTRAL: {(all_data['stance'] == 'NEUTRAL').sum():,} ({(all_data['stance'] == 'NEUTRAL').mean():.1%})\")\n",
    "\n",
    "print(f\"\\nüí™ Confidence:\")\n",
    "print(f\"  Mean: {all_data['stance_confidence'].mean():.3f}\")\n",
    "print(f\"  Median: {all_data['stance_confidence'].median():.3f}\")\n",
    "print(f\"  High confidence (>0.7): {(all_data['stance_confidence'] > 0.7).mean():.1%}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Files:\")\n",
    "print(f\"  {output_path / 'comments_with_stance.parquet'}\")\n",
    "print(f\"  {output_path / 'submissions_with_stance.parquet'}\")\n",
    "print(f\"  {output_path / 'model_comparison_metrics.csv'}\")\n",
    "print(f\"  {output_path / 'model_selection.json'}\")\n",
    "print(f\"  {output_path / 'stance_detection_metadata.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì STANCE DETECTION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
