{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2712bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n",
      "Python version: 3.12.0 (v3.12.0:0fb18b02c8, Oct  2 2023, 09:45:56) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "✓ Environment configured\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "workspace_root = Path().cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "print(f\"Project root: {workspace_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"✓ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6815d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Thesis pipeline utilities\n",
    "from thesis_pipeline.io.paths import get_data_path\n",
    "from thesis_pipeline.io.parquet import read_parquet, write_parquet\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60945309",
   "metadata": {},
   "source": [
    "## 1. Load Submissions and Comments from Gold Layer\n",
    "\n",
    "Load monthly submissions and comments, then merge to create thread pseudo-documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7629ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold submissions: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit/submissions\n",
      "Gold comments: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit/comments\n",
      "Output: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/03_qa/reddit\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "gold_submissions = get_data_path('gold') / 'reddit' / 'submissions'\n",
    "gold_comments = get_data_path('gold') / 'reddit' / 'comments'\n",
    "output_path = get_data_path('qa', 'reddit', create=True)\n",
    "\n",
    "print(f\"Gold submissions: {gold_submissions}\")\n",
    "print(f\"Gold comments: {gold_comments}\")\n",
    "print(f\"Output: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b7653da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2016-09 submissions: 386,214\n",
      "Loaded 2016-10 submissions: 537,217\n",
      "\n",
      "Total submissions: 923,431\n",
      "Columns: ['submission_id', 'title', 'selftext', 'created_utc', 'subreddit_id', 'subreddit', 'num_comments']\n",
      "\n",
      "Sample:\n",
      "  submission_id                                              title selftext  \\\n",
      "0        50kc6b  Third Party Politics To combat Two Party Polit...            \n",
      "1        50kc7a  Italy told to brace itself for 'September assa...            \n",
      "\n",
      "   created_utc subreddit_id         subreddit  num_comments  \n",
      "0   1472688004     t5_2cneq          politics             1  \n",
      "1   1472688011     t5_3bwj3  abetterworldnews             0  \n"
     ]
    }
   ],
   "source": [
    "# Load submissions (monthly files)\n",
    "months = ['2016-09', '2016-10']\n",
    "\n",
    "submissions_dfs = []\n",
    "for month in months:\n",
    "    df = read_parquet(gold_submissions / f'{month}.parquet')\n",
    "    submissions_dfs.append(df)\n",
    "    print(f\"Loaded {month} submissions: {len(df):,}\")\n",
    "\n",
    "df_submissions = pd.concat(submissions_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal submissions: {len(df_submissions):,}\")\n",
    "print(f\"Columns: {df_submissions.columns.tolist()}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(df_submissions.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22dda458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2016-09 comments: 3,766,133\n",
      "Loaded 2016-10 comments: 4,932,790\n",
      "\n",
      "Total comments: 8,698,923\n",
      "Columns: ['comment_id', 'submission_id', 'created_utc', 'subreddit_id', 'subreddit', 'body']\n",
      "\n",
      "Unique submissions referenced: 501,969\n"
     ]
    }
   ],
   "source": [
    "# Load comments (monthly files)\n",
    "comments_dfs = []\n",
    "for month in months:\n",
    "    df = read_parquet(gold_comments / f'{month}.parquet')\n",
    "    comments_dfs.append(df)\n",
    "    print(f\"Loaded {month} comments: {len(df):,}\")\n",
    "\n",
    "df_comments = pd.concat(comments_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal comments: {len(df_comments):,}\")\n",
    "print(f\"Columns: {df_comments.columns.tolist()}\")\n",
    "print(f\"\\nUnique submissions referenced: {df_comments['submission_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "44a81d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering bot comments...\n",
      "  Before: 8,698,923\n",
      "  After: 8,587,661\n",
      "  Removed: 111,262 bot comments\n"
     ]
    }
   ],
   "source": [
    "# Filter out bot comments\n",
    "print(\"\\nFiltering bot comments...\")\n",
    "before_filter = len(df_comments)\n",
    "print(f\"  Before: {before_filter:,}\")\n",
    "\n",
    "bot_signature = \"*I am a bot, and this action was performed automatically.\"\n",
    "df_comments = df_comments[~df_comments['body'].str.contains(bot_signature, na=False, regex=False)].copy()\n",
    "\n",
    "after_filter = len(df_comments)\n",
    "print(f\"  After: {after_filter:,}\")\n",
    "print(f\"  Removed: {before_filter - after_filter:,} bot comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9b6cb",
   "metadata": {},
   "source": [
    "## 2. Create Thread Pseudo-Documents\n",
    "\n",
    "Combine submission text with all comments to create thread-level documents for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8afcf83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation:\n",
      "  Submissions: 923,431\n",
      "  Comments: 8,587,661\n",
      "  Submissions with missing title: 0\n",
      "  Submissions with missing selftext: 0\n",
      "  Comments with missing body: 0\n",
      "\n",
      "Date ranges:\n",
      "  Submissions: 2016-09-01 to 2016-10-31\n",
      "  Comments: 2016-09-01 to 2016-10-31\n"
     ]
    }
   ],
   "source": [
    "# Validate data\n",
    "print(\"Data validation:\")\n",
    "print(f\"  Submissions: {len(df_submissions):,}\")\n",
    "print(f\"  Comments: {len(df_comments):,}\")\n",
    "print(f\"  Submissions with missing title: {df_submissions['title'].isna().sum()}\")\n",
    "print(f\"  Submissions with missing selftext: {df_submissions['selftext'].isna().sum()}\")\n",
    "print(f\"  Comments with missing body: {df_comments['body'].isna().sum()}\")\n",
    "\n",
    "# Date ranges\n",
    "sub_dates = pd.to_datetime(df_submissions['created_utc'], unit='s')\n",
    "com_dates = pd.to_datetime(df_comments['created_utc'], unit='s')\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Submissions: {sub_dates.min().date()} to {sub_dates.max().date()}\")\n",
    "print(f\"  Comments: {com_dates.min().date()} to {com_dates.max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a424028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping comments by submission...\n",
      "Grouped into 472,803 submissions with comments\n",
      "Comments per submission: min=1, median=3, max=24157\n"
     ]
    }
   ],
   "source": [
    "# Group comments by submission\n",
    "print(\"Grouping comments by submission...\")\n",
    "\n",
    "comment_groups = df_comments.groupby('submission_id').agg({\n",
    "    'body': lambda x: ' '.join(x.fillna('').astype(str)),\n",
    "    'comment_id': 'count'\n",
    "}).rename(columns={'body': 'all_comments_text', 'comment_id': 'n_comments'}).reset_index()\n",
    "\n",
    "print(f\"Grouped into {len(comment_groups):,} submissions with comments\")\n",
    "print(f\"Comments per submission: min={comment_groups['n_comments'].min()}, \"\n",
    "      f\"median={comment_groups['n_comments'].median():.0f}, \"\n",
    "      f\"max={comment_groups['n_comments'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36fca357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating thread pseudo-documents...\n",
      "\n",
      "Created 923,431 thread pseudo-documents\n",
      "  With comments: 472,803\n",
      "  Without comments: 450,628\n"
     ]
    }
   ],
   "source": [
    "# Merge submissions with comment aggregations\n",
    "print(\"Creating thread pseudo-documents...\")\n",
    "\n",
    "# Clean submission text\n",
    "df_submissions['title'] = df_submissions['title'].fillna('').astype(str)\n",
    "df_submissions['selftext'] = df_submissions['selftext'].fillna('').astype(str)\n",
    "\n",
    "# Merge\n",
    "thread_pseudodocs = df_submissions.merge(\n",
    "    comment_groups, \n",
    "    on='submission_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values (submissions with no comments)\n",
    "thread_pseudodocs['all_comments_text'] = thread_pseudodocs['all_comments_text'].fillna('')\n",
    "thread_pseudodocs['n_comments'] = thread_pseudodocs['n_comments'].fillna(0).astype(int)\n",
    "\n",
    "# Create pseudo-document: title + selftext + all comments\n",
    "thread_pseudodocs['pseudodoc_text'] = (\n",
    "    thread_pseudodocs['title'] + ' ' + \n",
    "    thread_pseudodocs['selftext'] + ' ' + \n",
    "    thread_pseudodocs['all_comments_text']\n",
    ").str.strip()\n",
    "\n",
    "print(f\"\\nCreated {len(thread_pseudodocs):,} thread pseudo-documents\")\n",
    "print(f\"  With comments: {(thread_pseudodocs['n_comments'] > 0).sum():,}\")\n",
    "print(f\"  Without comments: {(thread_pseudodocs['n_comments'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a592847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering to submissions with >= 1 comment...\n",
      "  Before: 923,431\n",
      "  After: 472,803\n",
      "  Removed: 450,628 submissions without replies\n",
      "\n",
      "✓ Final thread pseudo-documents: 472,803\n",
      "Columns: ['submission_id', 'title', 'selftext', 'n_comments', 'pseudodoc_text', 'pseudodoc_length', 'pseudodoc_tokens_approx', 'title_length', 'selftext_length', 'created_utc', 'subreddit', 'subreddit_id']\n"
     ]
    }
   ],
   "source": [
    "# Filter to only submissions with at least one comment\n",
    "# (No discussion = cannot measure polarization)\n",
    "print(f\"\\nFiltering to submissions with >= 1 comment...\")\n",
    "print(f\"  Before: {len(thread_pseudodocs):,}\")\n",
    "\n",
    "thread_pseudodocs = thread_pseudodocs[thread_pseudodocs['n_comments'] > 0].copy()\n",
    "\n",
    "print(f\"  After: {len(thread_pseudodocs):,}\")\n",
    "print(f\"  Removed: {len(df_submissions) - len(thread_pseudodocs):,} submissions without replies\")\n",
    "\n",
    "# Compute text statistics\n",
    "thread_pseudodocs['pseudodoc_length'] = thread_pseudodocs['pseudodoc_text'].str.len()\n",
    "thread_pseudodocs['pseudodoc_tokens_approx'] = thread_pseudodocs['pseudodoc_text'].str.split().str.len()\n",
    "thread_pseudodocs['title_length'] = thread_pseudodocs['title'].str.len()\n",
    "thread_pseudodocs['selftext_length'] = thread_pseudodocs['selftext'].str.len()\n",
    "\n",
    "# Select and order columns\n",
    "final_cols = [\n",
    "    'submission_id',\n",
    "    'title',\n",
    "    'selftext',\n",
    "    'n_comments',\n",
    "    'pseudodoc_text',\n",
    "    'pseudodoc_length',\n",
    "    'pseudodoc_tokens_approx',\n",
    "    'title_length',\n",
    "    'selftext_length',\n",
    "    'created_utc',\n",
    "    'subreddit',\n",
    "    'subreddit_id'\n",
    "]\n",
    "\n",
    "thread_pseudodocs = thread_pseudodocs[final_cols]\n",
    "\n",
    "print(f\"\\n✓ Final thread pseudo-documents: {len(thread_pseudodocs):,}\")\n",
    "print(f\"Columns: {thread_pseudodocs.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21656d59",
   "metadata": {},
   "source": [
    "## 3. Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5788aebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "THREAD PSEUDO-DOCUMENT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total threads with comments: 472,803\n",
      "Total submissions: 923,431\n",
      "Total comments: 8,587,661\n",
      "\n",
      "Metric                                 Mean       Median          Min          Max\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Thread statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"THREAD PSEUDO-DOCUMENT STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal threads with comments: {len(thread_pseudodocs):,}\")\n",
    "print(f\"Total submissions: {len(df_submissions):,}\")\n",
    "print(f\"Total comments: {len(df_comments):,}\")\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Mean':>12} {'Median':>12} {'Min':>12} {'Max':>12}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f97ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments/thread                        18.2            3            1        24157\n",
      "Pseudodoc length (chars)               4036          686           14      3908716\n",
      "Pseudodoc tokens (approx)               665          100            2       661263\n",
      "\n",
      "Unique subreddits: 354\n",
      "Top 5 subreddits:\n",
      "  The_Donald: 212,355\n",
      "  politics: 61,121\n",
      "  EnoughTrumpSpam: 24,646\n",
      "  willis7737_news: 17,614\n",
      "  hillaryclinton: 12,124\n"
     ]
    }
   ],
   "source": [
    "# Comments per thread\n",
    "stats = thread_pseudodocs['n_comments'].describe()\n",
    "print(f\"{'Comments/thread':<30} {stats['mean']:>12.1f} {stats['50%']:>12.0f} {stats['min']:>12.0f} {stats['max']:>12.0f}\")\n",
    "\n",
    "# Pseudo-document length\n",
    "stats = thread_pseudodocs['pseudodoc_length'].describe()\n",
    "print(f\"{'Pseudodoc length (chars)':<30} {stats['mean']:>12.0f} {stats['50%']:>12.0f} {stats['min']:>12.0f} {stats['max']:>12.0f}\")\n",
    "\n",
    "# Tokens (approx)\n",
    "stats = thread_pseudodocs['pseudodoc_tokens_approx'].describe()\n",
    "print(f\"{'Pseudodoc tokens (approx)':<30} {stats['mean']:>12.0f} {stats['50%']:>12.0f} {stats['min']:>12.0f} {stats['max']:>12.0f}\")\n",
    "\n",
    "# Subreddit distribution\n",
    "print(f\"\\nUnique subreddits: {thread_pseudodocs['subreddit'].nunique():,}\")\n",
    "print(f\"Top 5 subreddits:\")\n",
    "for sub, count in thread_pseudodocs['subreddit'].value_counts().head(5).items():\n",
    "    print(f\"  {sub}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4063de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE PSEUDO-DOCUMENTS\n",
      "================================================================================\n",
      "\n",
      "Submission 50kc92:\n",
      "  Title: When is the speech?\n",
      "  Comments: 1\n",
      "  Subreddit: The_Donald\n",
      "  Length: 62 chars, ~14 tokens\n",
      "  Text preview: When is the speech?  I thought it was at 7 pm? like right now!...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Submission 567qm2:\n",
      "  Title: No Hurricane is stopping my support!!\n",
      "  Comments: 1\n",
      "  Subreddit: The_Donald\n",
      "  Length: 70 chars, ~11 tokens\n",
      "  Text preview: No Hurricane is stopping my support!! [deleted] #KEK BE SAFE CENTIPEDE...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Submission 5afnem:\n",
      "  Title: Trump's childhood Halloween costume\n",
      "  Comments: 1\n",
      "  Subreddit: EnoughTrumpSpam\n",
      "  Length: 601 chars, ~26 tokens\n",
      "  Text preview: Trump's childhood Halloween costume [deleted] \n",
      "\n",
      "Snapshots:\n",
      "\n",
      "1. *This Post* - [Error](https://archive.is/?run=1&amp;url=http%3A%2F%2Fwww.southpark.com.br%2Fwp-content%2Fuploads%2F2011%2F11%2Feric-cartman-hitler.jpg \"error auto-archiving; click to submit it!\"), [1](https://web.archive.org/20161031230022/http://www.southpark.com.br/wp-content/uploads/2011/11/eric-cartman-hitler.jpg), [2](http://megalodon.jp/2016-1101-0800-24/www.southpark.com.br/wp-content/uploads/2011/11/eric-cartman-hitler.jpg)\n",
      "\n",
      "*^(I am a bot.) ^\\([*Info*](/r/SnapshillBot) ^/ ^[*Contact*](/message/compose?to=\\/r\\/SnapshillBot))*...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample pseudo-documents\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PSEUDO-DOCUMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in [0, len(thread_pseudodocs)//2, -1]:\n",
    "    row = thread_pseudodocs.iloc[i]\n",
    "    print(f\"\\nSubmission {row['submission_id']}:\")\n",
    "    print(f\"  Title: {row['title'][:100]}\")\n",
    "    print(f\"  Comments: {row['n_comments']}\")\n",
    "    print(f\"  Subreddit: {row['subreddit']}\")\n",
    "    print(f\"  Length: {row['pseudodoc_length']:,} chars, ~{row['pseudodoc_tokens_approx']:,} tokens\")\n",
    "    print(f\"  Text preview: {row['pseudodoc_text'][:1500]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4785f1",
   "metadata": {},
   "source": [
    "## 4. Create Comment-Submission Mapping\n",
    "\n",
    "Map all comments to their submissions for stance detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d168a71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating comment-submission mapping...\n",
      "Valid submissions (with >= 1 comment): 472,803\n",
      "Comments from valid submissions: 8,587,661\n",
      "\n",
      "✓ Created mapping for 8,587,661 comments\n",
      "Columns: ['comment_id', 'submission_id', 'comment_body', 'comment_created_utc', 'subreddit', 'subreddit_id', 'title', 'selftext']\n"
     ]
    }
   ],
   "source": [
    "# Create comment-submission mapping for stance detection\n",
    "# Only include comments from threads that have discussion (n_comments > 0)\n",
    "\n",
    "print(\"Creating comment-submission mapping...\")\n",
    "\n",
    "valid_submissions = set(thread_pseudodocs['submission_id'].values)\n",
    "print(f\"Valid submissions (with >= 1 comment): {len(valid_submissions):,}\")\n",
    "\n",
    "# Filter comments to only those with valid submissions\n",
    "df_comments_filtered = df_comments[df_comments['submission_id'].isin(valid_submissions)].copy()\n",
    "print(f\"Comments from valid submissions: {len(df_comments_filtered):,}\")\n",
    "\n",
    "# Merge with submission info\n",
    "submission_info = thread_pseudodocs[['submission_id', 'title', 'selftext', 'subreddit', 'subreddit_id']].copy()\n",
    "\n",
    "comment_map = df_comments_filtered.merge(submission_info, on='submission_id', how='left', suffixes=('_comment', '_submission'))\n",
    "\n",
    "# Rename for clarity\n",
    "comment_map = comment_map.rename(columns={\n",
    "    'body': 'comment_body',\n",
    "    'created_utc': 'comment_created_utc',\n",
    "    'subreddit_comment': 'subreddit',  # Use comment's subreddit (should match submission's)\n",
    "    'subreddit_id_comment': 'subreddit_id'\n",
    "})\n",
    "\n",
    "# Select final columns\n",
    "final_cols = [\n",
    "    'comment_id',\n",
    "    'submission_id',\n",
    "    'comment_body',\n",
    "    'comment_created_utc',\n",
    "    'subreddit',\n",
    "    'subreddit_id',\n",
    "    'title',\n",
    "    'selftext'\n",
    "]\n",
    "\n",
    "comment_map = comment_map[final_cols]\n",
    "\n",
    "print(f\"\\n✓ Created mapping for {len(comment_map):,} comments\")\n",
    "print(f\"Columns: {comment_map.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2a700db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating thread metadata...\n",
      "✓ Created metadata for 472,803 threads\n",
      "Columns: ['submission_id', 'title', 'n_comments', 'created_utc', 'subreddit']\n"
     ]
    }
   ],
   "source": [
    "# Create thread metadata\n",
    "print(\"\\nCreating thread metadata...\")\n",
    "\n",
    "thread_metadata = thread_pseudodocs[['submission_id', 'title', 'n_comments', 'created_utc', 'subreddit']].copy()\n",
    "\n",
    "print(f\"✓ Created metadata for {len(thread_metadata):,} threads\")\n",
    "print(f\"Columns: {thread_metadata.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5c2d246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "Comment breakdown:\n",
      "  Total comments: 8,587,661\n",
      "  Submissions (comment_id == submission_id): 0\n",
      "  Replies (comment_id != submission_id): 8,587,661\n",
      "\n",
      "Sample thread verification (submission_id: 50l8nh):\n",
      "  n_comments from metadata: 3\n",
      "\n",
      "\n",
      "  Total comments in map: 3\n",
      "\n",
      "\n",
      "  Submission title: Broward prosecutors reviewing elections office posting results early\n",
      "  Submission selftext: ...\n",
      "Comments for submission_id 50l8nh:\n",
      "\n",
      "First 3 comments from this thread:\n",
      "  Comment d74z8bv: When Broward County posted election results online before the polls closed Tuesd...\n",
      "  Comment d75tpta: \"Intent gross negligence\"\n",
      "\n",
      "Further solidifying the Hillary defense. Next time i ...\n",
      "  Comment d75ttux: Make sure you are wearing a crown....\n"
     ]
    }
   ],
   "source": [
    "# Verify data quality\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check that submissions and replies are properly separated\n",
    "print(f\"\\nComment breakdown:\")\n",
    "print(f\"  Total comments: {len(comment_map):,}\")\n",
    "print(f\"  Submissions (comment_id == submission_id): {(comment_map['comment_id'] == comment_map['submission_id']).sum():,}\")\n",
    "print(f\"  Replies (comment_id != submission_id): {(comment_map['comment_id'] != comment_map['submission_id']).sum():,}\")\n",
    "\n",
    "# Sample a thread to verify structure\n",
    "sample_thread = thread_pseudodocs.iloc[1000]\n",
    "print(f\"\\nSample thread verification (submission_id: {sample_thread['submission_id']}):\")\n",
    "print(f\"  n_comments from metadata: {sample_thread['n_comments']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "# Get all comments for this thread\n",
    "thread_comments = comment_map[comment_map['submission_id'] == sample_thread['submission_id']].sort_values('comment_created_utc')\n",
    "print(f\"  Total comments in map: {len(thread_comments)}\")\n",
    "\n",
    "# Show first few comments to verify structure\n",
    "print(\"\\n\")\n",
    "print(f\"  Submission title: {sample_thread['title']}\")\n",
    "print(f\"  Submission selftext: {sample_thread['selftext'][:200]}...\")\n",
    "print(f\"Comments for submission_id {sample_thread['submission_id']}:\")\n",
    "print(f\"\\nFirst 3 comments from this thread:\")\n",
    "for idx, row in thread_comments.head(3).iterrows():\n",
    "    print(f\"  Comment {row['comment_id']}: {row['comment_body'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459ebb6",
   "metadata": {},
   "source": [
    "## 5. Save Outputs\n",
    "\n",
    "Save three key outputs:\n",
    "1. **thread_pseudodocs.parquet**: Thread-level pseudo-documents (submission + all comments) for topic modeling\n",
    "2. **thread_metadata.parquet**: Thread-level statistics (submission_id, title, n_comments, created_utc, subreddit)\n",
    "3. **comment_thread_map.parquet**: All comments with full context for stance detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dcfb5a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving thread pseudo-documents...\n",
      "✓ Wrote 472,803 rows to thread_pseudodocs.parquet (1150.6 MB)\n",
      "✓ Saved 472,803 thread pseudo-documents\n",
      "  Location: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/03_qa/reddit/thread_pseudodocs.parquet\n",
      "\n",
      "Saving thread metadata...\n",
      "✓ Wrote 472,803 rows to thread_metadata.parquet (30.3 MB)\n",
      "✓ Saved 472,803 thread metadata records\n",
      "  Location: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/03_qa/reddit/thread_metadata.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save thread pseudo-documents\n",
    "print(\"Saving thread pseudo-documents...\")\n",
    "thread_output = output_path / 'thread_pseudodocs.parquet'\n",
    "write_parquet(thread_pseudodocs, thread_output)\n",
    "print(f\"✓ Saved {len(thread_pseudodocs):,} thread pseudo-documents\")\n",
    "print(f\"  Location: {thread_output}\")\n",
    "\n",
    "# Save thread metadata\n",
    "print(\"\\nSaving thread metadata...\")\n",
    "metadata_output = output_path / 'thread_metadata.parquet'\n",
    "write_parquet(thread_metadata, metadata_output)\n",
    "print(f\"✓ Saved {len(thread_metadata):,} thread metadata records\")\n",
    "print(f\"  Location: {metadata_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9815ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving comment-thread mapping...\n",
      "✓ Wrote 8,587,661 rows to comment_thread_map.parquet (2387.7 MB)\n",
      "✓ Saved 8,587,661 comments (submissions + replies)\n",
      "  Location: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/03_qa/reddit/comment_thread_map.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save comment-thread mapping with full context\n",
    "print(\"\\nSaving comment-thread mapping...\")\n",
    "comment_output = output_path / 'comment_thread_map.parquet'\n",
    "write_parquet(comment_map, comment_output)\n",
    "print(f\"✓ Saved {len(comment_map):,} comments (submissions + replies)\")\n",
    "print(f\"  Location: {comment_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974215fc",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Generate metadata for this processing run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f617516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUN SUMMARY\n",
      "============================================================\n",
      "\n",
      "Notebook: 14_reddit_corpus_prep_topics\n",
      "  Gold submissions: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit/submissions\n",
      "  Gold comments: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit/comments\n",
      "  Months: ['2016-09', '2016-10']\n",
      "  Submissions (raw): 923,431\n",
      "  Comments (raw): 8,587,661\n",
      "\n",
      "Outputs:\n",
      "  Thread pseudo-documents: 472,803\n",
      "  Comments in mapping: 8,587,661\n",
      "\n",
      "Summary saved to: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/03_qa/reddit/run_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Create summary metadata\n",
    "summary = {\n",
    "    'notebook': '14_reddit_corpus_prep_topics',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'inputs': {\n",
    "        'gold_submissions': str(gold_submissions),\n",
    "        'gold_comments': str(gold_comments),\n",
    "        'months': months,\n",
    "        'n_submissions_raw': len(df_submissions),\n",
    "        'n_comments_raw': len(df_comments)\n",
    "    },\n",
    "    'outputs': {\n",
    "        'thread_pseudodocs': str(thread_output),\n",
    "        'comment_thread_map': str(comment_output),\n",
    "        'thread_metadata': str(metadata_output),\n",
    "        'n_threads': len(thread_pseudodocs),\n",
    "        'n_comments_in_map': len(comment_map)\n",
    "    },\n",
    "    'statistics': {\n",
    "        'comments_per_thread': {\n",
    "            'mean': float(thread_pseudodocs['n_comments'].mean()),\n",
    "            'median': float(thread_pseudodocs['n_comments'].median()),\n",
    "            'min': int(thread_pseudodocs['n_comments'].min()),\n",
    "            'max': int(thread_pseudodocs['n_comments'].max())\n",
    "        },\n",
    "        'pseudodoc_length_chars': {\n",
    "            'mean': float(thread_pseudodocs['pseudodoc_length'].mean()),\n",
    "            'median': float(thread_pseudodocs['pseudodoc_length'].median()),\n",
    "            'min': int(thread_pseudodocs['pseudodoc_length'].min()),\n",
    "            'max': int(thread_pseudodocs['pseudodoc_length'].max())\n",
    "        },\n",
    "        'pseudodoc_tokens_approx': {\n",
    "            'mean': float(thread_pseudodocs['pseudodoc_tokens_approx'].mean()),\n",
    "            'median': float(thread_pseudodocs['pseudodoc_tokens_approx'].median())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = output_path / 'run_metadata.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RUN SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNotebook: {summary['notebook']}\")\n",
    "print(f\"  Gold submissions: {summary['inputs']['gold_submissions']}\")\n",
    "print(f\"  Gold comments: {summary['inputs']['gold_comments']}\")\n",
    "print(f\"  Months: {summary['inputs']['months']}\")\n",
    "print(f\"  Submissions (raw): {summary['inputs']['n_submissions_raw']:,}\")\n",
    "print(f\"  Comments (raw): {summary['inputs']['n_comments_raw']:,}\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  Thread pseudo-documents: {summary['outputs']['n_threads']:,}\")\n",
    "print(f\"  Comments in mapping: {summary['outputs']['n_comments_in_map']:,}\")\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
