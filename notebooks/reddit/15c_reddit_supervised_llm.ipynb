{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a3e6be",
   "metadata": {},
   "source": [
    "# NOT SELECTED DUE TOLONG EXECUTION TIME -> 24h plus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8763bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"✓ Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335ff697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      " Input dir: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/03_qa/reddit\n",
      " Output dir: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/02_topics/03_gold/reddit\n",
      " Model: gemma3:4b\n",
      " Ollama URL: http://localhost:11434\n",
      " Workers: 16\n",
      " Docs per call: 16\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path('/Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment')\n",
    "INPUT_DIR = BASE_DIR / 'data' / '01_corpus' / '03_qa' / 'reddit'\n",
    "DATA_DIR = BASE_DIR / 'data' / '02_topics' / '03_gold' / 'reddit'\n",
    "OUTPUT_DIR = DATA_DIR\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ollama configuration\n",
    "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
    "MODEL_NAME = 'gemma3:4b'  # or faster variant / quantized\n",
    "\n",
    "# Processing configuration\n",
    "MAX_WORKERS = 16               # increase parallelism\n",
    "BATCH_DOCS_PER_CALL = 16       # docs per model call (tune)\n",
    "MAX_TOKENS = 4                 # only return a small JSON integer/array\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\" Input dir: {INPUT_DIR}\")\n",
    "print(f\" Output dir: {OUTPUT_DIR}\")\n",
    "print(f\" Model: {MODEL_NAME}\")\n",
    "print(f\" Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "print(f\" Workers: {MAX_WORKERS}\")\n",
    "print(f\" Docs per call: {BATCH_DOCS_PER_CALL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40109709",
   "metadata": {},
   "source": [
    "## Check Ollama Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab760360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama is running\n",
      "\n",
      "Available models: ['gemma3:4b']\n",
      "✓ Model 'gemma3:4b' is available\n"
     ]
    }
   ],
   "source": [
    "# Check Ollama\n",
    "def check_ollama():\n",
    "    try:\n",
    "        resp = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            models = resp.json().get('models', [])\n",
    "            model_names = [m['name'] for m in models]\n",
    "            print(\"✓ Ollama is running\")\n",
    "            print(f\"\\nAvailable models: {model_names}\")\n",
    "            if any(MODEL_NAME in m for m in model_names):\n",
    "                print(f\"✓ Model '{MODEL_NAME}' is available\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\n⚠️ Model '{MODEL_NAME}' not found\")\n",
    "                print(f\"To install: ollama pull {MODEL_NAME}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"❌ Ollama status code: {resp.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Ollama is not running\")\n",
    "        print(\"\\nStart Ollama and pull model:\")\n",
    "        print(\"  ollama serve\")\n",
    "        print(f\"  ollama pull {MODEL_NAME}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "ollama_ready = check_ollama()\n",
    "if not ollama_ready:\n",
    "    print(\"\\n⚠️ Please set up Ollama before continuing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92adc6",
   "metadata": {},
   "source": [
    "## Define Political Topic Taxonomy\n",
    "\n",
    "20 political topics adapted from the Comparative Agendas Project (CAP) framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602968eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Political Topic Taxonomy (20 Topics) ===\n",
      "\n",
      " 0. Elections & Voting\n",
      " 1. Presidential Politics\n",
      " 2. Congress & Legislation\n",
      " 3. Healthcare Policy\n",
      " 4. Immigration & Borders\n",
      " 5. Economy & Employment\n",
      " 6. Budget & Taxation\n",
      " 7. Education Policy\n",
      " 8. Criminal Justice\n",
      " 9. Gun Rights & Control\n",
      "10. Environment & Climate\n",
      "11. Energy Policy\n",
      "12. Foreign Policy & Diplomacy\n",
      "13. Defense & Military\n",
      "14. Trade Policy\n",
      "15. Social Issues\n",
      "16. Civil Rights & Discrimination\n",
      "17. Media & Free Speech\n",
      "18. Technology & Privacy\n",
      "19. Infrastructure\n"
     ]
    }
   ],
   "source": [
    "# Taxonomy\n",
    "POLITICAL_TOPICS = [\n",
    "    \"Elections & Voting\",\n",
    "    \"Presidential Politics\",\n",
    "    \"Congress & Legislation\",\n",
    "    \"Healthcare Policy\",\n",
    "    \"Immigration & Borders\",\n",
    "    \"Economy & Employment\",\n",
    "    \"Budget & Taxation\",\n",
    "    \"Education Policy\",\n",
    "    \"Criminal Justice\",\n",
    "    \"Gun Rights & Control\",\n",
    "    \"Environment & Climate\",\n",
    "    \"Energy Policy\",\n",
    "    \"Foreign Policy & Diplomacy\",\n",
    "    \"Defense & Military\",\n",
    "    \"Trade Policy\",\n",
    "    \"Social Issues\",\n",
    "    \"Civil Rights & Discrimination\",\n",
    "    \"Media & Free Speech\",\n",
    "    \"Technology & Privacy\",\n",
    "    \"Infrastructure\",\n",
    "]\n",
    "\n",
    "topic_to_id = {t: i for i, t in enumerate(POLITICAL_TOPICS)}\n",
    "id_to_topic = {i: t for i, t in enumerate(POLITICAL_TOPICS)}\n",
    "\n",
    "print(\"\\n=== Political Topic Taxonomy (20 Topics) ===\\n\")\n",
    "for i, topic in enumerate(POLITICAL_TOPICS):\n",
    "    print(f\"{i:2d}. {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6b01f",
   "metadata": {},
   "source": [
    "## Create Classification Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb0606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched prompt helper\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a political discourse classifier. \"\n",
    "    \"Given multiple short texts, classify each text into exactly ONE of these 20 topics, \"\n",
    "    \"and return ONLY a JSON array of 20 integer topic IDs in the same order.\\n\\n\"\n",
    "    \"Topics (ID: label):\\n\" +\n",
    "    \"\\n\".join([f\"{i}: {label}\" for i, label in enumerate(POLITICAL_TOPICS)]) +\n",
    "    \"\\n\\n\"\n",
    "    \"Output format example for 3 texts: [0, 5, 12]\\n\"\n",
    "    \"No explanation, no extra text.\"\n",
    ")\n",
    "\n",
    "def build_batched_prompt(texts):\n",
    "    lines = []\n",
    "    for idx, t in enumerate(texts):\n",
    "        # keep it short & robust\n",
    "        t_clean = str(t).replace(\"\\n\", \" \").strip()\n",
    "        lines.append(f\"{idx}: {t_clean}\")\n",
    "    joined = \"\\n\".join(lines)\n",
    "    user_prompt = (\n",
    "        \"Classify each of the following texts into one topic ID.\\n\\n\"\n",
    "        f\"{joined}\\n\\n\"\n",
    "        \"Return ONLY a JSON array of integers, e.g. [0, 5, 12].\"\n",
    "    )\n",
    "    return SYSTEM_PROMPT, user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3b49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-level Ollama call for a batch\n",
    "\n",
    "def classify_batch_with_ollama(texts):\n",
    "    \"\"\"\n",
    "    texts: list[str] of length <= BATCH_DOCS_PER_CALL\n",
    "    returns: list[int] of topic IDs, same length\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt = build_batched_prompt(texts)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": f\"{system_prompt}\\n\\nUSER:\\n{user_prompt}\",\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"num_predict\": MAX_TOKENS,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    resp = requests.post(\n",
    "        f\"{OLLAMA_BASE_URL}/api/generate\",\n",
    "        json=payload,\n",
    "        timeout=120,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    raw = data.get(\"response\", \"\").strip()\n",
    "\n",
    "    # Expect something like: [0, 5, 12]\n",
    "    try:\n",
    "        # Be defensive: sometimes model wraps in backticks or text\n",
    "        start = raw.find(\"[\")\n",
    "        end = raw.rfind(\"]\")\n",
    "        if start == -1 or end == -1:\n",
    "            raise ValueError(f\"Invalid JSON array: {raw}\")\n",
    "        arr_str = raw[start:end+1]\n",
    "        ids = json.loads(arr_str)\n",
    "        # enforce list[int] of correct length\n",
    "        if not isinstance(ids, list) or len(ids) != len(texts):\n",
    "            raise ValueError(f\"Length mismatch or invalid list: {ids}\")\n",
    "        clean_ids = []\n",
    "        for v in ids:\n",
    "            try:\n",
    "                clean_ids.append(int(v))\n",
    "            except Exception:\n",
    "                clean_ids.append(0)\n",
    "        return clean_ids\n",
    "    except Exception as e:\n",
    "        # Fallback: mark all as topic 0\n",
    "        print(f\"Parse error: {e} | raw: {raw[:200]}\")\n",
    "        return [0] * len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level classification over all documents\n",
    "def chunk_iterable(seq, size):\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield i, seq[i:i+size]\n",
    "\n",
    "def classify_all_documents(documents, max_workers=MAX_WORKERS, batch_docs=BATCH_DOCS_PER_CALL):\n",
    "    \"\"\"\n",
    "    documents: list[str]\n",
    "    returns: list[int] topic_ids aligned with documents\n",
    "    \"\"\"\n",
    "    n = len(documents)\n",
    "    topic_ids = [0] * n\n",
    "\n",
    "    # Build batch index mapping\n",
    "    batches = list(chunk_iterable(documents, batch_docs))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        for batch_idx, (start, texts) in enumerate(batches):\n",
    "            fut = executor.submit(classify_batch_with_ollama, texts)\n",
    "            futures[fut] = (start, len(texts))\n",
    "\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Classifying (batched)\"):\n",
    "            start, length = futures[fut]\n",
    "            try:\n",
    "                batch_ids = fut.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Worker error: {e}\")\n",
    "                batch_ids = [0] * length\n",
    "            # write into main array\n",
    "            topic_ids[start:start+length] = batch_ids\n",
    "\n",
    "    return topic_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f91594",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3508cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents to classify: 433,973\n",
      "Parallel workers: 16\n",
      "Docs per model call: 16\n"
     ]
    }
   ],
   "source": [
    "# Load your pseudodocs (adapt path/column name if needed)\n",
    "if not ollama_ready:\n",
    "    raise RuntimeError(\"Ollama not ready\")\n",
    "\n",
    "input_file = INPUT_DIR / \"thread_pseudodocs.parquet\"  # adjust to your file\n",
    "thread_docs = pd.read_parquet(input_file)\n",
    "\n",
    "# Make sure 'pseudodoc_text' exists, as in your original notebook\n",
    "documents = thread_docs['pseudodoc_text'].astype(str).tolist()\n",
    "\n",
    "print(f\"\\nDocuments to classify: {len(documents):,}\")\n",
    "print(f\"Parallel workers: {MAX_WORKERS}\")\n",
    "print(f\"Docs per model call: {BATCH_DOCS_PER_CALL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d33cb",
   "metadata": {},
   "source": [
    "## Classify All Documents\n",
    "\n",
    "Process documents in parallel for maximum speed. Estimated time: 1-2 hours for ~430k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee063d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecea135f35624e40918604a59f139c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying (batched):   0%|          | 0/27124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse error: Invalid JSON array: ```json | raw: ```json\n",
      "Parse error: Invalid JSON array: [0, | raw: [0,\n",
      "Parse error: Invalid JSON array: [1, | raw: [1,\n",
      "Parse error: Invalid JSON array: [1, | raw: [1,\n",
      "Parse error: Invalid JSON array: [0, | raw: [0,\n",
      "Parse error: Invalid JSON array: [1, | raw: [1,\n",
      "Parse error: Invalid JSON array: [0, | raw: [0,\n",
      "Parse error: Invalid JSON array: [1, | raw: [1,\n",
      "Parse error: Invalid JSON array: Here's | raw: Here's\n"
     ]
    }
   ],
   "source": [
    "# Run classification\n",
    "start_time = datetime.now()\n",
    "topic_ids = classify_all_documents(documents)\n",
    "end_time = datetime.now()\n",
    "\n",
    "duration_min = (end_time - start_time).total_seconds() / 60\n",
    "throughput = len(documents) / duration_min if duration_min > 0 else 0\n",
    "\n",
    "print(\"\\n✓ Classification complete!\")\n",
    "print(f\" Duration: {duration_min:.2f} minutes\")\n",
    "print(f\" Throughput: {throughput:.0f} docs/minute\")\n",
    "print(f\" Avg time per doc: {duration_min * 60 / len(documents):.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b78d07",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298143ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ollama_ready and 'topic_ids' in locals():\n",
    "    # Add results to dataframe\n",
    "    thread_docs['supervised_topic_id'] = topic_ids\n",
    "    thread_docs['supervised_topic_label'] = [POLITICAL_TOPICS[tid] for tid in topic_ids]\n",
    "    thread_docs['supervised_topic_confidence'] = confidences\n",
    "    \n",
    "    print(\"\\n=== Classification Results ===\")\n",
    "    print(f\"\\nTotal documents classified: {len(thread_docs):,}\")\n",
    "    print(f\"\\nTopic distribution:\")\n",
    "    topic_dist = thread_docs['supervised_topic_label'].value_counts().sort_index()\n",
    "    for label, count in topic_dist.items():\n",
    "        pct = count / len(thread_docs) * 100\n",
    "        print(f\"  {label:30s}: {count:7,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nConfidence statistics:\")\n",
    "    print(f\"  Mean confidence: {thread_docs['supervised_topic_confidence'].mean():.3f}\")\n",
    "    print(f\"  Median confidence: {thread_docs['supervised_topic_confidence'].median():.3f}\")\n",
    "    print(f\"  High confidence (1.0): {(thread_docs['supervised_topic_confidence'] == 1.0).sum():,} ({(thread_docs['supervised_topic_confidence'] == 1.0).sum()/len(thread_docs)*100:.2f}%)\")\n",
    "    print(f\"  Low confidence (0.5): {(thread_docs['supervised_topic_confidence'] == 0.5).sum():,} ({(thread_docs['supervised_topic_confidence'] == 0.5).sum()/len(thread_docs)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No results to analyze - run classification first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd71d16",
   "metadata": {},
   "source": [
    "## Visualize Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af35986",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ollama_ready and 'topic_ids' in locals():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    \n",
    "    # Topic distribution\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Topic counts\n",
    "    topic_counts = thread_docs['supervised_topic_id'].value_counts().sort_index()\n",
    "    ax = axes[0]\n",
    "    topic_labels_plot = [POLITICAL_TOPICS[i] for i in topic_counts.index]\n",
    "    ax.bar(range(len(topic_counts)), topic_counts.values, color='steelblue', alpha=0.7)\n",
    "    ax.set_xticks(range(len(topic_counts)))\n",
    "    ax.set_xticklabels(topic_labels_plot, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Document Count')\n",
    "    ax.set_title('Document Distribution Across Political Topics (Local SLM)')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Confidence distribution by topic\n",
    "    ax = axes[1]\n",
    "    topic_conf = thread_docs.groupby('supervised_topic_id')['supervised_topic_confidence'].mean().sort_index()\n",
    "    topic_conf_labels = [POLITICAL_TOPICS[i] for i in topic_conf.index]\n",
    "    ax.bar(range(len(topic_conf)), topic_conf.values, color='coral', alpha=0.7)\n",
    "    ax.set_xticks(range(len(topic_conf)))\n",
    "    ax.set_xticklabels(topic_conf_labels, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Mean Confidence')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(y=0.75, color='red', linestyle='--', alpha=0.5, label='High Confidence Threshold')\n",
    "    ax.set_title('Mean Classification Confidence by Topic')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'supervised_llm_topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Visualization saved\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No results to visualize - run classification first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d725d1e",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ollama_ready and 'topic_ids' in locals():\n",
    "    # Temporal trends\n",
    "    thread_docs['date'] = pd.to_datetime(thread_docs['date_temp'])\n",
    "    temporal_df = thread_docs.groupby(['date', 'supervised_topic_label']).size().reset_index(name='count')\n",
    "    \n",
    "    # Top 5 topics over time\n",
    "    top_topics = thread_docs['supervised_topic_label'].value_counts().head(5).index\n",
    "    temporal_top = temporal_df[temporal_df['supervised_topic_label'].isin(top_topics)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for topic in top_topics:\n",
    "        topic_data = temporal_top[temporal_top['supervised_topic_label'] == topic]\n",
    "        ax.plot(topic_data['date'], topic_data['count'], label=topic, marker='o', markersize=3, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Document Count')\n",
    "    ax.set_title('Top 5 Topics Over Time (Sep-Oct 2016) - Local SLM Classification')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'supervised_llm_temporal_trends.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Temporal analysis complete\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No results for temporal analysis - run classification first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aca856",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach topic labels & simple confidence placeholder\n",
    "\n",
    "thread_docs['supervised_topic_id'] = topic_ids\n",
    "thread_docs['supervised_topic_label'] = [id_to_topic.get(tid, \"Unknown\") for tid in topic_ids]\n",
    "\n",
    "# If you still want a crude confidence, you can set all to 1.0 or 0.75\n",
    "thread_docs['supervised_topic_confidence'] = 1.0\n",
    "\n",
    "print(\"\\n=== Classification Results (basic) ===\")\n",
    "print(f\"Total documents: {len(thread_docs):,}\")\n",
    "print(\"\\nTopic distribution:\")\n",
    "topic_dist = thread_docs['supervised_topic_label'].value_counts().sort_index()\n",
    "for label, count in topic_dist.items():\n",
    "    pct = count / len(thread_docs) * 100\n",
    "    print(f\" {label:30s}: {count:7,} ({pct:5.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4173e",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results + metadata\n",
    "\n",
    "output_file = OUTPUT_DIR / 'thread_pseudodocs_with_supervised_topics_llm_batched.parquet'\n",
    "thread_docs.to_parquet(output_file, index=False)\n",
    "\n",
    "duration = duration_min\n",
    "metadata = {\n",
    "    \"method\": \"local_slm_classification_batched\",\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"taxonomy\": \"Comparative Agendas Project (CAP)\",\n",
    "    \"num_topics\": 20,\n",
    "    \"topics\": POLITICAL_TOPICS,\n",
    "    \"num_documents\": len(thread_docs),\n",
    "    \"classification_date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"duration_minutes\": duration,\n",
    "    \"throughput_docs_per_minute\": throughput,\n",
    "    \"workers\": MAX_WORKERS,\n",
    "    \"docs_per_call\": BATCH_DOCS_PER_CALL,\n",
    "}\n",
    "\n",
    "metadata_file = OUTPUT_DIR / 'supervised_llm_classification_metadata_batched.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {output_file}\")\n",
    "print(f\"✓ Metadata saved to: {metadata_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
