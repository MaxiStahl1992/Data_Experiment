{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7125f7",
   "metadata": {},
   "source": [
    "## 1. Verify GPU & Install Dependencies\n",
    "\n",
    "**VS Code + Colab Kernel Integration**\n",
    "\n",
    "This notebook uses a Colab T4 GPU kernel connected through VS Code. Data is loaded from your local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f21f25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (Colab kernel)\n",
    "!pip install -q transformers torch pandas pyarrow fastparquet tqdm scikit-learn\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c20b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì GPU available: Tesla T4\n",
      "  CUDA version: 12.6\n",
      "  Memory: 14.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úì GPU available: {gpu_name}\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  NO GPU DETECTED!\")\n",
    "    print(\"   Go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "    raise RuntimeError(\"GPU required for fast processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7399882",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Setup Paths\n",
    "\n",
    "**Google Colab Web Interface**: You can mount your Google Drive directly to access files.\n",
    "\n",
    "Make sure these files are in your Google Drive:\n",
    "- `MyDrive/02_Master/11_Thesis/Data_Experiment/data/reddit/topics/comments_expanded_with_topics.parquet`\n",
    "- `MyDrive/02_Master/11_Thesis/Data_Experiment/data/reddit/topics/submissions_expanded_with_topics.parquet`\n",
    "- `MyDrive/02_Master/11_Thesis/Data_Experiment/data/reddit/polarisation/test_set_annotation.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd43dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE UPLOAD\n",
      "================================================================================\n",
      "\n",
      "üì§ Please upload the following 3 files:\n",
      "   1. comments_expanded_with_topics.parquet\n",
      "   2. submissions_expanded_with_topics.parquet\n",
      "   3. test_set_annotations.parquet\n",
      "\n",
      "‚è≥ Click the 'Choose Files' button below when it appears...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-6c45b907-e3e5-4619-923b-7d719113b1c5\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-6c45b907-e3e5-4619-923b-7d719113b1c5\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3223858288.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n‚úÖ {len(uploaded)} files uploaded successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MOUNTING GOOGLE DRIVE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìÇ Mounting your Google Drive...\")\n",
    "print(\"   You'll be asked to authorize access.\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
    "print(f\"   Access your files at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths from Google Drive\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "TIME_PERIOD = '2016-09_2016-10'\n",
    "\n",
    "# Google Drive paths\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/02_Master/11_Thesis/Data_Experiment/data/reddit')\n",
    "\n",
    "comments_path = DRIVE_ROOT / 'topics' / 'comments_expanded_with_topics.parquet'\n",
    "submissions_path = DRIVE_ROOT / 'topics' / 'submissions_expanded_with_topics.parquet'\n",
    "annotations_path = DRIVE_ROOT / 'polarisation' / 'test_set_annotation.parquet'\n",
    "\n",
    "# Output directory (in Google Drive for automatic sync)\n",
    "OUTPUT_DIR = Path('/content/drive/MyDrive/02_Master/11_Thesis/Data_Experiment/data/reddit/polarisation') / TIME_PERIOD\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PATHS CONFIGURED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Comments: {comments_path}\")\n",
    "print(f\"  Submissions: {submissions_path}\")\n",
    "print(f\"  Annotations: {annotations_path}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print()\n",
    "\n",
    "# Verify files exist\n",
    "if not comments_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Comments file not found at: {comments_path}\")\n",
    "if not submissions_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Submissions file not found at: {submissions_path}\")\n",
    "if not annotations_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Annotations file not found at: {annotations_path}\")\n",
    "\n",
    "print(\"‚úì All input files verified\")\n",
    "print(f\"  Comments: {comments_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "print(f\"  Submissions: {submissions_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "print(f\"  Annotations: {annotations_path.stat().st_size / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comments_df = pd.read_parquet(comments_path)\n",
    "print(f\"‚úì Comments: {len(comments_df):,} rows\")\n",
    "\n",
    "submissions_df = pd.read_parquet(submissions_path)\n",
    "print(f\"‚úì Submissions: {len(submissions_df):,} rows\")\n",
    "\n",
    "test_annotations = pd.read_parquet(annotations_path)\n",
    "print(f\"‚úì Annotations: {len(test_annotations):,} samples\")\n",
    "\n",
    "print(f\"\\nTotal texts to process: {len(comments_df) + len(submissions_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7e43f",
   "metadata": {},
   "source": [
    "## 3. Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94022214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion-based model (best performer from model comparison)\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "\n",
    "class EmotionBasedModel:\n",
    "    \"\"\"Map emotions (anger, disgust, etc.) to affective polarization levels.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, num_classes: int = 3):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        # Configure pipeline with GPU and large batch size\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            batch_size=512  # Large batches for GPU\n",
    "        )\n",
    "        print(\"‚úì Model loaded\")\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Map emotions to polarization:\n",
    "        - anger, disgust ‚Üí 1-2 (adversarial/intolerant)\n",
    "        - fear ‚Üí 1 (adversarial)\n",
    "        - joy, love, surprise, sadness, neutral ‚Üí 0 (none)\n",
    "        \"\"\"\n",
    "        results = self.pipe(texts, truncation=True, max_length=512)\n",
    "        labels = []\n",
    "        \n",
    "        for result in results:\n",
    "            emotion = result['label'].lower()\n",
    "            score = result['score']\n",
    "            \n",
    "            if emotion in ['anger', 'disgust']:\n",
    "                # High confidence anger/disgust ‚Üí intolerant\n",
    "                if score > 0.7:\n",
    "                    labels.append(2)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "            elif emotion == 'fear':\n",
    "                labels.append(1)  # Adversarial\n",
    "            else:\n",
    "                labels.append(0)  # None\n",
    "        \n",
    "        return np.array(labels)\n",
    "    \n",
    "    def predict_proba(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Get probability distribution (simple one-hot based on predictions).\"\"\"\n",
    "        predictions = self.predict(texts)\n",
    "        probs = np.zeros((len(predictions), self.num_classes))\n",
    "        for i, pred in enumerate(predictions):\n",
    "            probs[i, pred] = 1.0\n",
    "        return probs\n",
    "\n",
    "print(\"‚úì Model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5ce18",
   "metadata": {},
   "source": [
    "## 4. Determine Number of Classes from Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check annotation distribution\n",
    "test_annotated = test_annotations[test_annotations['affective_polarization_label'].notna()].copy()\n",
    "\n",
    "print(\"Annotation Statistics:\")\n",
    "print(f\"  Total annotated: {len(test_annotated)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "label_counts = test_annotated['affective_polarization_label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "\n",
    "# Determine number of classes\n",
    "if 3 not in label_counts.index or label_counts.get(3, 0) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No Level 3 (Belligerent) examples found.\")\n",
    "    print(\"    Using 3-class classification: 0=None, 1=Adversarial, 2=Intolerant\")\n",
    "    num_classes = 3\n",
    "    class_labels = [0, 1, 2]\n",
    "    class_names = ['None', 'Adversarial', 'Intolerant']\n",
    "else:\n",
    "    print(\"\\n‚úì Using 4-class classification\")\n",
    "    num_classes = 4\n",
    "    class_labels = [0, 1, 2, 3]\n",
    "    class_names = ['None', 'Adversarial', 'Intolerant', 'Belligerent']\n",
    "\n",
    "print(f\"\\n‚úì Configuration: {num_classes} classes\")\n",
    "print(f\"  Labels: {class_labels}\")\n",
    "print(f\"  Names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7a7ef",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best model (Emotion-English from local testing)\n",
    "MODEL_NAME = 'j-hartmann/emotion-english-distilroberta-base'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model = EmotionBasedModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Model ready for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9a467",
   "metadata": {},
   "source": [
    "## 6. Process Comments (Main Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process comments with GPU optimization\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING COMMENTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total comments: {len(comments_df):,}\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print()\n",
    "\n",
    "# Extract texts\n",
    "comment_texts = comments_df['text'].tolist()\n",
    "\n",
    "# GPU-optimized batch size (T4 can handle large batches)\n",
    "batch_size = 1024  # Increased for GPU\n",
    "\n",
    "all_predictions = []\n",
    "all_probs = []\n",
    "\n",
    "print(f\"Processing in batches of {batch_size}...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "first_batch = True\n",
    "\n",
    "for i in tqdm(range(0, len(comment_texts), batch_size), desc=\"Processing comments\"):\n",
    "    batch = comment_texts[i:i+batch_size]\n",
    "    \n",
    "    # Get predictions\n",
    "    preds = model.predict(batch)\n",
    "    probs = model.predict_proba(batch)\n",
    "    \n",
    "    all_predictions.extend(preds)\n",
    "    all_probs.extend(probs)\n",
    "    \n",
    "    # Time estimation after first batch\n",
    "    if first_batch and i == 0:\n",
    "        first_batch = False\n",
    "        elapsed = time.time() - start_time\n",
    "        total_batches = (len(comment_texts) + batch_size - 1) // batch_size\n",
    "        estimated_time = elapsed * total_batches\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  First batch: {elapsed:.1f}s for {len(batch)} texts\")\n",
    "        print(f\"üìä Estimated total: {estimated_time/60:.1f} minutes ({estimated_time/3600:.1f} hours)\")\n",
    "        print(f\"üí° Speed: ~{len(batch)/elapsed:.0f} texts/second\\n\")\n",
    "\n",
    "# Add results to dataframe\n",
    "comments_with_polarization = comments_df.copy()\n",
    "comments_with_polarization['affective_polarization_label'] = all_predictions\n",
    "comments_with_polarization['affective_polarization_score'] = [\n",
    "    pred / (num_classes - 1) for pred in all_predictions\n",
    "]\n",
    "\n",
    "# Add probability columns\n",
    "for i in range(num_classes):\n",
    "    comments_with_polarization[f'polarization_prob_{i}'] = [probs[i] for probs in all_probs]\n",
    "\n",
    "comments_with_polarization['polarization_confidence'] = [max(probs) for probs in all_probs]\n",
    "\n",
    "# Summary\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì COMMENTS PROCESSING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed_total/60:.1f} minutes ({elapsed_total/3600:.2f} hours)\")\n",
    "print(f\"üí° Average speed: {len(comment_texts)/elapsed_total:.0f} texts/second\")\n",
    "print(f\"\\nAffective Polarization Distribution:\")\n",
    "print(comments_with_polarization['affective_polarization_label'].value_counts().sort_index())\n",
    "print(f\"\\nMean score: {comments_with_polarization['affective_polarization_score'].mean():.3f}\")\n",
    "print(f\"Mean confidence: {comments_with_polarization['polarization_confidence'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb09c0",
   "metadata": {},
   "source": [
    "## 7. Process Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa170a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process submissions\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING SUBMISSIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total submissions: {len(submissions_df):,}\")\n",
    "print()\n",
    "\n",
    "submission_texts = submissions_df['text'].tolist()\n",
    "\n",
    "all_predictions = []\n",
    "all_probs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(0, len(submission_texts), batch_size), desc=\"Processing submissions\"):\n",
    "    batch = submission_texts[i:i+batch_size]\n",
    "    \n",
    "    preds = model.predict(batch)\n",
    "    probs = model.predict_proba(batch)\n",
    "    \n",
    "    all_predictions.extend(preds)\n",
    "    all_probs.extend(probs)\n",
    "\n",
    "# Add results\n",
    "submissions_with_polarization = submissions_df.copy()\n",
    "submissions_with_polarization['affective_polarization_label'] = all_predictions\n",
    "submissions_with_polarization['affective_polarization_score'] = [\n",
    "    pred / (num_classes - 1) for pred in all_predictions\n",
    "]\n",
    "\n",
    "for i in range(num_classes):\n",
    "    submissions_with_polarization[f'polarization_prob_{i}'] = [probs[i] for probs in all_probs]\n",
    "\n",
    "submissions_with_polarization['polarization_confidence'] = [max(probs) for probs in all_probs]\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì SUBMISSIONS PROCESSING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚è±Ô∏è  Time: {elapsed_total/60:.1f} minutes\")\n",
    "print(f\"\\nAffective Polarization Distribution:\")\n",
    "print(submissions_with_polarization['affective_polarization_label'].value_counts().sort_index())\n",
    "print(f\"\\nMean score: {submissions_with_polarization['affective_polarization_score'].mean():.3f}\")\n",
    "print(f\"Mean confidence: {submissions_with_polarization['polarization_confidence'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbc7b5",
   "metadata": {},
   "source": [
    "## 8. Save & Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37994b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are automatically saved to Google Drive\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTS SAVED TO GOOGLE DRIVE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ All files saved to Google Drive!\")\n",
    "print(f\"\\nüìÇ Output location:\")\n",
    "print(f\"   {OUTPUT_DIR}\")\n",
    "print(f\"\\nüìÅ Files saved:\")\n",
    "for filepath in OUTPUT_DIR.glob('*.parquet'):\n",
    "    file_size_mb = filepath.stat().st_size / 1024**2\n",
    "    print(f\"   ‚úì {filepath.name} ({file_size_mb:.1f} MB)\")\n",
    "\n",
    "metadata_file = OUTPUT_DIR / 'metadata.json'\n",
    "if metadata_file.exists():\n",
    "    file_size_kb = metadata_file.stat().st_size / 1024\n",
    "    print(f\"   ‚úì {metadata_file.name} ({file_size_kb:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nüíæ Files will sync to your local machine automatically via Google Drive sync.\")\n",
    "print(f\"\\nüîÑ Or access directly from:\")\n",
    "print(f\"   drive.google.com/drive/my-drive/02_Master/11_Thesis/Data_Experiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f312ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets to Colab runtime\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Save comments\n",
    "comments_output = OUTPUT_DIR / 'comments_with_polarization.parquet'\n",
    "comments_with_polarization.to_parquet(comments_output, index=False)\n",
    "file_size_mb = comments_output.stat().st_size / 1024**2\n",
    "print(f\"‚úì Comments saved: {comments_output.name}\")\n",
    "print(f\"  Shape: {comments_with_polarization.shape}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB\\n\")\n",
    "\n",
    "# Save submissions\n",
    "submissions_output = OUTPUT_DIR / 'submissions_with_polarization.parquet'\n",
    "submissions_with_polarization.to_parquet(submissions_output, index=False)\n",
    "file_size_mb = submissions_output.stat().st_size / 1024**2\n",
    "print(f\"‚úì Submissions saved: {submissions_output.name}\")\n",
    "print(f\"  Shape: {submissions_with_polarization.shape}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB\\n\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'time_period': TIME_PERIOD,\n",
    "    'model_used': MODEL_NAME,\n",
    "    'processing_environment': 'Google Colab (VS Code kernel)',\n",
    "    'device': str(model.device),\n",
    "    'batch_size': batch_size,\n",
    "    'num_classes': num_classes,\n",
    "    'class_labels': class_labels,\n",
    "    'class_names': class_names,\n",
    "    'comments': {\n",
    "        'total': len(comments_with_polarization),\n",
    "        'distribution': comments_with_polarization['affective_polarization_label'].value_counts().to_dict(),\n",
    "        'mean_score': float(comments_with_polarization['affective_polarization_score'].mean()),\n",
    "        'mean_confidence': float(comments_with_polarization['polarization_confidence'].mean())\n",
    "    },\n",
    "    'submissions': {\n",
    "        'total': len(submissions_with_polarization),\n",
    "        'distribution': submissions_with_polarization['affective_polarization_label'].value_counts().to_dict(),\n",
    "        'mean_score': float(submissions_with_polarization['affective_polarization_score'].mean()),\n",
    "        'mean_confidence': float(submissions_with_polarization['polarization_confidence'].mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Metadata saved: {metadata_path.name}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ FILES SAVED TO COLAB RUNTIME\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62040e",
   "metadata": {},
   "source": [
    "## 9. Quick Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f78e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_texts = len(comments_with_polarization) + len(submissions_with_polarization)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total texts processed: {total_texts:,}\")\n",
    "print(f\"  Comments: {len(comments_with_polarization):,}\")\n",
    "print(f\"  Submissions: {len(submissions_with_polarization):,}\")\n",
    "\n",
    "# Combined distribution\n",
    "combined_labels = list(comments_with_polarization['affective_polarization_label']) + \\\n",
    "                  list(submissions_with_polarization['affective_polarization_label'])\n",
    "\n",
    "print(f\"\\nüìà Overall Affective Polarization Distribution:\")\n",
    "for label, name in zip(class_labels, class_names):\n",
    "    count = combined_labels.count(label)\n",
    "    pct = count / len(combined_labels) * 100\n",
    "    print(f\"  Level {label} ({name:12s}): {count:6,} ({pct:5.1f}%)\")\n",
    "\n",
    "# Overall scores\n",
    "combined_scores = list(comments_with_polarization['affective_polarization_score']) + \\\n",
    "                  list(submissions_with_polarization['affective_polarization_score'])\n",
    "combined_confidence = list(comments_with_polarization['polarization_confidence']) + \\\n",
    "                      list(submissions_with_polarization['polarization_confidence'])\n",
    "\n",
    "print(f\"\\nüìä Score Statistics:\")\n",
    "print(f\"  Mean polarization score: {np.mean(combined_scores):.3f}\")\n",
    "print(f\"  Median polarization score: {np.median(combined_scores):.3f}\")\n",
    "print(f\"  Mean confidence: {np.mean(combined_confidence):.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete! Results saved to Google Drive.\")\n",
    "print(f\"   Files will sync to your OneDrive/Thesis folder automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfac8b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Done!\n",
    "\n",
    "### Workflow Summary\n",
    "\n",
    "1. ‚úÖ Mounted Google Drive (Section 2)\n",
    "2. ‚úÖ Processed ~700k texts with T4 GPU (~30-60 minutes)\n",
    "3. ‚úÖ Saved results directly to Google Drive (Section 8)\n",
    "\n",
    "### Output Files Location\n",
    "\n",
    "Files saved to Google Drive:\n",
    "```\n",
    "MyDrive/02_Master/11_Thesis/Data_Experiment/\n",
    "‚îî‚îÄ‚îÄ data/reddit/polarisation/2016-09_2016-10/\n",
    "    ‚îú‚îÄ‚îÄ comments_with_polarization.parquet\n",
    "    ‚îú‚îÄ‚îÄ submissions_with_polarization.parquet\n",
    "    ‚îî‚îÄ‚îÄ metadata.json\n",
    "```\n",
    "\n",
    "Files will automatically sync to your local machine if you have Google Drive desktop sync enabled.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Wait for Google Drive sync** (or download files manually from Google Drive)\n",
    "2. **Load results** in your local notebook (18_sentiment_detetction.ipynb) \n",
    "3. **Create validation sample** for quality checking\n",
    "4. **Run analysis** on the processed data\n",
    "\n",
    "### Performance Notes\n",
    "\n",
    "- **T4 GPU**: ~150-300 texts/second (30-60 min for full dataset)\n",
    "\n",
    "- **Upgrade to H100**: If T4 is slow, change runtime type to A100/H100 for 3-5x speedup4. Run all cells (Runtime ‚Üí Run all)\n",
    "\n",
    "- **Batch size**: 1024 (optimal for T4). Increase to 2048+ for H1003. Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "\n",
    "2. Right-click ‚Üí Open with ‚Üí Google Colaboratory\n",
    "\n",
    "### How to Run in Google Colab1. Upload this notebook to Google Drive\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
