{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ee396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "âœ“ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "workspace_root = Path().cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "# Thesis pipeline utilities\n",
    "from thesis_pipeline.io.paths import get_data_path\n",
    "from thesis_pipeline.io.parquet import read_parquet, write_parquet\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05e8ea",
   "metadata": {},
   "source": [
    "# 1. Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b6967fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics data: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/02_topics/reddit\n",
      "Gold data: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "topics_path = get_data_path('topics', 'reddit')\n",
    "gold_path = get_data_path('gold', 'reddit')\n",
    "\n",
    "print(f\"Topics data: {topics_path}\")\n",
    "print(f\"Gold data: {gold_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bb320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Loaded supervised topics:\n",
      "  Thread documents: 433,973\n",
      "  Date range: 1472688024 to 1477954796\n",
      "  Topics defined: 20\n",
      "\n",
      "  Sample topic definition:\n",
      "    ID: 0\n",
      "    Label: Elections & Voting\n",
      "    Description: Electoral processes, political campaigns, voting rights, electoral reform, voter registration, election results, polling, ballots, primaries, caucuses, electoral college\n"
     ]
    }
   ],
   "source": [
    "# Load supervised topic assignments (multi-label)\n",
    "thread_docs = read_parquet(topics_path / 'embeddings' /'thread_pseudodocs_with_supervised_topics_multilabel.parquet')\n",
    "\n",
    "# Load metadata for topic definitions\n",
    "with open(topics_path / 'embeddings' / 'supervised_multilabel_classification_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Extract topic definitions\n",
    "topics_info = metadata['topics']\n",
    "topic_definitions = {\n",
    "    int(tid): {\n",
    "        'id': int(tid),\n",
    "        'label': topics_info[str(tid)]['label'],\n",
    "        'description': topics_info[str(tid)]['description']\n",
    "    }\n",
    "    for tid in range(20)\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“Š Loaded supervised topics:\")\n",
    "print(f\"  Thread documents: {len(thread_docs):,}\")\n",
    "print(f\"  Date range: {thread_docs['created_utc'].min():.0f} to {thread_docs['created_utc'].max():.0f}\")\n",
    "print(f\"  Topics defined: {len(topic_definitions)}\")\n",
    "print(f\"\\n  Sample topic definition:\")\n",
    "print(f\"    ID: {topic_definitions[0]['id']}\")\n",
    "print(f\"    Label: {topic_definitions[0]['label']}\")\n",
    "print(f\"    Description: {topic_definitions[0]['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759b61b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Loading gold data from:\n",
      "  Comments: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit/comments\n",
      "  Submissions: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/01_corpus/02_gold/reddit/submissions\n",
      "\n",
      "  Found 2 comment files: ['2016-09.parquet', '2016-10.parquet']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f812aca94b48c4861d3a30039a7fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading comments:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 2 submission files: ['2016-09.parquet', '2016-10.parquet']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21392b40387448fa9e7b10c4117c8d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading submissions:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_path = gold_path / 'comments'\n",
    "submissions_path = gold_path / 'submissions'\n",
    "\n",
    "print(f\"\\nðŸ“¦ Loading gold data from:\")\n",
    "print(f\"  Comments: {comments_path}\")\n",
    "print(f\"  Submissions: {submissions_path}\")\n",
    "\n",
    "# Load comments\n",
    "comment_files = sorted(comments_path.glob('2016-*.parquet'))\n",
    "print(f\"\\n  Found {len(comment_files)} comment files: {[f.name for f in comment_files]}\")\n",
    "\n",
    "comments_chunks = []\n",
    "for file in tqdm(comment_files, desc=\"Loading comments\"):\n",
    "    chunk = read_parquet(file)\n",
    "    chunk['is_submission'] = False\n",
    "    comments_chunks.append(chunk)\n",
    "\n",
    "comments_df = pd.concat(comments_chunks, ignore_index=True) if comments_chunks else pd.DataFrame()\n",
    "\n",
    "# Load submissions\n",
    "submission_files = sorted(submissions_path.glob('2016-*.parquet'))\n",
    "print(f\"  Found {len(submission_files)} submission files: {[f.name for f in submission_files]}\")\n",
    "\n",
    "submissions_chunks = []\n",
    "for file in tqdm(submission_files, desc=\"Loading submissions\"):\n",
    "    chunk = read_parquet(file)\n",
    "    chunk['is_submission'] = True\n",
    "    submissions_chunks.append(chunk)\n",
    "\n",
    "submissions_df = pd.concat(submissions_chunks, ignore_index=True) if submissions_chunks else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31839ab",
   "metadata": {},
   "source": [
    "# 2. Map Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951b2f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Merged topics with data:\n",
      "  Submissions with topics: 433,973\n",
      "    Unique submissions: 433,973\n",
      "    Average topics per submission: 1.27\n",
      "\n",
      "  Comments with topics: 8,624,040\n",
      "    Unique comments: 8,624,040\n",
      "    Average topics per comment: 1.36\n"
     ]
    }
   ],
   "source": [
    "# Extract topic columns from thread_docs\n",
    "submission_topics = thread_docs[['submission_id', 'supervised_topics', 'supervised_topic_labels']].copy()\n",
    "\n",
    "# Merge topics onto submissions (submission_id â†’ submission_id)\n",
    "submissions_with_topics = submissions_df.merge(\n",
    "    submission_topics,\n",
    "    on='submission_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Merge topics onto comments (submission_id â†’ submission_id)\n",
    "comments_with_topics = comments_df.merge(\n",
    "    submission_topics,\n",
    "    on='submission_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Merged topics with data:\")\n",
    "print(f\"  Submissions with topics: {len(submissions_with_topics):,}\")\n",
    "print(f\"    Unique submissions: {submissions_with_topics['submission_id'].nunique():,}\")\n",
    "print(f\"    Average topics per submission: {submissions_with_topics['supervised_topics'].apply(len).mean():.2f}\")\n",
    "print(f\"\\n  Comments with topics: {len(comments_with_topics):,}\")\n",
    "print(f\"    Unique comments: {comments_with_topics['comment_id'].nunique():,}\")\n",
    "print(f\"    Average topics per comment: {comments_with_topics['supervised_topics'].apply(len).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37570d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a567e971a74f62b61a2e717a434cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Expanding submissions:   0%|          | 0/433973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce0eb12227049009213d3253c3add5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Expanding comments:   0%|          | 0/8624040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Expanded to (text, topic) pairs:\n",
      "  Submissions: 549,962 rows from 433,973 unique submissions\n",
      "  Comments: 11,710,114 rows from 8,624,040 unique comments\n",
      "  Total pairs: 12,260,076\n"
     ]
    }
   ],
   "source": [
    "# Expand submissions to one row per (submission, topic) pair\n",
    "submissions_expanded = []\n",
    "\n",
    "for idx, row in tqdm(submissions_with_topics.iterrows(), total=len(submissions_with_topics), desc=\"Expanding submissions\"):\n",
    "    topics = row['supervised_topics']\n",
    "    topic_labels = row['supervised_topic_labels']\n",
    "    \n",
    "    for topic_id, topic_label in zip(topics, topic_labels):\n",
    "        submissions_expanded.append({\n",
    "            'submission_id': row['submission_id'],\n",
    "            'created_utc': row['created_utc'],\n",
    "            'text': row['title'],\n",
    "            'topic_id': topic_id,\n",
    "            'topic_label': topic_label,\n",
    "            'topic_description': topic_definitions[topic_id]['description']\n",
    "        })\n",
    "\n",
    "submissions_expanded_df = pd.DataFrame(submissions_expanded)\n",
    "\n",
    "# Expand comments to one row per (comment, topic) pair\n",
    "comments_expanded = []\n",
    "\n",
    "for idx, row in tqdm(comments_with_topics.iterrows(), total=len(comments_with_topics), desc=\"Expanding comments\"):\n",
    "    topics = row['supervised_topics']\n",
    "    topic_labels = row['supervised_topic_labels']\n",
    "    \n",
    "    for topic_id, topic_label in zip(topics, topic_labels):\n",
    "        comments_expanded.append({\n",
    "            'comment_id': row['comment_id'],\n",
    "            'submission_id': row['submission_id'],\n",
    "            'created_utc': row['created_utc'],\n",
    "            'text': row['body'],\n",
    "            'topic_id': topic_id,\n",
    "            'topic_label': topic_label,\n",
    "            'topic_description': topic_definitions[topic_id]['description']\n",
    "        })\n",
    "\n",
    "comments_expanded_df = pd.DataFrame(comments_expanded)\n",
    "\n",
    "print(f\"\\nâœ“ Expanded to (text, topic) pairs:\")\n",
    "print(f\"  Submissions: {len(submissions_expanded_df):,} rows from {submissions_expanded_df['submission_id'].nunique():,} unique submissions\")\n",
    "print(f\"  Comments: {len(comments_expanded_df):,} rows from {comments_expanded_df['comment_id'].nunique():,} unique comments\")\n",
    "print(f\"  Total pairs: {len(submissions_expanded_df) + len(comments_expanded_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ce8ed",
   "metadata": {},
   "source": [
    "# 3. Save Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b839b36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Wrote 549,962 rows to submissions_expanded_with_topics.parquet (29.6 MB)\n",
      "âœ“ Saved submissions: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/02_topics/reddit/embeddings/submissions_expanded_with_topics.parquet\n",
      "  Rows: 549,962\n",
      "âœ“ Wrote 11,710,114 rows to comments_expanded_with_topics.parquet (1327.3 MB)\n",
      "âœ“ Saved comments: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/02_topics/reddit/embeddings/comments_expanded_with_topics.parquet\n",
      "  Rows: 11,710,114\n",
      "\n",
      "âœ“ All files saved to: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment/data/02_topics/reddit/embeddings\n"
     ]
    }
   ],
   "source": [
    "# Save expanded dataframes with topics\n",
    "output_path = topics_path / 'embeddings'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save submissions expanded\n",
    "submissions_output = output_path / 'submissions_expanded_with_topics.parquet'\n",
    "write_parquet(submissions_expanded_df, submissions_output)\n",
    "print(f\"âœ“ Saved submissions: {submissions_output}\")\n",
    "print(f\"  Rows: {len(submissions_expanded_df):,}\")\n",
    "\n",
    "# Save comments expanded\n",
    "comments_output = output_path / 'comments_expanded_with_topics.parquet'\n",
    "write_parquet(comments_expanded_df, comments_output)\n",
    "print(f\"âœ“ Saved comments: {comments_output}\")\n",
    "print(f\"  Rows: {len(comments_expanded_df):,}\")\n",
    "\n",
    "print(f\"\\nâœ“ All files saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
