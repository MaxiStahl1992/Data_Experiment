{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b345212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit download started: 2025-12-18T14:35:57.626263\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"Reddit download started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6debb63",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0500c71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Politosphere\n",
      "Zenodo DOI: 10.5281/zenodo.5851729\n",
      "Output directory: data/00_raw/reddit/politosphere_2016-09_2016-10\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "reddit_cfg = configs['reddit']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Setup output directory\n",
    "raw_reddit_dir = workspace_root / 'data/00_raw/reddit/politosphere_2016-09_2016-10'\n",
    "raw_reddit_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Source: {reddit_cfg['source']['name']}\")\n",
    "print(f\"Zenodo DOI: {reddit_cfg['source']['zenodo_doi']}\")\n",
    "print(f\"Output directory: {raw_reddit_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568f86",
   "metadata": {},
   "source": [
    "## 2. File Structure (from manual inspection)\n",
    "\n",
    "Based on the Zenodo repository structure:\n",
    "- **Comment files by month:** `comments_YYYY-MM.bz2` (bzip2 compressed)\n",
    "- **Network files by year:** `networks_YYYY.csv`\n",
    "- **Metadata files:** `subreddits_metadata.json`, `users_metadata.json`\n",
    "\n",
    "**For Sep-Oct 2016, we need:**\n",
    "1. `comments_2016-09.bz2`\n",
    "2. `comments_2016-10.bz2`\n",
    "3. `networks_2016.csv` (thread relationships)\n",
    "4. `subreddits_metadata.json`\n",
    "5. `users_metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3e7fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenodo DOI: 10.5281/zenodo.5851729\n",
      "Record ID: 5851729\n",
      "\n",
      "Files to download: 5\n",
      "  - comments_2016-09.bz2 (comments)\n",
      "  - comments_2016-10.bz2 (comments)\n",
      "  - networks_2016.csv (networks)\n",
      "  - subreddits_metadata.json (metadata)\n",
      "  - users_metadata.json (metadata)\n"
     ]
    }
   ],
   "source": [
    "# Zenodo record information\n",
    "zenodo_doi = reddit_cfg['source']['zenodo_doi']\n",
    "record_id = zenodo_doi.split('.')[-1]  # Extract record ID: 5851729\n",
    "\n",
    "# Direct download URLs (bypassing API due to rate limits)\n",
    "zenodo_base_url = f\"https://zenodo.org/records/{record_id}/files\"\n",
    "\n",
    "# Files we need for Sep-Oct 2016\n",
    "files_to_download = [\n",
    "    {\n",
    "        'filename': 'comments_2016-09.bz2',\n",
    "        'url': f\"{zenodo_base_url}/comments_2016-09.bz2\",\n",
    "        'type': 'comments',\n",
    "        'period': '2016-09'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'comments_2016-10.bz2',\n",
    "        'url': f\"{zenodo_base_url}/comments_2016-10.bz2\",\n",
    "        'type': 'comments',\n",
    "        'period': '2016-10'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'networks_2016.csv',\n",
    "        'url': f\"{zenodo_base_url}/networks_2016.csv\",\n",
    "        'type': 'networks',\n",
    "        'period': '2016'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'subreddits_metadata.json',\n",
    "        'url': f\"{zenodo_base_url}/subreddits_metadata.json\",\n",
    "        'type': 'metadata',\n",
    "        'period': 'all'\n",
    "    },\n",
    "    {\n",
    "        'filename': 'users_metadata.json',\n",
    "        'url': f\"{zenodo_base_url}/users_metadata.json\",\n",
    "        'type': 'metadata',\n",
    "        'period': 'all'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Zenodo DOI: {zenodo_doi}\")\n",
    "print(f\"Record ID: {record_id}\")\n",
    "print(f\"\\nFiles to download: {len(files_to_download)}\")\n",
    "for f in files_to_download:\n",
    "    print(f\"  - {f['filename']} ({f['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977405b",
   "metadata": {},
   "source": [
    "## 3. Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383b3406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Download function ready\n"
     ]
    }
   ],
   "source": [
    "def download_file(url, destination, filename=None):\n",
    "    \"\"\"\n",
    "    Download a file from URL with progress bar.\n",
    "    \n",
    "    Args:\n",
    "        url: Download URL\n",
    "        destination: Directory to save file\n",
    "        filename: Optional filename override\n",
    "    \n",
    "    Returns:\n",
    "        Path to downloaded file\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = url.split('/')[-1]\n",
    "    \n",
    "    filepath = Path(destination) / filename\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if filepath.exists():\n",
    "        print(f\"âœ“ File already exists: {filename}\")\n",
    "        return filepath\n",
    "    \n",
    "    print(f\"Downloading: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filepath, 'wb') as f, tqdm(\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            size = f.write(chunk)\n",
    "            pbar.update(size)\n",
    "    \n",
    "    print(f\"âœ“ Downloaded: {filename} ({filepath.stat().st_size / (1024**2):.1f} MB)\")\n",
    "    return filepath\n",
    "\n",
    "print(\"âœ“ Download function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd05c9a",
   "metadata": {},
   "source": [
    "## 4. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9f217f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads...\n",
      "================================================================================\n",
      "âœ“ File already exists: comments_2016-09.bz2\n",
      "âœ“ File already exists: comments_2016-10.bz2\n",
      "âœ“ File already exists: networks_2016.csv\n",
      "âœ“ File already exists: subreddits_metadata.json\n",
      "Downloading: users_metadata.json\n",
      "âœ— Failed to download users_metadata.json: 429 Client Error: Too Many Requests for url: https://zenodo.org/records/5851729/files/users_metadata.json\n",
      "================================================================================\n",
      "\n",
      "âœ“ Successfully downloaded: 4/5 files\n",
      "âœ— Errors: 1\n",
      "  - Failed to download users_metadata.json: 429 Client Error: Too Many Requests for url: https://zenodo.org/records/5851729/files/users_metadata.json\n",
      "\n",
      "ðŸ’¡ Tip: Rerun this cell to retry failed downloads (existing files will be skipped)\n"
     ]
    }
   ],
   "source": [
    "# Download all required files\n",
    "import time\n",
    "\n",
    "downloaded_files = []\n",
    "download_errors = []\n",
    "\n",
    "print(\"Starting downloads...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, file_info in enumerate(files_to_download):\n",
    "    try:\n",
    "        url = file_info['url']\n",
    "        filename = file_info['filename']\n",
    "        \n",
    "        filepath = download_file(url, raw_reddit_dir, filename)\n",
    "        downloaded_files.append(filepath)\n",
    "        \n",
    "        # Add delay between downloads to avoid rate limiting (skip for last file)\n",
    "        if i < len(files_to_download) - 1:\n",
    "            time.sleep(5)  # 2 second delay\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to download {filename}: {e}\"\n",
    "        print(f\"âœ— {error_msg}\")\n",
    "        download_errors.append(error_msg)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ“ Successfully downloaded: {len(downloaded_files)}/{len(files_to_download)} files\")\n",
    "\n",
    "if download_errors:\n",
    "    print(f\"âœ— Errors: {len(download_errors)}\")\n",
    "    for err in download_errors:\n",
    "        print(f\"  - {err}\")\n",
    "    print(\"\\nðŸ’¡ Tip: Rerun this cell to retry failed downloads (existing files will be skipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231bfcd",
   "metadata": {},
   "source": [
    "## 5. Verify Downloaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d659c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded files summary:\n",
      "================================================================================\n",
      "âœ“ comments_2016-09.bz2: 500.1 MB\n",
      "âœ“ comments_2016-10.bz2: 616.0 MB\n",
      "âœ“ networks_2016.csv: 0.3 MB\n",
      "âœ“ subreddits_metadata.json: 0.1 MB\n",
      "\n",
      "Total downloaded: 1116.5 MB (1.09 GB)\n",
      "  Comments: 1116.1 MB\n",
      "  Networks: 0.3 MB\n",
      "  Metadata: 0.1 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloaded files summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_size = 0\n",
    "file_breakdown = {'comments': 0, 'networks': 0, 'metadata': 0}\n",
    "\n",
    "for filepath in downloaded_files:\n",
    "    size_mb = filepath.stat().st_size / (1024**2)\n",
    "    total_size += size_mb\n",
    "    \n",
    "    # Categorize file\n",
    "    if 'comments' in filepath.name:\n",
    "        file_breakdown['comments'] += size_mb\n",
    "    elif 'networks' in filepath.name:\n",
    "        file_breakdown['networks'] += size_mb\n",
    "    elif 'metadata' in filepath.name:\n",
    "        file_breakdown['metadata'] += size_mb\n",
    "    \n",
    "    print(f\"âœ“ {filepath.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nTotal downloaded: {total_size:.1f} MB ({total_size/1024:.2f} GB)\")\n",
    "print(f\"  Comments: {file_breakdown['comments']:.1f} MB\")\n",
    "print(f\"  Networks: {file_breakdown['networks']:.1f} MB\")\n",
    "print(f\"  Metadata: {file_breakdown['metadata']:.1f} MB\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e67b41",
   "metadata": {},
   "source": [
    "## 6. Inspect File Contents (Quick Peek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2041e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from comments_2016-09.bz2 (first 5 lines):\n",
      "================================================================================\n",
      "1: {\"author\":\"krb7H\",\"body\":\"trump seems to be gaining supporters at an increasing rate. I remember when he had single-digit chances of winning. Now it almost seems like it could be a competition. The cu...\n",
      "2: {\"author\":\"OQcjv\",\"body\":\"Hi `alictrmods`. Thank you for participating in \\/r\\/Politics. However, [your submission](https:\\/\\/www.reddit.com\\/r\\/politics\\/comments\\/50juq5\\/press_ignores_kaepernicks_h...\n",
      "3: {\"author\":\"mQu7y\",\"body\":\"The Mistakes of the Obama...\",\"body_cleaned\":\"the mistakes of the obama ...\",\"controversiality\":0,\"created_utc\":1472688002,\"distinguished\":null,\"edited\":false,\"gilded\":0,\"id\"...\n",
      "4: {\"author\":\"yEoqZ\",\"body\":\"Right. Same difference. The mexican's pay.\",\"body_cleaned\":\"right . same difference . the mexican 's pay .\",\"controversiality\":0,\"created_utc\":1472688003,\"distinguished\":null...\n",
      "5: {\"author\":\"b5Wub\",\"body\":\"These were stipulations for some of the progressive policies on the Democratic platform. Unfortunately, the platform will be shelved and democratics will continue to be centr...\n",
      "================================================================================\n",
      "\n",
      "Sample from subreddits_metadata.json (first 1000 chars):\n",
      "================================================================================\n",
      "{\"subreddit\":\"2012Elections\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"2016Elections\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"2016_elections\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"2ALiberals\",\"banned\":0,\"gun\":1,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"AOC\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"dem\",\"politician\":1,\"region\":\"\"}\n",
      "{\"subreddit\":\"Abortiondebate\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"ActiveMeasures\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"AgainstHateSubreddits\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"AgainstTheChimpire\",\"banned\":1,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"Agorism\",\"banned\":0,\"gun\":0,\"meta\":0,\"party\":\"\",\"politician\":0,\"region\":\"\"}\n",
      "{\"subreddit\":\"Albertapolitics\",\"banned\":\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "\n",
    "# Quick peek at comment file structure (first 5 lines)\n",
    "comment_file = raw_reddit_dir / 'comments_2016-09.bz2'\n",
    "if comment_file.exists():\n",
    "    print(\"Sample from comments_2016-09.bz2 (first 5 lines):\")\n",
    "    print(\"=\" * 80)\n",
    "    with bz2.open(comment_file, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            # Truncate long lines for display\n",
    "            display_line = line[:200] + '...' if len(line) > 200 else line\n",
    "            print(f\"{i+1}: {display_line.strip()}\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"âš  Comment file not found for inspection\")\n",
    "\n",
    "# Check metadata file structure\n",
    "metadata_file = raw_reddit_dir / 'subreddits_metadata.json'\n",
    "if metadata_file.exists():\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        # Read first 1000 chars to see structure\n",
    "        sample = f.read(1000)\n",
    "        print(f\"\\nSample from subreddits_metadata.json (first 1000 chars):\")\n",
    "        print(\"=\" * 80)\n",
    "        print(sample)\n",
    "        print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"âš  Subreddits metadata file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba5fdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: {\"author\":\"krb7H\",\"body\":\"trump seems to be gaining supporters at an increasing rate. I remember when he had single-digit chances of winning. Now it almost seems like it could be a competition. The current trend of the graph on 538 terrifies me\",\"body_cleaned\":\"trump seems to be gaining supporters at an increasing rate . i remember when he had single-digit chances of winning . now it almost seems like it could be a competition . the current trend of the graph on 538 terrifies me\",\"controversiality\":0,\"created_utc\":1472688001,\"distinguished\":null,\"edited\":false,\"gilded\":0,\"id\":\"d74qmz3\",\"language\":\"en\",\"link_id\":\"t3_50grgt\",\"parent_id\":\"t1_d74ft4z\",\"retrieved_on\":1475215923,\"score\":3,\"subreddit\":\"politics\",\"subreddit_id\":\"t5_2cneq\"}\n",
      "2: {\"author\":\"OQcjv\",\"body\":\"Hi `alictrmods`. Thank you for participating in \\/r\\/Politics. However, [your submission](https:\\/\\/www.reddit.com\\/r\\/politics\\/comments\\/50juq5\\/press_ignores_kaepernicks_hillary_for_prison\\/) has been removed for the following reason(s):\\n\\n* [Off-Topic](http:\\/\\/www.reddit.com\\/r\\/politics\\/wiki\\/rulesandregs#wiki_the_.2Fr.2Fpolitics_on_topic_statement): All submissions to \\/r\\/politics need to be explicitly about **current US politics**.\\n\\n \\n\\n\\n\\nIf you have any questions about this removal, please feel free to [message the moderators.](https:\\/\\/www.reddit.com\\/message\\/compose?to=\\/r\\/politics&amp;subject=Question regarding the removal of this submission by \\/u\\/xxxxx&amp;message=I have a question regarding the removal of this [submission.](https:\\/\\/www.reddit.com\\/r\\/politics\\/comments\\/50juq5\\/press_ignores_kaepernicks_hillary_for_prison\\/?context=10000\\\\))\",\"body_cleaned\":\"hi ` alictrmods ` . thank you for participating in SUBREDDIT . however , [ your submission ] ( URL ) has been removed for the following reason ( s ) : * [ off-topic ] ( URL ) : all submissions to SUBREDDIT need to be explicitly about **current us politics** . if you have any questions about this removal , please feel free to [ message the moderators . ] ( URL regarding the removal of this submission by USER & message=i have a question regarding the removal of this [ submission . ] ( URL\\\\ ) )\",\"controversiality\":0,\"created_utc\":1472688001,\"distinguished\":\"moderator\",\"edited\":false,\"gilded\":0,\"id\":\"d74qmze\",\"language\":\"en\",\"link_id\":\"t3_50juq5\",\"parent_id\":\"t3_50juq5\",\"retrieved_on\":1475215923,\"score\":1,\"subreddit\":\"politics\",\"subreddit_id\":\"t5_2cneq\"}\n"
     ]
    }
   ],
   "source": [
    "with bz2.open(comment_file, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 2:\n",
    "                  break;\n",
    "            print(f\"{i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987c4c6",
   "metadata": {},
   "source": [
    "## 7. Save Download Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b8b35c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Download metadata saved: data/00_raw/reddit/politosphere_2016-09_2016-10/download_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Record download details\n",
    "download_metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'source': {\n",
    "        'name': reddit_cfg['source']['name'],\n",
    "        'zenodo_doi': zenodo_doi,\n",
    "        'github_repo': reddit_cfg['source']['github_repo'],\n",
    "        'record_id': record_id\n",
    "    },\n",
    "    'period': {\n",
    "        'start': global_cfg['validation_run']['period_start'],\n",
    "        'end': global_cfg['validation_run']['period_end']\n",
    "    },\n",
    "    'files': [\n",
    "        {\n",
    "            'filename': f.name,\n",
    "            'path': str(f.relative_to(workspace_root)),\n",
    "            'size_mb': round(f.stat().st_size / (1024**2), 2),\n",
    "            'type': 'comments' if 'comments' in f.name else 'networks' if 'networks' in f.name else 'metadata'\n",
    "        }\n",
    "        for f in downloaded_files\n",
    "    ],\n",
    "    'summary': {\n",
    "        'total_files': len(downloaded_files),\n",
    "        'total_size_mb': round(total_size, 2),\n",
    "        'total_size_gb': round(total_size/1024, 2),\n",
    "        'errors': download_errors if download_errors else None\n",
    "    },\n",
    "    'output_directory': str(raw_reddit_dir.relative_to(workspace_root))\n",
    "}\n",
    "\n",
    "metadata_file = raw_reddit_dir / 'download_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(download_metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Download metadata saved: {metadata_file.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b2286",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Download Complete!**\n",
    "\n",
    "Files downloaded:\n",
    "- `comments_2016-09.bz2` (Sep 2016 comments)\n",
    "- `comments_2016-10.bz2` (Oct 2016 comments)\n",
    "- `networks_2016.csv` (thread relationships)\n",
    "- `subreddits_metadata.json` (subreddit information)\n",
    "- `users_metadata.json` (user information)\n",
    "\n",
    "**Next Step:** Proceed to notebook 11 to extract, filter, and create daily silver layer files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
