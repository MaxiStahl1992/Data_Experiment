{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398d5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread pseudodocuments started: 2025-12-18T15:02:57.531437\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"Thread pseudodocuments started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863304b5",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba08639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver input: data/01_silver/reddit\n",
      "Gold output: data/03_gold/reddit\n",
      "Thread context usage: inference_only\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "reddit_cfg = configs['reddit']\n",
    "global_cfg = configs['global']\n",
    "\n",
    "# Input/output directories\n",
    "silver_reddit_dir = workspace_root / 'data/01_silver/reddit'\n",
    "gold_reddit_dir = workspace_root / 'data/02_gold/reddit'\n",
    "gold_reddit_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Silver input: {silver_reddit_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Gold output: {gold_reddit_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Thread context usage: {reddit_cfg['processing']['thread_context_usage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62616481",
   "metadata": {},
   "source": [
    "## 2. Process Daily Files with Thread Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97363de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 61 daily files...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing daily files: 100%|██████████| 61/61 [00:18<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Total comments: 8,785,795\n",
      "With thread context: 8,785,795 (100.0%)\n",
      "Files written: 61\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all silver layer files\n",
    "silver_files = sorted(silver_reddit_dir.glob('2016-*.parquet'))\n",
    "\n",
    "print(f\"Processing {len(silver_files)} daily files...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_comments = 0\n",
    "total_with_thread = 0\n",
    "files_written = []\n",
    "\n",
    "for silver_file in tqdm(silver_files, desc=\"Processing daily files\"):\n",
    "    # Read silver layer\n",
    "    df = pd.read_parquet(silver_file)\n",
    "    total_comments += len(df)\n",
    "    \n",
    "    # Use link_id as thread_id (already present in data)\n",
    "    df['thread_id'] = df['link_id'].str.replace('t3_', '', regex=False)\n",
    "    \n",
    "    # Determine if comment is top-level (parent is thread) or reply (parent is comment)\n",
    "    df['is_top_level'] = df['parent_id'].str.startswith('t3_')\n",
    "    \n",
    "    # Count how many have thread context (should be 100% since link_id is always present)\n",
    "    has_thread = df['thread_id'].notna().sum()\n",
    "    total_with_thread += has_thread\n",
    "    \n",
    "    # Reorder columns: keep timestamp-related first\n",
    "    column_order = [\n",
    "        'date',\n",
    "        'created_utc',\n",
    "        'comment_id',\n",
    "        'thread_id',\n",
    "        'is_top_level',\n",
    "        'author',\n",
    "        'subreddit',\n",
    "        'subreddit_id',\n",
    "        'body',\n",
    "        'cleaned_body',\n",
    "        'score',\n",
    "        'parent_id'\n",
    "    ]\n",
    "    \n",
    "    # Only keep columns that exist\n",
    "    column_order = [col for col in column_order if col in df.columns]\n",
    "    df = df[column_order]\n",
    "    \n",
    "    # Write to gold layer (same date, with thread context)\n",
    "    output_file = gold_reddit_dir / silver_file.name\n",
    "    df.to_parquet(output_file, compression='snappy', index=False)\n",
    "    files_written.append(output_file)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total comments: {total_comments:,}\")\n",
    "print(f\"With thread context: {total_with_thread:,} ({100*total_with_thread/total_comments:.1f}%)\")\n",
    "print(f\"Files written: {len(files_written)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e6f48",
   "metadata": {},
   "source": [
    "## 3. Verify Timestamp Integrity\n",
    "\n",
    "**Critical check:** Ensure timestamps remain comment-based, not thread-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d69bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp verification: 2016-10-01.parquet\n",
      "================================================================================\n",
      "Expected date: 2016-10-01\n",
      "Actual dates in file: ['2016-10-01']\n",
      "✓ Timestamp integrity verified: all comments match file date\n",
      "\n",
      "Sample with thread context (first 3):\n",
      "\n",
      "  Comment date: 2016-10-01\n",
      "  Thread ID: 55aq8o\n",
      "  Top-level: True\n",
      "  Comment: *Someone* gets it ..\n",
      "\n",
      "...\n",
      "\n",
      "  Comment date: 2016-10-01\n",
      "  Thread ID: 557ste\n",
      "  Top-level: False\n",
      "  Comment: &gt;Dollar coins finally catch on thanks to triumphant RenFaire nerds. \n",
      "\n",
      "Alright...\n",
      "\n",
      "  Comment date: 2016-10-01\n",
      "  Thread ID: 55ao69\n",
      "  Top-level: False\n",
      "  Comment: Something something socialism only works until......\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample file and verify timestamps\n",
    "if files_written:\n",
    "    sample_file = files_written[30]  # Mid-October\n",
    "    \n",
    "    print(f\"Timestamp verification: {sample_file.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_gold = pd.read_parquet(sample_file)\n",
    "    \n",
    "    # Check that all dates match filename\n",
    "    expected_date = sample_file.stem\n",
    "    actual_dates = df_gold['date'].unique()\n",
    "    \n",
    "    print(f\"Expected date: {expected_date}\")\n",
    "    print(f\"Actual dates in file: {actual_dates}\")\n",
    "    \n",
    "    if len(actual_dates) == 1 and actual_dates[0] == expected_date:\n",
    "        print(\"✓ Timestamp integrity verified: all comments match file date\")\n",
    "    else:\n",
    "        print(\"✗ WARNING: Date mismatch detected!\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(f\"\\nSample with thread context (first 3):\")\n",
    "    for idx, row in df_gold.head(3).iterrows():\n",
    "        print(f\"\\n  Comment date: {row['date']}\")\n",
    "        print(f\"  Thread ID: {row['thread_id']}\")\n",
    "        print(f\"  Top-level: {row['is_top_level']}\")\n",
    "        print(f\"  Comment: {row['body'][:80]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠ No files to verify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012306b",
   "metadata": {},
   "source": [
    "## 4. Thread Context Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00db5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread context coverage by date:\n",
      "================================================================================\n",
      "      date  total_comments  with_thread_id  top_level_comments  thread_pct  top_level_pct\n",
      "2016-09-01          109184          109184               31339       100.0      28.702924\n",
      "2016-09-02           98775           98775               26888       100.0      27.221463\n",
      "2016-09-03           74731           74731               20825       100.0      27.866615\n",
      "2016-09-04           80990           80990               21031       100.0      25.967403\n",
      "2016-09-05           85066           85066               23980       100.0      28.189876\n",
      "2016-09-06          110391          110391               29266       100.0      26.511219\n",
      "2016-09-07          105031          105031               30600       100.0      29.134256\n",
      "2016-09-08          123756          123756               36093       100.0      29.164647\n",
      "2016-09-09           98605           98605               28615       100.0      29.019827\n",
      "2016-09-10           87648           87648               25401       100.0      28.980696\n",
      "\n",
      "================================================================================\n",
      "Average thread ID coverage: 100.0%\n",
      "Average top-level comments: 28.1%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze coverage across all files\n",
    "print(\"Thread context coverage by date:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "coverage_stats = []\n",
    "\n",
    "for gold_file in files_written[:10]:  # Sample first 10 days\n",
    "    df = pd.read_parquet(gold_file)\n",
    "    \n",
    "    total = len(df)\n",
    "    with_thread = df['thread_id'].notna().sum()\n",
    "    top_level = df['is_top_level'].sum()\n",
    "    \n",
    "    coverage_stats.append({\n",
    "        'date': gold_file.stem,\n",
    "        'total_comments': total,\n",
    "        'with_thread_id': with_thread,\n",
    "        'top_level_comments': top_level,\n",
    "        'thread_pct': 100 * with_thread / total if total > 0 else 0,\n",
    "        'top_level_pct': 100 * top_level / total if total > 0 else 0\n",
    "    })\n",
    "\n",
    "df_coverage = pd.DataFrame(coverage_stats)\n",
    "print(df_coverage.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Average thread ID coverage: {df_coverage['thread_pct'].mean():.1f}%\")\n",
    "print(f\"Average top-level comments: {df_coverage['top_level_pct'].mean():.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c5808",
   "metadata": {},
   "source": [
    "## 5. Save Processing Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29203bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gold layer metadata saved: data/03_gold/reddit/gold_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save gold layer metadata\n",
    "gold_metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'input': {\n",
    "        'silver_directory': str(silver_reddit_dir.relative_to(workspace_root)),\n",
    "        'files_processed': int(len(silver_files))  # Convert to native int\n",
    "    },\n",
    "    'thread_context': {\n",
    "        'source': 'link_id field from comments (already present)',\n",
    "        'thread_titles_available': False,\n",
    "        'note': 'Politosphere dataset does not include submission/thread titles'\n",
    "    },\n",
    "    'processing': {\n",
    "        'total_comments': int(total_comments),  # Convert to native int\n",
    "        'with_thread_context': int(total_with_thread),  # Convert to native int\n",
    "        'thread_coverage_pct': round(100 * total_with_thread / total_comments, 2) if total_comments > 0 else 0\n",
    "    },\n",
    "    'output': {\n",
    "        'directory': str(gold_reddit_dir.relative_to(workspace_root)),\n",
    "        'files_written': int(len(files_written)),  # Convert to native int\n",
    "        'dates_covered': sorted([f.stem for f in files_written])\n",
    "    },\n",
    "    'timestamp_rule': {\n",
    "        'indexing_field': reddit_cfg['processing']['time_index_field'],\n",
    "        'thread_context_usage': reddit_cfg['processing']['thread_context_usage'],\n",
    "        'note': 'Thread context (thread_id, parent) is for inference only. All timestamps remain comment.created_utc'\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_file = gold_reddit_dir / 'gold_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(gold_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Gold layer metadata saved: {metadata_file.relative_to(workspace_root)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
