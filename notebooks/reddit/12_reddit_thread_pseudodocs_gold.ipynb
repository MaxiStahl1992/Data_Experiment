{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398d5393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold layer processing started: 2025-12-21T09:05:35.215081\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "# Reddit Gold Layer - Monthly Aggregation\n",
    "# Combines submissions and comments into monthly parquet files with clean schemas\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "while not (workspace_root / '.git').exists() and workspace_root != workspace_root.parent:\n",
    "    workspace_root = workspace_root.parent\n",
    "\n",
    "print(f\"Gold layer processing started: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863304b5",
   "metadata": {},
   "source": [
    "## 1. Setup Paths\n",
    "\n",
    "Define input (silver) and output (gold) directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba08639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments silver: data/01_corpus/01_silver/reddit/politosphere\n",
      "Submissions silver: data/01_corpus/01_silver/reddit/submissions\n",
      "\n",
      "Gold submissions: data/01_corpus/02_gold/reddit/submissions\n",
      "Gold comments: data/01_corpus/02_gold/reddit/comments\n"
     ]
    }
   ],
   "source": [
    "# Input paths\n",
    "comments_silver_dir = workspace_root / 'data/01_corpus/01_silver/reddit/politosphere'\n",
    "submissions_silver_dir = workspace_root / 'data/01_corpus/01_silver/reddit/submissions'\n",
    "\n",
    "# Output paths\n",
    "gold_submissions_dir = workspace_root / 'data/01_corpus/02_gold/reddit/submissions'\n",
    "gold_comments_dir = workspace_root / 'data/01_corpus/02_gold/reddit/comments'\n",
    "\n",
    "gold_submissions_dir.mkdir(parents=True, exist_ok=True)\n",
    "gold_comments_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Comments silver: {comments_silver_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Submissions silver: {submissions_silver_dir.relative_to(workspace_root)}\")\n",
    "print(f\"\\nGold submissions: {gold_submissions_dir.relative_to(workspace_root)}\")\n",
    "print(f\"Gold comments: {gold_comments_dir.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62616481",
   "metadata": {},
   "source": [
    "## 2. Language Detection Setup\n",
    "\n",
    "Load language detection model for comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97363de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Language detection ready (using langdetect library)\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect language of text, return 'unknown' on failure\"\"\"\n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return 'unknown'\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "print(\"✓ Language detection ready (using langdetect library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e6f48",
   "metadata": {},
   "source": [
    "## 3. Process Submissions by Month\n",
    "\n",
    "Aggregate daily submission files into monthly parquet files with clean schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d69bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing submissions for 2016-09...\n",
      "================================================================================\n",
      "Found 30 daily files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2016-09: 100%|██████████| 30/30 [00:00<00:00, 67.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 386,214 submissions\n",
      "✓ Saved: 2016-09.parquet\n",
      "  Columns: ['submission_id', 'title', 'selftext', 'created_utc', 'subreddit_id', 'subreddit', 'num_comments']\n",
      "  Rows: 386,214\n",
      "\n",
      "Processing submissions for 2016-10...\n",
      "================================================================================\n",
      "Found 31 daily files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2016-10: 100%|██████████| 31/31 [00:00<00:00, 62.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 537,217 submissions\n",
      "✓ Saved: 2016-10.parquet\n",
      "  Columns: ['submission_id', 'title', 'selftext', 'created_utc', 'subreddit_id', 'subreddit', 'num_comments']\n",
      "  Rows: 537,217\n",
      "\n",
      "================================================================================\n",
      "✓ Submissions processing complete\n"
     ]
    }
   ],
   "source": [
    "months = ['2016-09', '2016-10']\n",
    "\n",
    "for month in months:\n",
    "    print(f\"\\nProcessing submissions for {month}...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Find all daily files for this month\n",
    "    daily_files = sorted(submissions_silver_dir.glob(f'{month}-*.parquet'))\n",
    "    print(f\"Found {len(daily_files)} daily files\")\n",
    "    \n",
    "    if not daily_files:\n",
    "        print(f\"⚠️  No files found for {month}\")\n",
    "        continue\n",
    "    \n",
    "    # Load and concatenate\n",
    "    dfs = []\n",
    "    for f in tqdm(daily_files, desc=f\"Loading {month}\"):\n",
    "        df = pd.read_parquet(f)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df_month = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Loaded {len(df_month):,} submissions\")\n",
    "    \n",
    "    # Apply clean schema (no author for GDPR compliance)\n",
    "    df_gold = df_month[[\n",
    "        'thread_id',\n",
    "        'title',\n",
    "        'selftext',\n",
    "        'created_utc',\n",
    "        'subreddit_id',\n",
    "        'subreddit',\n",
    "        'num_comments'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Rename thread_id to submission_id for clarity\n",
    "    df_gold = df_gold.rename(columns={'thread_id': 'submission_id'})\n",
    "    \n",
    "    # Save monthly file\n",
    "    output_file = gold_submissions_dir / f'{month}.parquet'\n",
    "    df_gold.to_parquet(output_file, compression='snappy', index=False)\n",
    "    \n",
    "    print(f\"✓ Saved: {output_file.name}\")\n",
    "    print(f\"  Columns: {list(df_gold.columns)}\")\n",
    "    print(f\"  Rows: {len(df_gold):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Submissions processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012306b",
   "metadata": {},
   "source": [
    "## 4. Process Comments by Month\n",
    "\n",
    "Aggregate daily comment files into monthly parquet files, filtering out orphaned comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00db5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comments for 2016-09...\n",
      "================================================================================\n",
      "Loaded 386,214 valid submission IDs\n",
      "Found 30 daily files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2016-09: 100%|██████████| 30/30 [00:02<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,812,645 comments\n",
      "Filtered orphaned comments: dropped 46,512 (1.2%)\n",
      "Remaining: 3,766,133 comments with valid submissions\n",
      "\n",
      "✓ Saved: 2016-09.parquet\n",
      "  Columns: ['comment_id', 'submission_id', 'parent_id', 'created_utc', 'subreddit_id', 'subreddit', 'body']\n",
      "  Rows: 3,766,133\n",
      "\n",
      "Processing comments for 2016-10...\n",
      "================================================================================\n",
      "Loaded 537,217 valid submission IDs\n",
      "Found 31 daily files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2016-10: 100%|██████████| 31/31 [00:02<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4,973,150 comments\n",
      "Filtered orphaned comments: dropped 40,360 (0.8%)\n",
      "Remaining: 4,932,790 comments with valid submissions\n",
      "\n",
      "✓ Saved: 2016-10.parquet\n",
      "  Columns: ['comment_id', 'submission_id', 'parent_id', 'created_utc', 'subreddit_id', 'subreddit', 'body']\n",
      "  Rows: 4,932,790\n",
      "\n",
      "================================================================================\n",
      "✓ Comments processing complete\n"
     ]
    }
   ],
   "source": [
    "for month in months:\n",
    "    print(f\"\\nProcessing comments for {month}...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load submissions for this month to get valid submission_ids\n",
    "    submissions_file = gold_submissions_dir / f'{month}.parquet'\n",
    "    if not submissions_file.exists():\n",
    "        print(f\"⚠️  Submissions file not found: {submissions_file.name}\")\n",
    "        continue\n",
    "    \n",
    "    df_subs = pd.read_parquet(submissions_file)\n",
    "    valid_submission_ids = set(df_subs['submission_id'])\n",
    "    print(f\"Loaded {len(valid_submission_ids):,} valid submission IDs\")\n",
    "    \n",
    "    # Find all daily comment files for this month\n",
    "    daily_files = sorted(comments_silver_dir.glob(f'{month}-*.parquet'))\n",
    "    print(f\"Found {len(daily_files)} daily files\")\n",
    "    \n",
    "    if not daily_files:\n",
    "        print(f\"⚠️  No files found for {month}\")\n",
    "        continue\n",
    "    \n",
    "    # Load and concatenate\n",
    "    dfs = []\n",
    "    for f in tqdm(daily_files, desc=f\"Loading {month}\"):\n",
    "        df = pd.read_parquet(f)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df_month = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Loaded {len(df_month):,} comments\")\n",
    "    \n",
    "    # Extract submission_id from link_id (remove t3_ prefix)\n",
    "    df_month['submission_id'] = df_month['link_id'].str.replace('t3_', '', regex=False)\n",
    "    \n",
    "    # Filter out comments without matching submissions (deleted/orphaned)\n",
    "    before_filter = len(df_month)\n",
    "    df_month = df_month[df_month['submission_id'].isin(valid_submission_ids)]\n",
    "    after_filter = len(df_month)\n",
    "    dropped = before_filter - after_filter\n",
    "    \n",
    "    print(f\"Filtered orphaned comments: dropped {dropped:,} ({100*dropped/before_filter:.1f}%)\")\n",
    "    print(f\"Remaining: {after_filter:,} comments with valid submissions\")\n",
    "    \n",
    "    # Apply clean schema (no author for GDPR compliance)\n",
    "    df_gold = df_month[[\n",
    "        'comment_id',\n",
    "        'submission_id',\n",
    "        'parent_id',\n",
    "        'created_utc',\n",
    "        'subreddit_id',\n",
    "        'subreddit',\n",
    "        'body'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Save monthly file\n",
    "    output_file = gold_comments_dir / f'{month}.parquet'\n",
    "    df_gold.to_parquet(output_file, compression='snappy', index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Saved: {output_file.name}\")\n",
    "    print(f\"  Columns: {list(df_gold.columns)}\")\n",
    "    print(f\"  Rows: {len(df_gold):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Comments processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c5808",
   "metadata": {},
   "source": [
    "## 5. Verify Output and Coverage\n",
    "\n",
    "Check that submissions and comments are properly linked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29203bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying submission-comment linkage...\n",
      "================================================================================\n",
      "\n",
      "2016-09:\n",
      "  Submissions: 386,214\n",
      "  Comments: 3,766,133\n",
      "  Unique submission_ids in comments: 206,491\n",
      "  Matching submissions: 206,491 (100.0%)\n",
      "\n",
      "2016-10:\n",
      "  Submissions: 537,217\n",
      "  Comments: 4,932,790\n",
      "  Unique submission_ids in comments: 295,478\n",
      "  Matching submissions: 295,478 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "✓ Verification complete\n",
      "\n",
      "Final output:\n",
      "  data/01_corpus/02_gold/reddit/submissions/\n",
      "    - 2016-09.parquet\n",
      "    - 2016-10.parquet\n",
      "  data/01_corpus/02_gold/reddit/comments/\n",
      "    - 2016-09.parquet\n",
      "    - 2016-10.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerifying submission-comment linkage...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for month in months:\n",
    "    submissions_file = gold_submissions_dir / f'{month}.parquet'\n",
    "    comments_file = gold_comments_dir / f'{month}.parquet'\n",
    "    \n",
    "    if not submissions_file.exists() or not comments_file.exists():\n",
    "        print(f\"⚠️  Missing files for {month}\")\n",
    "        continue\n",
    "    \n",
    "    df_sub = pd.read_parquet(submissions_file)\n",
    "    df_com = pd.read_parquet(comments_file)\n",
    "    \n",
    "    # Check coverage\n",
    "    submission_ids_from_subs = set(df_sub['submission_id'])\n",
    "    submission_ids_from_coms = set(df_com['submission_id'])\n",
    "    \n",
    "    coverage = len(submission_ids_from_coms & submission_ids_from_subs)\n",
    "    missing = len(submission_ids_from_coms - submission_ids_from_subs)\n",
    "    \n",
    "    print(f\"\\n{month}:\")\n",
    "    print(f\"  Submissions: {len(df_sub):,}\")\n",
    "    print(f\"  Comments: {len(df_com):,}\")\n",
    "    print(f\"  Unique submission_ids in comments: {len(submission_ids_from_coms):,}\")\n",
    "    print(f\"  Matching submissions: {coverage:,} ({100*coverage/len(submission_ids_from_coms):.1f}%)\")\n",
    "    if missing > 0:\n",
    "        print(f\"  ⚠️  Comments with missing submissions: {missing:,}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Verification complete\")\n",
    "print(f\"\\nFinal output:\")\n",
    "print(f\"  {gold_submissions_dir.relative_to(workspace_root)}/\")\n",
    "print(f\"    - 2016-09.parquet\")\n",
    "print(f\"    - 2016-10.parquet\")\n",
    "print(f\"  {gold_comments_dir.relative_to(workspace_root)}/\")\n",
    "print(f\"    - 2016-09.parquet\")\n",
    "print(f\"    - 2016-10.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
