{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7805c470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation plan initialized: 2025-12-18T14:05:49.621369\n",
      "Workspace: /Users/stahlma/Desktop/01_Studium/11_Thesis/Data_Experiment\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "workspace_root = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_root / 'src'))\n",
    "\n",
    "from thesis_pipeline.io.config import load_all_configs\n",
    "\n",
    "print(f\"Validation plan initialized: {datetime.now().isoformat()}\")\n",
    "print(f\"Workspace: {workspace_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688fb90",
   "metadata": {},
   "source": [
    "## 1. Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98252ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: 2016-09-01 to 2016-10-31\n",
      "Seed: thesis_sep_oct_2016_v1\n",
      "Daily news limit: 10,000 articles/day\n"
     ]
    }
   ],
   "source": [
    "configs = load_all_configs(workspace_root / 'configs')\n",
    "global_cfg = configs['global']\n",
    "reddit_cfg = configs['reddit']\n",
    "news_cfg = configs['news']\n",
    "\n",
    "period_start = global_cfg['validation_run']['period_start']\n",
    "period_end = global_cfg['validation_run']['period_end']\n",
    "seed = global_cfg['validation_run']['seed_string']\n",
    "\n",
    "print(f\"Period: {period_start} to {period_end}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"Daily news limit: {news_cfg['sampling']['daily_limit_n']:,} articles/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f7393",
   "metadata": {},
   "source": [
    "## 2. Calculate Time Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8972fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total days: 61\n",
      "Date range: 2016-09-01 to 2016-10-31\n",
      "\n",
      "First 5 dates: ['2016-09-01', '2016-09-02', '2016-09-03', '2016-09-04', '2016-09-05']\n",
      "Last 5 dates: ['2016-10-27', '2016-10-28', '2016-10-29', '2016-10-30', '2016-10-31']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.strptime(period_start, '%Y-%m-%d')\n",
    "end_date = datetime.strptime(period_end, '%Y-%m-%d')\n",
    "num_days = (end_date - start_date).days + 1\n",
    "\n",
    "print(f\"Total days: {num_days}\")\n",
    "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Generate list of all dates\n",
    "all_dates = [start_date + timedelta(days=i) for i in range(num_days)]\n",
    "print(f\"\\nFirst 5 dates: {[d.strftime('%Y-%m-%d') for d in all_dates[:5]]}\")\n",
    "print(f\"Last 5 dates: {[d.strftime('%Y-%m-%d') for d in all_dates[-5:]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907965e",
   "metadata": {},
   "source": [
    "## 3. Estimated Data Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec72187c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== News (CC-NEWS) ===\n",
      "Daily limit: 10,000 articles/day\n",
      "Total articles (max): 610,000\n",
      "Estimated storage: ~2.9 GB (silver layer)\n",
      "\n",
      "=== Reddit (Politosphere) ===\n",
      "Unknown volume until download completes\n",
      "Period covers Sep-Oct 2016 US election season (high activity expected)\n",
      "\n",
      "=== Total Estimated Storage ===\n",
      "Raw + Silver + Gold: ~20-40 GB (conservative estimate)\n",
      "QA artifacts + reports: ~1-2 GB\n"
     ]
    }
   ],
   "source": [
    "# News estimates\n",
    "news_daily_limit = news_cfg['sampling']['daily_limit_n']\n",
    "estimated_news_articles = num_days * news_daily_limit\n",
    "\n",
    "# Conservative size estimates (compressed Parquet)\n",
    "avg_article_kb = 5  # ~5KB per article (text + metadata, compressed)\n",
    "estimated_news_gb = (estimated_news_articles * avg_article_kb) / (1024 * 1024)\n",
    "\n",
    "print(\"=== News (CC-NEWS) ===\")\n",
    "print(f\"Daily limit: {news_daily_limit:,} articles/day\")\n",
    "print(f\"Total articles (max): {estimated_news_articles:,}\")\n",
    "print(f\"Estimated storage: ~{estimated_news_gb:.1f} GB (silver layer)\")\n",
    "\n",
    "print(\"\\n=== Reddit (Politosphere) ===\")\n",
    "print(\"Unknown volume until download completes\")\n",
    "print(\"Period covers Sep-Oct 2016 US election season (high activity expected)\")\n",
    "\n",
    "print(\"\\n=== Total Estimated Storage ===\")\n",
    "print(f\"Raw + Silver + Gold: ~20-40 GB (conservative estimate)\")\n",
    "print(f\"QA artifacts + reports: ~1-2 GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6fa43",
   "metadata": {},
   "source": [
    "## 4. Execution Plan Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0318a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXECUTION PLAN: Sep-Oct 2016 Validation Run\n",
      "======================================================================\n",
      "\n",
      "1. Reddit Processing\n",
      "   Notebooks:\n",
      "     - 10_reddit_download_sep_oct_2016.ipynb\n",
      "     - 11_redit_extract_filter_silver.ipynb\n",
      "     - 12_reddit_thread_pseudodocs_gold.ipynb\n",
      "     - 13_reddit_dataset_qc_report.ipynb\n",
      "   Expected Outputs:\n",
      "     - data/00_raw/reddit/politosphere_2016-09_2016-10/ (Zenodo download)\n",
      "     - data/01_silver/reddit/YYYY-MM-DD.parquet (daily files)\n",
      "     - data/03_gold/reddit/YYYY-MM-DD.parquet (pseudodocs)\n",
      "     - reports/data_validation/2016-09_2016-10/reddit/\n",
      "\n",
      "2. News Processing\n",
      "   Notebooks:\n",
      "     - 20_news_hf_stream_sep_oct_2016.ipynb\n",
      "     - 21_news_filter_dedup_sample_silver.ipynb\n",
      "     - 22_news_dataset_qc_report.ipynb\n",
      "   Expected Outputs:\n",
      "     - data/00_raw/news/ccnews_2016-09_2016-10/ (HF download)\n",
      "     - data/01_silver/news/YYYY-MM-DD.parquet (daily files, max 10k/day)\n",
      "     - reports/data_validation/2016-09_2016-10/news/\n",
      "\n",
      "3. Cross-Domain Validation\n",
      "   Notebooks:\n",
      "     - 90_shared_schema_checks.ipynb\n",
      "     - 91_shared_determinism_checks.ipynb\n",
      "   Expected Outputs:\n",
      "     - data/04_qa/manifests/ (hash manifests)\n",
      "     - data/04_qa/snapshots/ (determinism comparison)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "execution_plan = [\n",
    "    {\n",
    "        'phase': 'Reddit Processing',\n",
    "        'notebooks': [\n",
    "            '10_reddit_download_sep_oct_2016.ipynb',\n",
    "            '11_redit_extract_filter_silver.ipynb',\n",
    "            '12_reddit_thread_pseudodocs_gold.ipynb',\n",
    "            '13_reddit_dataset_qc_report.ipynb'\n",
    "        ],\n",
    "        'outputs': [\n",
    "            'data/00_raw/reddit/politosphere_2016-09_2016-10/ (Zenodo download)',\n",
    "            'data/01_silver/reddit/YYYY-MM-DD.parquet (daily files)',\n",
    "            'data/03_gold/reddit/YYYY-MM-DD.parquet (pseudodocs)',\n",
    "            'reports/data_validation/2016-09_2016-10/reddit/'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'phase': 'News Processing',\n",
    "        'notebooks': [\n",
    "            '20_news_hf_stream_sep_oct_2016.ipynb',\n",
    "            '21_news_filter_dedup_sample_silver.ipynb',\n",
    "            '22_news_dataset_qc_report.ipynb'\n",
    "        ],\n",
    "        'outputs': [\n",
    "            'data/00_raw/news/ccnews_2016-09_2016-10/ (HF download)',\n",
    "            'data/01_silver/news/YYYY-MM-DD.parquet (daily files, max 10k/day)',\n",
    "            'reports/data_validation/2016-09_2016-10/news/'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'phase': 'Cross-Domain Validation',\n",
    "        'notebooks': [\n",
    "            '90_shared_schema_checks.ipynb',\n",
    "            '91_shared_determinism_checks.ipynb'\n",
    "        ],\n",
    "        'outputs': [\n",
    "            'data/04_qa/manifests/ (hash manifests)',\n",
    "            'data/04_qa/snapshots/ (determinism comparison)'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXECUTION PLAN: Sep-Oct 2016 Validation Run\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, phase_info in enumerate(execution_plan, 1):\n",
    "    print(f\"\\n{i}. {phase_info['phase']}\")\n",
    "    print(\"   Notebooks:\")\n",
    "    for nb in phase_info['notebooks']:\n",
    "        print(f\"     - {nb}\")\n",
    "    print(\"   Expected Outputs:\")\n",
    "    for output in phase_info['outputs']:\n",
    "        print(f\"     - {output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71b95f",
   "metadata": {},
   "source": [
    "## 5. Key Design Decisions Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ce1ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Key Design Decisions ===\n",
      "\n",
      "Platform Separation:\n",
      "  Reddit and News processed independently, no cross-platform timestamping\n",
      "\n",
      "Reddit Time Indexing:\n",
      "  Daily buckets use comment.created_utc only; thread context for inference only\n",
      "\n",
      "News Sampling:\n",
      "  Deterministic hash-based: top-10,000 by smallest hash(seed||url||date) per day\n",
      "\n",
      "Deduplication:\n",
      "  Reddit: exact (author+body+time), News: canonical_url within day\n",
      "\n",
      "Partitioning:\n",
      "  Daily Parquet files for both domains\n",
      "\n",
      "Data Access:\n",
      "  Reddit: Zenodo download, News: HuggingFace download then process\n",
      "\n",
      "Leakage Prevention:\n",
      "  Strict date filtering, no future information, Sep-Oct 2016 only\n",
      "\n",
      "Determinism:\n",
      "  Fixed seed: \"thesis_sep_oct_2016_v1\", reproducible sampling via hash ordering\n"
     ]
    }
   ],
   "source": [
    "design_decisions = {\n",
    "    'Platform Separation': 'Reddit and News processed independently, no cross-platform timestamping',\n",
    "    'Reddit Time Indexing': 'Daily buckets use comment.created_utc only; thread context for inference only',\n",
    "    'News Sampling': f'Deterministic hash-based: top-{news_daily_limit:,} by smallest hash(seed||url||date) per day',\n",
    "    'Deduplication': 'Reddit: exact (author+body+time), News: canonical_url within day',\n",
    "    'Partitioning': 'Daily Parquet files for both domains',\n",
    "    'Data Access': 'Reddit: Zenodo download, News: HuggingFace download then process',\n",
    "    'Leakage Prevention': 'Strict date filtering, no future information, Sep-Oct 2016 only',\n",
    "    'Determinism': f'Fixed seed: \"{seed}\", reproducible sampling via hash ordering'\n",
    "}\n",
    "\n",
    "print(\"=== Key Design Decisions ===\")\n",
    "for decision, description in design_decisions.items():\n",
    "    print(f\"\\n{decision}:\")\n",
    "    print(f\"  {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906ba9a",
   "metadata": {},
   "source": [
    "## 6. Capture Git Commit and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6dbfe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git branch: main\n",
      "Git commit: 175129e32cc0cbe6b9e9c1cbcfdf7fa8f0313d45\n",
      "Uncommitted changes: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    git_commit = subprocess.check_output(\n",
    "        ['git', 'rev-parse', 'HEAD'],\n",
    "        cwd=workspace_root,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    ).decode().strip()\n",
    "    \n",
    "    git_branch = subprocess.check_output(\n",
    "        ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n",
    "        cwd=workspace_root,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    ).decode().strip()\n",
    "    \n",
    "    git_status = subprocess.check_output(\n",
    "        ['git', 'status', '--porcelain'],\n",
    "        cwd=workspace_root\n",
    "    ).decode().strip()\n",
    "    \n",
    "    has_uncommitted = bool(git_status)\n",
    "    \n",
    "except subprocess.CalledProcessError:\n",
    "    git_commit = 'unknown'\n",
    "    git_branch = 'unknown'\n",
    "    has_uncommitted = False\n",
    "\n",
    "print(f\"Git branch: {git_branch}\")\n",
    "print(f\"Git commit: {git_commit}\")\n",
    "print(f\"Uncommitted changes: {has_uncommitted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89363c",
   "metadata": {},
   "source": [
    "## 7. Create Run Metadata JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metadata = {\n",
    "    'run_id': f'2016-09_2016-10_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'environment': {\n",
    "        'python_version': sys.version,\n",
    "        'workspace_root': str(workspace_root),\n",
    "        'git_commit': git_commit,\n",
    "        'git_branch': git_branch,\n",
    "        'has_uncommitted_changes': has_uncommitted\n",
    "    },\n",
    "    'configuration': {\n",
    "        'period_start': period_start,\n",
    "        'period_end': period_end,\n",
    "        'num_days': num_days,\n",
    "        'seed_string': seed,\n",
    "        'news_daily_limit': news_daily_limit,\n",
    "        'hash_algorithm': global_cfg['determinism']['hash_algorithm']\n",
    "    },\n",
    "    'estimates': {\n",
    "        'max_news_articles': estimated_news_articles,\n",
    "        'estimated_storage_gb': round(estimated_news_gb, 1)\n",
    "    },\n",
    "    'status': {\n",
    "        'phase': 'initialization',\n",
    "        'notebooks_completed': [],\n",
    "        'last_updated': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = workspace_root / global_cfg['paths']['artefacts'] / '2016-09_2016-10_run.json'\n",
    "metadata_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(run_metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Run metadata saved to: {metadata_path.relative_to(workspace_root)}\")\n",
    "print(f\"\\nRun ID: {run_metadata['run_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8f35a",
   "metadata": {},
   "source": [
    "## 8. Initialize Manifest Tracking Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9306585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manifest tracking structure for determinism checks\n",
    "manifest_structure = {\n",
    "    'run_id': run_metadata['run_id'],\n",
    "    'seed': seed,\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'domains': {\n",
    "        'reddit': {\n",
    "            'daily_files': {},  # Will be populated: 'YYYY-MM-DD': {'hash': 'xxx', 'row_count': N, 'size_bytes': M}\n",
    "            'total_rows': None,\n",
    "            'total_size_bytes': None\n",
    "        },\n",
    "        'news': {\n",
    "            'daily_files': {},\n",
    "            'total_rows': None,\n",
    "            'total_size_bytes': None\n",
    "        }\n",
    "    },\n",
    "    'determinism_runs': []  # Will store hashes from each rerun\n",
    "}\n",
    "\n",
    "manifest_path = workspace_root / 'data/04_qa/manifests' / 'run_manifest.json'\n",
    "manifest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest_structure, f, indent=2)\n",
    "\n",
    "print(f\"✓ Manifest structure initialized: {manifest_path.relative_to(workspace_root)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21565f8",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1bf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATA VALIDATION PLAN SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Run ID: {run_metadata['run_id']}\")\n",
    "print(f\"Period: {period_start} to {period_end} ({num_days} days)\")\n",
    "print(f\"Git commit: {git_commit[:8]}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"News daily limit: {news_daily_limit:,} articles/day\")\n",
    "print(f\"Max news articles: {estimated_news_articles:,}\")\n",
    "print(f\"Estimated storage: ~{estimated_news_gb:.1f} GB (news only)\")\n",
    "print(\"\\n✓ Run metadata saved\")\n",
    "print(\"✓ Manifest tracking initialized\")\n",
    "print(\"\\n=\" * 70)\n",
    "print(\"NEXT STEP: Begin Reddit processing\")\n",
    "print(\"  → Open: notebooks/reddit/10_reddit_download_sep_oct_2016.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
